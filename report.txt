# Report notes
  |  Report of 3-5 pages summarising the techniques and features youâ€™ve  |
  |  used for the classification, as well as the performance results.    |
  |                                                                      |
  |  The report should discuss any decisions you have made during the    |
  |  implementation, discuss the details of the classifiers and features |
  |  you've implemented, show results, a brief discussion of the results |
  |______________________________________________________________________|

## Preprocessing
----------------
  # -> what about removing the @usernames/#hashtags, is it advisable?
I decided to include usernames because some of them can convey semantic meaning which is correlated
across the community. In case of using bigrams, the careless removing of usernames removing would
introduce considerable noise because 2 tokens might end up next to each other even though they are
never used like that in real language. The same goes for hashtags, for example:
"The voice of #blacklivesmatter is on twitter" will turn into "The voice of is on twitter" although
'of is' is not a proper english bigram.
In summary, keeping the #hashtags and @usernames in the text is conceptually meaningful because they
are becoming part of people's communication, gaining new meanings, among the younger generations and
social media users. Specifically, those features can help with semantic segmentation in two ways:
Directly, by providing a potentially useful features and indirectly by not introducing noise in case
they were removed.

-> However, after testing the performance of the simple classifiers, I decided to replace usernames with
a generic username, because they do not seem very helpful overall and probably just add noise in the
system as they are not typically features representative of the sentiment (although in theory they could be)

-> Including Usernames can lead to overfitting because one particular username might be associated with
 mostly positive/negative tweets. It might even improve our performance on smaller sets of tweets (as we
  could see) but in the end it will inevitably not generalise very well and sooner or later our model
 will be overfitting and conceptually it will not lead to any improved understanding of language.

  # Emoticons and emojis
-> There are 59 types of emoticons and they are used 17019 times in the training set. -> wrong nums
-> There are 305 types of emojis and they are used 1794 times in the training set.    -> wrong nums
The way to handle emoticons was to identify the emoticon strings and then replace them with a substitute
text-only strings so that I remove other non-alphanumeric chars.

-> SVM coeficients show that emoticons have a big impact on classical classification (figure top20)

Therefore, I decided to account only for emoticons and get rid of any emojis in the tweets because the
benifit of including those features seems to be significant only for emoticons. For even further refinement
of the, it might be beneficial to include emojis too, but it will have only limited impact of the performance.



  # -> need to remove URLs!
My implementation of URLs removal acknowledges that this task has to be a trade off between recall
(what proportion of URLs we successfully remove) versus the false positive rate (how many non-URLs
do we remove). I decided to remove all words which include 'http' which seems very accurate as there
are not many contexts where you would use this substring outside of the URL. Unfortunately, large
proportion of URLs are not in this form. Therefore, I also remove any word with 'www' substring, that
is also a reasonable assumption. The problematic then becomes only the case when users type only the
form of URL without the prefix: google.com, google.it, google.fr and so on. However, this can be easily
mistaken with (incorrect) punctuation, an abbreviation etc. In those cases, we do not want to remove
the phrase. Therefore, I decided to use computationally more demanding approach by listing 100 most
common extensions (.com, .net, .gov...) and delete the URLs in the form 'web.extension', which preserves
most cases in which we want to keep the words while deleting most or all unwanted URLs.

  # -> There is a lot of noise/mistakes in the data and absence of interpunction.
One thing I noticed about the data during preprocessing is how much noise there is - many posts have
typos or are not written in grammatically correct English and this would certainly negatively impact
the performance of the classifier because the feature representation of the tweets will be inherently
noisy. Most tweets do not contain interpunction and even if they do, it is highly inconsistent or wrong.
I was originally considering using a start of the sentence token and end of the sentence token to gain
some potentially useful features. However, due to inconsistency and scarcity of interpunction, I decided
not to implement this because its benefits will not be significant and due to the noise it could be also
decreasing the accuracy of the classifier.

  # -> what about adding of the starting token?
Specifically, the starting/ending token of the tweet is another feature which could be considered but in
the BoW representation it would be superfluous, hence not useful, because every single tweet will have
this feature. Where it might have certain impact on the accuracy of the classifier is if we used bigram
features, because a word which occurs at the beginning or at the end might change the overall sentiment
of the tweet: consider the case when somebody says "I like this film. Just kidding." The fact that 'kidding'
is at the end of the sentence completely changes its sentiment. Therefore, it could in be a useful feature.

## Representation: BoW
----------------------
- my implementation
- sparse vectors -> more efficient ways to encode
- no use for starting/ending tokens
  * BOWsparse-glove.pkl -> usernames replaced according to GloVe preprocessing
  * BOWsparse-plain.pkl -> usernames kept

## Representation: TFIDF BoW
----------------------------
- my implementation of TFIDF wighting - it might add more precise features

## Representation: GloVE
------------------------
- the word lists from GloVE 6B do not contain hashtags nor usernames. Solution would be to use a different
GloVE system which was trained with twitter datasets (glove.twitter.27B.zip).
-> no hashtags in GloVE 6B -> I need to delete hashtags from preprocessing!
-> added a weighted average for the <OOV> vector


## Building Classifiers

## KNN
- The implementation is too slow, I have to account for sparseness of the BOW feature vectors (?)
- combined train and dev set
-  with emoticons, without usernames,   BOW - sparse implem.: avg F1 = 0.35, t = 9.7
-  with emoticons, without usernames,   BOW - dense implem.:  avg F1 = 0.35, t = 872 -> almost 100x slower!!
-  without emoticons, with usernames,   BOW - sparse implem.: avg F1 = 0.383, t = 8.79
-  without emoticons,without usernames, BOW -> Best params:  {'metric': 'cosine', 'n_neighbors': 9, 'weights': 'uniform'}

-  without emoticons, without usernames,TFIDF - sparse implem.: avg F1 = 0.424, t = 10.4
-  without emoticons,    with usernames,TFIDF - sparse implem.: avg F1 = 0.437, t = 9.6

## Multinomial NB
-  without emoticons,    with usernames,BOW - sparse implem.: avg F1 = 0.512, t = 0.05
-  without emoticons, without usernames,BOW - sparse implem.: avg F1 = 0.517, t = 0.04

-  without emoticons, without usernames,TFIDF - sparse implem.: avg F1 = 0.49, t = 0.05
-  without emoticons,    with usernames,TFIDF - sparse implem.: avg F1 = 0.473, t = 0.055


## Linear SVC
-  without emoticons,    with usernames,BOW- sparse implem.: avg F1 = 0.576, t = 0.3
-  without emoticons, without usernames,BOW- sparse implem.: avg F1 = 0.573, t = 0.3

-  without emoticons, without usernames,TFIDF - sparse implem.: avg F1 = 0.58, t = 0.256
-  without emoticons,    with usernames,TFIDF - sparse implem.: avg F1 = 0.571, t = 0.296

## Logistic Regression (MaxEnt) Classifier
- without emoticons,  without usernames,BOW- sparse implem.: avg F1 = 0.588, t = 7.1
- without emoticons,     with usernames,BOW- sparse implem.: avg F1 = 0.594, t = 8.3

-  without emoticons, without usernames,TFIDF - sparse implem.: avg F1 = 0.602, t = 7.8
-  without emoticons,    with usernames,TFIDF - sparse implem.: avg F1 = 0.599, t = 7.8

## LSTM implementation

-> v0 - simple LSTM
-> v1 - LSTM with additional hidden layer
    -> v1.1 (v1) on 10000 vocab
-> v2 - LSTM with additional hidden layer and dropout



-> the size of vocabulary is quite limited at 5000 words, I checked the difference in performance with 10000 vocab
    -> indeed, the lstm performs better with larger vocabulary - this means that if we should use only 5000 words,
    we have to choose them wisely and frequency of the word is not necessarily the best approach
-> checking for v1 - LSTM with weighted approach vocab selection (weighting rule 2*frequency + 1*sentiment)
    -> normal weighting: avg F1 = 0.549
    -> 2f-1s  weighting: avg F1 = 0.583 !!
    -> 1f-1s  weighting: avg F1 = 0.574
-> v1 - adding truncation for each batch - there are lot of zeros, because lot of tweets are short
    -> this implementation of truncation is very batch_size dependent - it does not truncate much for larger batches
    -> 2f-1s weighting, batch 128: avg F1 = 0.577




## Evaluation and Comparison of Classifiers
