{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required modules for preprocessing\n",
    "import os\n",
    "from os.path import join\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords as Stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# import required modules for classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import required modules for RNN classification\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Load training set, dev set and testing set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from: \n",
      "\t../semeval-tweets/twitter-training-data.txt\n",
      "\t../semeval-tweets/twitter-test1.txt\n",
      "\t../semeval-tweets/twitter-test2.txt\n",
      "\t../semeval-tweets/twitter-test3.txt\n",
      "\t../semeval-tweets/twitter-test4.txt\n",
      "\t../semeval-tweets/twitter-test5.txt\n",
      "\t../semeval-tweets/twitter-dev-data.txt\n"
     ]
    }
   ],
   "source": [
    "# Load training set, dev set and testing set\n",
    "\n",
    "dataDir = '../semeval-tweets'  # change to the proper directory\n",
    "#datasetStrings = ['twitter-training-data.txt', 'twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt', 'twitter-dev-data.txt']\n",
    "datasetStrings = ['twitter-training-data.txt', 'twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt', 'twitter-test4.txt', 'twitter-test5.txt', 'twitter-dev-data.txt']\n",
    "\n",
    "\n",
    "datasets = [join(dataDir, t) for t in datasetStrings]\n",
    "\n",
    "if len(datasets)==5:\n",
    "    print(f\"Extracting data from: \\n\\t{datasets[0]}\\n\\t{datasets[1]}\\n\\t{datasets[2]}\\n\\t{datasets[3]}\\n\\t{datasets[4]}\")\n",
    "    tweet_IDs = {}          # init dictionary with tweet IDs\n",
    "    tweet_sentiments = {}   # init dictionary with sentiments\n",
    "    tweet_texts = {}        # init dictionary with tweet texts\n",
    "    for DatasetString in datasets:\n",
    "        data_ID, data_sent, data_text  = {}, {}, {}    # temp dictionaries\n",
    "        with open(DatasetString, 'r', encoding='utf8') as f1:\n",
    "            for i, line in enumerate(f1):\n",
    "                fields = line.split('\\t')\n",
    "                data_ID[i] = fields[0]            # tweet IDs\n",
    "                data_sent[fields[0]] = fields[1]  # sentiments\n",
    "                data_text[fields[0]] = fields[2]  # tweet text\n",
    "        tweet_IDs[DatasetString] = data_ID\n",
    "        tweet_sentiments[DatasetString] = data_sent\n",
    "        tweet_texts[DatasetString] = data_text\n",
    "\n",
    "    # sentiment dictionaries\n",
    "    sent_train = tweet_sentiments[datasets[0]]\n",
    "    sent_test1 = tweet_sentiments[datasets[1]]\n",
    "    sent_test2 = tweet_sentiments[datasets[2]]\n",
    "    sent_test3 = tweet_sentiments[datasets[3]]\n",
    "    sent_dev = tweet_sentiments[datasets[4]]\n",
    "\n",
    "    # tweet text dictionaries\n",
    "    text_train = tweet_texts[datasets[0]]\n",
    "    text_test1 = tweet_texts[datasets[1]]\n",
    "    text_test2 = tweet_texts[datasets[2]]\n",
    "    text_test3 = tweet_texts[datasets[3]]\n",
    "    text_dev = tweet_texts[datasets[4]]\n",
    "\n",
    "    # tweet IDs dictionaries\n",
    "    IDs_train = tweet_IDs[datasets[0]]\n",
    "    IDs_test1 = tweet_IDs[datasets[1]]\n",
    "    IDs_test2 = tweet_IDs[datasets[2]]\n",
    "    IDs_test3 = tweet_IDs[datasets[3]]\n",
    "    IDs_dev = tweet_IDs[datasets[4]]\n",
    "\n",
    "    # saving the sentiments and IDs as a pickle file\n",
    "    if not os.path.isfile(\"preprocessing-sent-ID.pkl\"):\n",
    "        temp = [sent_train, sent_test1, sent_test2, sent_test3, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev]\n",
    "        with open(\"preprocessing-sent-ID.pkl\", 'wb') as out_file:\n",
    "            pickle.dump(temp, out_file, protocol=-1)\n",
    "\n",
    "if len(datasets)==7:\n",
    "    print(f\"Extracting data from: \\n\\t{datasets[0]}\\n\\t{datasets[1]}\\n\\t{datasets[2]}\\n\\t{datasets[3]}\\n\\t{datasets[4]}\\n\\t{datasets[5]}\\n\\t{datasets[6]}\")\n",
    "\n",
    "    tweet_IDs = {}          # init dictionary with tweet IDs\n",
    "    tweet_sentiments = {}   # init dictionary with sentiments\n",
    "    tweet_texts = {}        # init dictionary with tweet texts\n",
    "    for DatasetString in datasets:\n",
    "        data_ID, data_sent, data_text  = {}, {}, {}    # temp dictionaries\n",
    "        with open(DatasetString, 'r', encoding='utf8') as f1:\n",
    "            for i, line in enumerate(f1):\n",
    "                fields = line.split('\\t')\n",
    "                data_ID[i] = fields[0]            # tweet IDs\n",
    "                data_sent[fields[0]] = fields[1]  # sentiments\n",
    "                data_text[fields[0]] = fields[2]  # tweet text\n",
    "        tweet_IDs[DatasetString] = data_ID\n",
    "        tweet_sentiments[DatasetString] = data_sent\n",
    "        tweet_texts[DatasetString] = data_text\n",
    "\n",
    "    # sentiment dictionaries\n",
    "    sent_train = tweet_sentiments[datasets[0]]\n",
    "    sent_test1 = tweet_sentiments[datasets[1]]\n",
    "    sent_test2 = tweet_sentiments[datasets[2]]\n",
    "    sent_test3 = tweet_sentiments[datasets[3]]\n",
    "    sent_test4 = tweet_sentiments[datasets[4]]\n",
    "    sent_test5 = tweet_sentiments[datasets[5]]\n",
    "    sent_dev = tweet_sentiments[datasets[6]]\n",
    "\n",
    "    # tweet text dictionaries\n",
    "    text_train = tweet_texts[datasets[0]]\n",
    "    text_test1 = tweet_texts[datasets[1]]\n",
    "    text_test2 = tweet_texts[datasets[2]]\n",
    "    text_test3 = tweet_texts[datasets[3]]\n",
    "    text_test4 = tweet_texts[datasets[4]]\n",
    "    text_test5 = tweet_texts[datasets[5]]\n",
    "    text_dev = tweet_texts[datasets[6]]\n",
    "\n",
    "    # tweet IDs dictionaries\n",
    "    IDs_train = tweet_IDs[datasets[0]]\n",
    "    IDs_test1 = tweet_IDs[datasets[1]]\n",
    "    IDs_test2 = tweet_IDs[datasets[2]]\n",
    "    IDs_test3 = tweet_IDs[datasets[3]]\n",
    "    IDs_test4 = tweet_IDs[datasets[4]]\n",
    "    IDs_test5 = tweet_IDs[datasets[5]]\n",
    "    IDs_dev = tweet_IDs[datasets[6]]\n",
    "\n",
    "    temp = [sent_train, sent_test1, sent_test2, sent_test3, sent_test4,  sent_test5, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_test4, IDs_test5, IDs_dev]\n",
    "    with open(\"preprocessing-sent-ID7.pkl\", 'wb') as out_file:\n",
    "        pickle.dump(temp, out_file, protocol=-1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# auxiliary functions\n",
    "\n",
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    reading the testset and return a dictionary with: ID -> sentiment\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}  # init the dictionary\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "        n = acc['tp'] + acc['fn']\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "        itemcount += n\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)\n",
    "    return semevalmacrof1\n",
    "\n",
    "# removing stop words variables\n",
    "stopwords = Stopwords.words('english')\n",
    "stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its BoW representation as np.array\n",
    "def text2BOW(text_list, vocabulary, vocab2num, stopwords):\n",
    "    BOW_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in text_list:\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                BOW_vec[vocab2num[word]-1] += 1\n",
    "            else:\n",
    "                BOW_vec[vocab2num['<OOV>']-1] += 1\n",
    "    return BOW_vec\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its TFIDF representation as np.array\n",
    "def text2TFIDF(text_list, vocabulary, vocab2num, stopwords, DFfreq, Ntexts):\n",
    "    TFIDF_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in np.unique(text_list):\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                if DFfreq[word] == 0:    ###\n",
    "                    print('oh no:', word)###\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / DFfreq[word])\n",
    "                TFIDF_vec[vocab2num[word]-1] = tf * idf\n",
    "            else:\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / 0.000001 )\n",
    "                TFIDF_vec[vocab2num['<OOV>']-1] = tf * idf\n",
    "    return TFIDF_vec\n",
    "\n",
    "# convenience ftion for sentiment -> num\n",
    "def sent2num(sent):\n",
    "    if sent == 'negative':\n",
    "        return -1\n",
    "    if sent == 'neutral':\n",
    "        return 0\n",
    "    if sent == 'positive':\n",
    "        return 1\n",
    "\n",
    "# convenience ftion for num -> sentiment\n",
    "def num2sent(num):\n",
    "    if num == -1:\n",
    "        return 'negative'\n",
    "    if num == 0:\n",
    "        return 'neutral'\n",
    "    if num == 1:\n",
    "        return 'positive'\n",
    "\n",
    "# convert list of tokens (tweet) to an array of indexes\n",
    "def text_list2array(text_list, vocabulary_list, word2index_dict, max_length):\n",
    "    output_array = np.zeros(max_length, dtype=np.int16)\n",
    "    for i, word in enumerate(text_list):\n",
    "        if word in vocabulary_list:\n",
    "            output_array[i] = word2ID[word]     # update the index in vocab\n",
    "        else:\n",
    "            output_array[i] = word2ID['<OOV>']  # provide the index of OOV\n",
    "    return output_array\n",
    "\n",
    "# cuda settings\n",
    "USE_CUDA = False\n",
    "def cuda(v):\n",
    "    if USE_CUDA:\n",
    "        return v.cuda()\n",
    "    return v\n",
    "\n",
    "# auxiliary ftion for torch\n",
    "def toTensor(v,dtype = torch.float,requires_grad = False):\n",
    "    return cuda(Variable(v.clone().detach()).type(dtype).requires_grad_(requires_grad))\n",
    "\n",
    "# auxiliary ftion for torch tensors\n",
    "def toNumpy(v):\n",
    "    if USE_CUDA:\n",
    "        return v.detach().cpu().numpy()\n",
    "    return v.detach().numpy()\n",
    "\n",
    "# setup a random seed for reproducible results\n",
    "def random_seed_setup(seed):\n",
    "    torch.manual_seed(seed) #torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# auxiliary ftion to truncate the batch according to the longest\n",
    "def cut_off(x_batch):\n",
    "    xs = torch.zeros(x_batch.size())\n",
    "    xs[x_batch != 0] = 1\n",
    "    xs = torch.sum(xs, axis=1)\n",
    "    cut = int(torch.max(xs))\n",
    "    return cut\n",
    "\n",
    "# establish a Dataset class\n",
    "class tweet_dataset(Dataset):\n",
    "    def __init__(self, dataset_pkl):\n",
    "        # load the preprocessed (tokenized) data from pickle file 'dataset_pkl'\n",
    "        with open(dataset_pkl, 'rb') as inp_file:\n",
    "            temp = pickle.load(inp_file)\n",
    "            [xs, ys] = temp\n",
    "            xs = torch.from_numpy(xs)\n",
    "            ys = torch.from_numpy(ys) + 1   # adding 1 so that the classes are 0-negative, 1-neutral, 2-positive\n",
    "            self.x = toTensor(xs, dtype=torch.long, requires_grad=False)\n",
    "            self.y = toTensor(ys, dtype=torch.long, requires_grad=False)\n",
    "            self.n_samples = len(ys)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Data Preprocessing\n",
    "* lowercase text\n",
    "* regex cleaning\n",
    "   * Remove URLs\n",
    "   * Process emoticons\n",
    "   * Remove non-alphanumeric characters (leave hashtags and usernames)\n",
    "   * Process usernames and hashtags\n",
    "   * Remove numbers that are fully made of digits\n",
    "   * (Remove words with only 1 character)\n",
    "* Tokenisation\n",
    "* POS tagging\n",
    "* Lemmatization\n",
    "* Saving the processed output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## Preprocessing 1: Plain - keeping all usernames, hashtags and emojis -> this preprocessing is for BOW and TFIDF-weighted BOW\n",
    "\n",
    "# loading preprocessed datasets - if you already have the preprocessed file\n",
    "if len(datasets)==5:\n",
    "    file_to_load = \"preprocessing-plain.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            temp_dicts = pickle.load(inp_file)\n",
    "            txt_dicts = temp_dicts[0:5]\n",
    "            txtlist_dicts = temp_dicts[5:]\n",
    "\n",
    "else:\n",
    "    top100 = ['com', 'net', 'org', 'jp', 'de', 'uk', 'fr', 'br', 'it', 'ru', 'es', 'me', 'gov', 'pl', 'ca', 'au', 'cn', 'co', 'in', 'nl', 'edu', 'info', 'eu', 'ch', 'id', 'at', 'kr', 'cz', 'mx', 'be', 'tv', 'se', 'tr', 'tw', 'al', 'ua', 'ir', 'vn', 'cl', 'sk', 'ly', 'cc', 'to', 'no', 'fi', 'us', 'pt', 'dk', 'ar', 'hu', 'tk', 'gr', 'il', 'news', 'ro', 'my', 'biz', 'ie', 'za', 'nz', 'sg', 'ee', 'th', 'io', 'xyz', 'pe', 'bg', 'hk', 'rs', 'lt', 'link', 'ph', 'club', 'si', 'site', 'mobi', 'by', 'cat', 'wiki', 'la', 'ga', 'xxx', 'cf', 'hr', 'ng', 'jobs', 'online', 'kz', 'ug', 'gq', 'ae', 'is', 'lv', 'pro', 'fm', 'tips', 'ms', 'sa', 'app', 'lat']\n",
    "\n",
    "    # emoticons extracted from the tweets\n",
    "    emoticons = ['%)', ':&', '8-)', '=/', ':c', ':#', ':)))', ';)', 'd:', '=3', ':O', '8D', 'oO', ':o)', '*)', 'QQ', ':S', '=)', 'D8', ':]', 'O:)', 'XD', 'Q_Q', \":'(\", ':$', ':3', ':L', 'XP', ':-(', ':(', ':-)', ':-))', 'o.O', ':*', '0:3', ';;', ':D', ';D', '=]', ':@', ':)', ':))', ':/', '>:)', ':P', ':-)))', ';]', '^_^', \":')\", ':x', 'D:', ':^)', ':|', ';_;', '=p', ':b', '=D', ':o', 'DX']\n",
    "    emoticon_strings = ['emoticon' + str(num) for num in range(len(emoticons))]\n",
    "    emoticon2string = dict(zip(emoticons, emoticon_strings))\n",
    "    string2emoticon = dict(zip(emoticon_strings, emoticons))\n",
    "\n",
    "    if len(datasets)==7:\n",
    "        ID_dicts = [IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev]\n",
    "        txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev]\n",
    "\n",
    "\n",
    "    if len(datasets)==7:\n",
    "        ID_dicts = [IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_test4, IDs_test5, IDs_dev]\n",
    "        txt_dicts = [text_train, text_test1, text_test2, text_test3, text_test4, text_test5, text_dev]\n",
    "    txtlist_dicts = []\n",
    "    lemmatizer = WordNetLemmatizer()  # init the lemmatizer\n",
    "    POSconvert = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "\n",
    "    for i, IDdict in enumerate(ID_dicts):\n",
    "        output = txt_dicts[i]\n",
    "        output_txt = {}\n",
    "        for id in IDdict.values():\n",
    "            text = output[id].lower()\n",
    "\n",
    "            # replace/delete all URLs starting with 'http' and 'www'\n",
    "            new_text = re.sub(\"http[^\\s]*\", '', text)\n",
    "            new_text = re.sub(\"www[^\\s]*\", '', new_text)\n",
    "\n",
    "            # delete all URLs which have one of 100 most common extensions ('.com', '.net', ...)\n",
    "            for ext in top100:\n",
    "                re_string = \"[^\\s]*\\.\" + ext + \"[^\\s]*\"\n",
    "                new_text = re.sub(re_string, '', new_text)\n",
    "\n",
    "            # replace all emoticons with an emoticon string:  #emoticon42\n",
    "            for em in emoticons:\n",
    "                re_string = '\\s' + re.escape(em) + '\\s'\n",
    "                replace_string = ' ' + emoticon2string[em] + ' '\n",
    "                new_text = re.sub(re_string, replace_string, new_text)\n",
    "\n",
    "            # removing '&amp'\n",
    "            new_text = re.sub('&amp','', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars except for '# and @'\n",
    "            new_text = re.sub('[^\\w\\s@#]','', new_text)\n",
    "\n",
    "            # remove strings with '#' not on the beginning (to keep only hashtags)\n",
    "            new_text = re.sub('\\s[\\w]+#[\\w]*','', new_text)\n",
    "\n",
    "            # numbers fully made of digits\n",
    "            new_text = re.sub('\\s[\\d]+\\s','', new_text)\n",
    "\n",
    "            # remove words with only 1 character\n",
    "            new_text = re.sub('\\\\b\\\\w{1}\\\\b','', new_text)\n",
    "\n",
    "            # remove newline chars -> just aesthetics for printing, it doesn't matter with tokenizer\n",
    "            new_text = new_text.replace('\\n', ' ')\n",
    "\n",
    "            # replace a multiple spaces with a single space -> just aesthetics for printing\n",
    "            new_text = re.sub('\\s+',' ', new_text)\n",
    "\n",
    "            # do not delete @usernames\n",
    "            # do not delete #hashtags\n",
    "\n",
    "\n",
    "            # using the lemmatizer\n",
    "            txt_list = nltk.word_tokenize(new_text)     # tokenise the tweet\n",
    "            for k, word in enumerate(txt_list):         # fixing the separation of hashtags by the tokenizer\n",
    "                if word == '#' or word == '@':\n",
    "                    if k < len(txt_list) - 1:\n",
    "                        txt_list[k] = txt_list[k] + txt_list[k+1]\n",
    "                        txt_list.pop(k+1)\n",
    "            POS = nltk.pos_tag(txt_list)                  # POS tags from nltk\n",
    "            WordNetPOS = [POSconvert(P[1]) for P in POS]  # POS tags for lemmatizer\n",
    "            for j in range(len(txt_list)):\n",
    "                word = txt_list[j]\n",
    "                lemmatized = lemmatizer.lemmatize(word, WordNetPOS[j])  # process each token/word one by one\n",
    "                if lemmatized in emoticon_strings:                      # put the emoticons back in\n",
    "                    lemmatized = string2emoticon[lemmatized]\n",
    "                txt_list[j] = lemmatized                                # update the word in the txt_list\n",
    "\n",
    "            # UPDATE the dictionary\n",
    "            output_txt[id] = ' '.join(txt_list)\n",
    "            output[id] = txt_list\n",
    "\n",
    "        txt_dicts[i] = output_txt\n",
    "        txtlist_dicts.append(output)\n",
    "\n",
    "text_train = txt_dicts[0]\n",
    "text_test1 = txt_dicts[1]\n",
    "text_test2 = txt_dicts[2]\n",
    "text_test3 = txt_dicts[3]\n",
    "text_dev = txt_dicts[4]\n",
    "txtlist_train = txtlist_dicts[0]\n",
    "txtlist_test1 = txtlist_dicts[1]\n",
    "txtlist_test2 = txtlist_dicts[2]\n",
    "txtlist_test3 = txtlist_dicts[3]\n",
    "txtlist_dev = txtlist_dicts[4]\n",
    "\n",
    "# saving preprocessing.pkl\n",
    "file_to_save = \"preprocessing-plain.pkl\"\n",
    "if not os.path.isfile(file_to_save):\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev, txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev]\n",
    "    with open(file_to_save, 'wb') as out_file:\n",
    "        pickle.dump(txt_dicts, out_file, protocol=-1)\n",
    "\n",
    "if len(datasets)==7:\n",
    "    text_train = txt_dicts[0]\n",
    "    text_test1 = txt_dicts[1]\n",
    "    text_test2 = txt_dicts[2]\n",
    "    text_test3 = txt_dicts[3]\n",
    "    text_test4 = txt_dicts[4]\n",
    "    text_test5 = txt_dicts[5]\n",
    "    text_dev = txt_dicts[6]\n",
    "    txtlist_train = txtlist_dicts[0]\n",
    "    txtlist_test1 = txtlist_dicts[1]\n",
    "    txtlist_test2 = txtlist_dicts[2]\n",
    "    txtlist_test3 = txtlist_dicts[3]\n",
    "    txtlist_test4 = txtlist_dicts[4]\n",
    "    txtlist_test5 = txtlist_dicts[5]\n",
    "    txtlist_dev = txtlist_dicts[6]\n",
    "\n",
    "    # saving preprocessing.pkl\n",
    "    file_to_save = \"preprocessing-plain7.pkl\"\n",
    "    if not os.path.isfile(file_to_save):\n",
    "        txt_dicts = [text_train, text_test1, text_test2, text_test3, text_test4, text_test5, text_dev, txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_test4, txtlist_test5, txtlist_dev]\n",
    "        with open(file_to_save, 'wb') as out_file:\n",
    "            pickle.dump(txt_dicts, out_file, protocol=-1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felt privilege to play foo fighter song on guitar today with one of the plectrum from the gig on saturday\n",
      "@aaqibafzaal pakistan may be an islamic country but der be lot true muslim in india who love their country and can sacrifice all for it\n",
      "happy birthday to the coolest golfer in bali @tjvictoriacnd :) may you become cooler and cooler everyday stay humble little sister xx\n",
      "@simpplya tmills be go to tucson but the 29th and it on thursday :(\n",
      "hmmmmm where be the #blacklivesmatter when matter like this rise kid be disgrace\n",
      "@hypable all good im excite about 3rd season find home on netflix just want to make sure the reader have the news a it develop\n",
      "told my mom want to stay in hotel for my 18th with people but my birthday on valentine :-)) lucky me\n",
      "1st thing do after baggage claim be get up to date with @ronnaandbeverly bad blood ronna bev style make be home bit more okay\n",
      "bobby jindal want you to assimilate to heritage of the old confederacy even though his parent be from january dot dot dot\n",
      "@coolcat1304 watch itthe 1st time votedit in the nta award best factual program mite needwatch rest on itvplayer\n",
      "@lilbeast03 im sad that naruto manga be over and ill probably cry when the anime end kishimotosan be come to the u in october\n",
      "feel down this monday check out this guy who dress a baby prince george for week it might cheer you up\n",
      "huge play by #georgia to get out of the shadow of their own goal line on 2nd down lambert to sony for ayard gain very accurate pas\n",
      "kasich seek gop presidential nomination john kasich announce his run for the white house intuesday\n",
      "why do this horse have the same look of fear in it eye when next to marine le pen a romney do when next to tru\n",
      "ben carson be consider for the same cabinet position jack bauer once have mister carson youre no jack bauer\n",
      "john kasichs tone of surrender may not have faze the biased buckeye audience but it certainly bother the gop base @tperkins\n",
      "hommage to gary carter and the expo day tomorrow at rcup dollar hot dog andtix at thelevel #youppiwillbehere\n",
      "@pocketvolcano take it you hear ric flair be at dreamwave in jan\n",
      "@renuudesai yes mambetter google it you know some people dont have any work may create it a big scenethiz happen for many celebtc\n",
      "colour #3dprinting be en vogue for luxury eyewear maker @safilo1934\n",
      "@immortaltech dublin this saturday get ta get on the guinness\n",
      "@transferdicky antiimmigration sentiment be merely piece in bigger puzzle\n",
      "really like holly holm but be she ready for rousey thought\n",
      "which mean if andrade win she could get rousey next\n",
      "why do the medium keep give this brain dead moron air time former alaska gov sarah palin say sunday she\n",
      "@daniboothang @indeliblemarq__ myyr old cousin didnt know ice cube be rapperjust an superstar actor from the friday movie lol\n",
      "club remix next saturday night allstar will be in the building @geezyallstar @crazyronallstar\n",
      "@talk2cleo @kerrymacuska thats right lady im blame everything on kerry cleo this show may well eclipse the number of @jakeneedham\n",
      "cliff avril leave detroit lion game with back injury detroit lion defensive end cliff avril leave sunday ga\n",
      "make me sick to think we the american taxpayer will be pay for secret service detail for trump melania for\n",
      "@jimyeoman @pedallingveg bollock then theres ian brady myra hindley some bad hombre from that city see what do there\n",
      "my topare probably granny chiyo sakura v sasori guy v kissame the 2nd time and naruto v pain\n",
      "the guy at dunkin just say see you tomorrow because he know that im there every day\n",
      "watch the lookalikes teaser on sunday brunch be the only one who think the david beckham lookalike be definitely not lookalike\n",
      "dont think chelsea should appeal for torres red card let sturridge play tomorrow it the carling cup not pl\n",
      "@jonathamingo in other related news you know theres no park tomorrow\n",
      "an older article cite the legal issue behind the dakota access pipeline #dapl\n",
      "today life lesson courtesy of alabama if youre gon na play in texas you get ta have fiddle in the band\n",
      "we a people must do whatever it take to fight obamas lawlessness communist bent on destroy our heritage now not tomorrow\n",
      "you have to watch michael moore in trumplandi catch it on sho2\n",
      "my middle daughter just tell me 5th grade presentation in her class be on seth rollins #school #wwe #education\n",
      "mack sure you check out @carolinakidz1 and @trilla_guapo this saturday @club bodi\n",
      "uk release of star war episode vii the force awaken to be day earlier than expect dec\n",
      "nialls go to beindays no one gon na beill lock him in forever211 prepares squad cmon\n",
      "oracle set the date for it first quarter fiscal yearearnings announcement redwood shore ca mar\n",
      "@jayjbooth such close call cheer pal do well first month at ibm do miss the friday call though haha hope youre well\n",
      "@mdavisbot @justanactor sweet jesus #thewalkingdead\n",
      "@petestavros @megynkelly well good for you\n",
      "phone to consider if you really want the galaxy note via @yahoo\n"
     ]
    }
   ],
   "source": [
    "# checking the preprocessed output\n",
    "for id in list(IDs_train.values())[0:50]:\n",
    "    print(text_train[id])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "## Preprocessing 2: GloVe - replacing usernames with 'username', hashtags with 'hashtag' and keeping only GloVe emoticons\n",
    "\n",
    "# loading preprocessed datasets - if you already have the preprocessed file\n",
    "if len(datasets)==5:\n",
    "    file_to_load = \"preprocessing-glove.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            temp_dicts = pickle.load(inp_file)\n",
    "            txt_dicts = temp_dicts[0:5]\n",
    "            txtlist_dicts = temp_dicts[5:]\n",
    "\n",
    "else:\n",
    "    top100 = ['com', 'net', 'org', 'jp', 'de', 'uk', 'fr', 'br', 'it', 'ru', 'es', 'me', 'gov', 'pl', 'ca', 'au', 'cn', 'co', 'in', 'nl', 'edu', 'info', 'eu', 'ch', 'id', 'at', 'kr', 'cz', 'mx', 'be', 'tv', 'se', 'tr', 'tw', 'al', 'ua', 'ir', 'vn', 'cl', 'sk', 'ly', 'cc', 'to', 'no', 'fi', 'us', 'pt', 'dk', 'ar', 'hu', 'tk', 'gr', 'il', 'news', 'ro', 'my', 'biz', 'ie', 'za', 'nz', 'sg', 'ee', 'th', 'io', 'xyz', 'pe', 'bg', 'hk', 'rs', 'lt', 'link', 'ph', 'club', 'si', 'site', 'mobi', 'by', 'cat', 'wiki', 'la', 'ga', 'xxx', 'cf', 'hr', 'ng', 'jobs', 'online', 'kz', 'ug', 'gq', 'ae', 'is', 'lv', 'pro', 'fm', 'tips', 'ms', 'sa', 'app', 'lat']\n",
    "\n",
    "    # emoticons in the glove embeddings\n",
    "    glove_emoticons = [';)', '=)', ':]', ':3', ':(', ':-)', '0:3', ':@', ':)', ':|', '=p']\n",
    "    glove_emoticon_strings = ['emoticon' + str(num) for num in range(len(emoticons))]\n",
    "    emoticon2string = dict(zip(glove_emoticons, glove_emoticon_strings))\n",
    "    string2emoticon = dict(zip(glove_emoticon_strings, glove_emoticons))\n",
    "\n",
    "    ID_dicts = [IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev]\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev]\n",
    "    txtlist_dicts = []\n",
    "\n",
    "    if len(datasets)==7:\n",
    "        ID_dicts = [IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_test4, IDs_test5, IDs_dev]\n",
    "        txt_dicts = [text_train, text_test1, text_test2, text_test3, text_test4, text_test5, text_dev]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  # init the lemmatizer\n",
    "    POSconvert = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "\n",
    "    for i, IDdict in enumerate(ID_dicts):\n",
    "        output = txt_dicts[i]\n",
    "        output_txt = {}\n",
    "        for id in IDdict.values():\n",
    "            text = output[id].lower()\n",
    "\n",
    "            # replace/delete all URLs starting with 'http' and 'www'\n",
    "            new_text = re.sub(\"http[^\\s]*\", '', text)\n",
    "            new_text = re.sub(\"www[^\\s]*\", '', new_text)\n",
    "\n",
    "            # delete all URLs which have one of 100 most common extensions ('.com', '.net', ...)\n",
    "            for ext in top100:\n",
    "                re_string = \"[^\\s]*\\.\" + ext + \"[^\\s]*\"\n",
    "                new_text = re.sub(re_string, '', new_text)\n",
    "\n",
    "            #replace all emoticons with an emoticon string:  #emoticon42\n",
    "            for em in glove_emoticons:\n",
    "                re_string = '\\s' + re.escape(em) + '\\s'\n",
    "                replace_string = ' ' + emoticon2string[em] + ' '\n",
    "                new_text = re.sub(re_string, replace_string, new_text)\n",
    "\n",
    "            # removing '&amp'\n",
    "            new_text = re.sub('&amp','', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars except for '# and @'\n",
    "            new_text = re.sub('[^\\w\\s@#]','', new_text)\n",
    "\n",
    "            # replace all @usernames with 'username'\n",
    "            new_text = re.sub('\\s@[^\\s]+',' username', new_text)  # middle\n",
    "            new_text = re.sub('^@[^\\s]+','username', new_text)    # start\n",
    "\n",
    "            # remove strings with '#' not on the beginning (to keep only hashtags)\n",
    "            new_text = re.sub('\\s[\\w]+#[\\w]*','', new_text)\n",
    "\n",
    "            # replace #hashtags with 'hashtag' and '#hashtag1 #hashtag2' with 'hashtags'\n",
    "            new_text = re.sub('#[^\\s]*\\s',' hashtag ', new_text)\n",
    "            new_text = re.sub('\\s#[^\\s]*$',' hashtag ', new_text)\n",
    "            new_text = re.sub('(\\s+hashtag){2,}', ' hashtags', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars\n",
    "            new_text = re.sub('[^\\w\\s]','', new_text)\n",
    "\n",
    "            # numbers fully made of digits\n",
    "            new_text = re.sub('\\s[\\d]+\\s','', new_text)\n",
    "\n",
    "            # remove words with only 1 character\n",
    "            new_text = re.sub('\\\\b\\\\w{1}\\\\b','', new_text)\n",
    "\n",
    "            # remove newline chars\n",
    "            new_text = new_text.replace('\\n', ' ')\n",
    "\n",
    "            # replace a multiple spaces with a single space\n",
    "            new_text = re.sub('\\s+',' ', new_text)\n",
    "\n",
    "            # using the lemmatizer\n",
    "            txt_list = nltk.word_tokenize(new_text)       # tokenise the tweet\n",
    "            POS = nltk.pos_tag(txt_list)                  # POS tag the tweet\n",
    "            WordNetPOS = [POSconvert(P[1]) for P in POS]  # convert POS tags to use in lemmatizer\n",
    "            for j in range(len(txt_list)):\n",
    "                word = txt_list[j]\n",
    "                lemmatized = lemmatizer.lemmatize(word, WordNetPOS[j])  # process each token/word one by one\n",
    "                if lemmatized in glove_emoticon_strings:                # replace the emoticon strings\n",
    "                    lemmatized = string2emoticon[lemmatized]\n",
    "                txt_list[j] = lemmatized                                # update the word in the txt_list\n",
    "\n",
    "            # UPDATE the dictionary\n",
    "            output_txt[id] = ' '.join(txt_list)\n",
    "            output[id] = txt_list\n",
    "\n",
    "        txt_dicts[i] = output_txt\n",
    "        txtlist_dicts.append(output)\n",
    "\n",
    "text_train = txt_dicts[0]\n",
    "text_test1 = txt_dicts[1]\n",
    "text_test2 = txt_dicts[2]\n",
    "text_test3 = txt_dicts[3]\n",
    "text_dev = txt_dicts[4]\n",
    "txtlist_train = txtlist_dicts[0]\n",
    "txtlist_test1 = txtlist_dicts[1]\n",
    "txtlist_test2 = txtlist_dicts[2]\n",
    "txtlist_test3 = txtlist_dicts[3]\n",
    "txtlist_dev = txtlist_dicts[4]\n",
    "\n",
    "# saving the preprocessed dictionaries as preprocessing-glove.pkl\n",
    "file_to_save = \"preprocessing-glove.pkl\"\n",
    "if not os.path.isfile(file_to_save):\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev, txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev]\n",
    "    with open(file_to_save, 'wb') as out_file:\n",
    "        pickle.dump(txt_dicts, out_file, protocol=-1)\n",
    "\n",
    "if len(datasets)==7:\n",
    "    text_train = txt_dicts[0]\n",
    "    text_test1 = txt_dicts[1]\n",
    "    text_test2 = txt_dicts[2]\n",
    "    text_test3 = txt_dicts[3]\n",
    "    text_test4 = txt_dicts[4]\n",
    "    text_test5 = txt_dicts[5]\n",
    "    text_dev = txt_dicts[6]\n",
    "    txtlist_train = txtlist_dicts[0]\n",
    "    txtlist_test1 = txtlist_dicts[1]\n",
    "    txtlist_test2 = txtlist_dicts[2]\n",
    "    txtlist_test3 = txtlist_dicts[3]\n",
    "    txtlist_test4 = txtlist_dicts[4]\n",
    "    txtlist_test5 = txtlist_dicts[5]\n",
    "    txtlist_dev = txtlist_dicts[6]\n",
    "\n",
    "    # saving preprocessing.pkl\n",
    "    file_to_save = \"preprocessing-glove7.pkl\"\n",
    "    if not os.path.isfile(file_to_save):\n",
    "        txt_dicts = [text_train, text_test1, text_test2, text_test3, text_test4, text_test5, text_dev, txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_test4, txtlist_test5, txtlist_dev]\n",
    "        with open(file_to_save, 'wb') as out_file:\n",
    "            pickle.dump(txt_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felt privilege to play foo fighter song on guitar today with one of the plectrum from the gig on saturday\n",
      "username pakistan may be an islamic country but der be lot true muslim in india who love their country and can sacrifice all for it\n",
      "happy birthday to the coolest golfer in bali username :) may you become cooler and cooler everyday stay humble little sister xx\n",
      "username tmills be go to tucson but the 29th and it on thursday\n",
      "hmmmmm where be the hashtag when matter like this rise kid be disgrace\n",
      "username all good im excite about 3rd season find home on netflix just want to make sure the reader have the news it develop\n",
      "told my mom want to stay in hotel for my 18th with people but my birthday on valentine lucky me\n",
      "1st thing do after baggage claim be get up to date with username bad blood ronna bev style make be home bit more okay\n",
      "bobby jindal want you to assimilate to heritage of the old confederacy even though his parent be from january dot dot dot\n",
      "username watch itthe 1st time votedit in the nta award best factual program mite needwatch rest on itvplayer\n",
      "username im sad that naruto manga be over and ill probably cry when the anime end kishimotosan be come to the in october\n",
      "feel down this monday check out this guy who dress baby prince george for week it might cheer you up\n",
      "huge play by hashtag to get out of the shadow of their own goal line on 2nd down lambert to sony for ayard gain very accurate pa\n",
      "kasich seek gop presidential nomination john kasich announce his run for the white house intuesday\n",
      "why do this horse have the same look of fear in it eye when next to marine le pen romney do when next to tru\n",
      "ben carson be consider for the same cabinet position jack bauer once have mister carson youre no jack bauer\n",
      "john kasichs tone of surrender may not have faze the biased buckeye audience but it certainly bother the gop base username\n",
      "hommage to gary carter and the expo day tomorrow at rcup dollar hot dog andtix at thelevel hashtag\n",
      "username take it you hear ric flair be at dreamwave in jan\n",
      "username yes mambetter google it you know some people dont have any work may create it big scenethiz happen for many celebtc\n",
      "colour hashtag be en vogue for luxury eyewear maker username\n",
      "username dublin this saturday get ta get on the guinness\n",
      "username antiimmigration sentiment be merely piece in bigger puzzle\n",
      "really like holly holm but be she ready for rousey thought\n",
      "which mean if andrade win she could get rousey next\n",
      "why do the medium keep give this brain dead moron air time former alaska gov sarah palin say sunday she\n",
      "username username myyr old cousin didnt know ice cube be rapperjust an superstar actor from the friday movie lol\n",
      "club remix next saturday night allstar will be in the building username username\n",
      "username username thats right lady im blame everything on kerry cleo this show may well eclipse the number of username\n",
      "cliff avril leave detroit lion game with back injury detroit lion defensive end cliff avril leave sunday ga\n",
      "make me sick to think we the american taxpayer will be pay for secret service detail for trump melania for\n",
      "username username bollock then theres ian brady myra hindley some bad hombre from that city see what do there\n",
      "my topare probably granny chiyo sakura sasori guy kissame the 2nd time and naruto pain\n",
      "the guy at dunkin just say see you tomorrow because he know that im there every day\n",
      "watch the lookalikes teaser on sunday brunch be the only one who think the david beckham lookalike be definitely not lookalike\n",
      "dont think chelsea should appeal for torres red card let sturridge play tomorrow it the carling cup not pl\n",
      "username in other related news you know theres no park tomorrow\n",
      "an older article cite the legal issue behind the dakota access pipeline hashtag\n",
      "today life lesson courtesy of alabama if youre gon na play in texas you get ta have fiddle in the band\n",
      "we people must do whatever it take to fight obamas lawlessness communist bent on destroy our heritage now not tomorrow\n",
      "you have to watch michael moore in trumplandi catch it on sho2\n",
      "my middle daughter just tell me 5th grade presentation in her class be on seth rollins hashtags\n",
      "mack sure you check out username and username this saturday username bodi\n",
      "uk release of star war episode vii the force awaken to be day earlier than expect dec\n",
      "nialls go to beindays no one gon na beill lock him in forever211 prepares squad cmon\n",
      "oracle set the date for it first quarter fiscal yearearnings announcement redwood shore ca mar\n",
      "username such close call cheer pal do well first month at ibm do miss the friday call though haha hope youre well\n",
      "username username sweet jesus hashtag\n",
      "username username well good for you\n",
      "phone to consider if you really want the galaxy note via username\n"
     ]
    }
   ],
   "source": [
    "# checking the preprocessed output\n",
    "for id in list(IDs_train.values())[0:50]:\n",
    "    print(text_train[id])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: Bag of words\n",
    "Bag of words vectorisation: please note that the following code can take up to 5-10 minutes to run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sparse processing.\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words (BoW) feature extraction - my implementation:\n",
    "\n",
    "if len(datasets)==5:\n",
    "    # loading preprocessed BoW file if it exists\n",
    "    file_to_load =  \"BOWsparse.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            temp = pickle.load(inp_file)\n",
    "            [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev, vocabulary] = temp\n",
    "else:\n",
    "    # loading preprocessing data\n",
    "    if len(datasets)==7:\n",
    "        file_to_load = \"preprocessing-plain.pkl\"\n",
    "        if os.path.isfile(file_to_load):\n",
    "            with open(file_to_load, 'rb') as inp_file:\n",
    "                t = pickle.load(inp_file)\n",
    "                text_train, text_test1, text_test2, text_test3, text_dev = t[0], t[1], t[2], t[3], t[4]\n",
    "                txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev = t[5], t[6], t[7], t[8], t[9]\n",
    "\n",
    "    if len(datasets)==7:\n",
    "        with open(\"preprocessing-plain7.pkl\", 'rb') as inp_file:\n",
    "            t = pickle.load(inp_file)\n",
    "            text_train, text_test1, text_test2, text_test3, text_test4, text_test5, text_dev = t[0], t[1], t[2], t[3], t[4], t[5], t[6]\n",
    "            txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_test4, txtlist_test5, txtlist_dev = t[7], t[8], t[9], t[10], t[11], t[12], t[13]\n",
    "\n",
    "    ## 1) removing stop words\n",
    "    stopwords = Stopwords.words('english')\n",
    "    stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "    ## 2) extracting the dictionary/vocabulary\n",
    "    freq = FreqDist()   # frequency distribution\n",
    "    txtlist_dicts = [txtlist_train, txtlist_dev]\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in tweet:\n",
    "                if not word in stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "    nums = range(1, len(freq.keys())+1)\n",
    "    vocabulary = list(freq.keys())              # creating the dictionary\n",
    "    vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out of vocabulary' word\n",
    "    vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "    vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words -> len: 69742\n",
    "\n",
    "    BOW_train = {}\n",
    "    for ID, tweet in txtlist_train.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "        BOW_train[ID] = BOW\n",
    "\n",
    "    BOW_test1 = {}\n",
    "    for ID, tweet in txtlist_test1.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "        BOW_test1[ID] = BOW\n",
    "\n",
    "    BOW_test2 = {}\n",
    "    for ID, tweet in txtlist_test2.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "        BOW_test2[ID] = BOW\n",
    "\n",
    "    BOW_test3 = {}\n",
    "    for ID, tweet in txtlist_test3.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "        BOW_test3[ID] = BOW\n",
    "\n",
    "    BOW_dev = {}\n",
    "    for ID, tweet in txtlist_dev.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "        BOW_dev[ID] = BOW\n",
    "\n",
    "    if len(datasets)==7:\n",
    "        BOW_test4 = {}\n",
    "        for ID, tweet in txtlist_test4.items():\n",
    "            BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "            BOW_test4[ID] = BOW\n",
    "\n",
    "        BOW_test5 = {}\n",
    "        for ID, tweet in txtlist_test5.items():\n",
    "            BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "            BOW_test5[ID] = BOW\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Starting sparse processing.\")\n",
    "\n",
    "    # sparse representation -> BOW vectors are sparse, so sparse representation saves memory and time\n",
    "    vector_list = [BOW_train[id] for id in BOW_train.keys()]\n",
    "    dense_train = np.vstack(vector_list)    # shape (45101, 67761)\n",
    "    sparse_train = csr_matrix(dense_train)  # getting the sparse matrix\n",
    "    del dense_train # remove from memory\n",
    "\n",
    "    vector_list = [BOW_test1[id] for id in BOW_test1.keys()]\n",
    "    dense_test1 = np.vstack(vector_list)    # shape (3531, 67761)\n",
    "    sparse_test1 = csr_matrix(dense_test1)  # getting the sparse matrix\n",
    "    del dense_test1 # remove from memory\n",
    "\n",
    "    vector_list = [BOW_test2[id] for id in BOW_test2.keys()]\n",
    "    dense_test2 = np.vstack(vector_list)    # shape (1853, 67761)\n",
    "    sparse_test2 = csr_matrix(dense_test2)  # getting the sparse matrix\n",
    "    del dense_test2 # remove from memory\n",
    "\n",
    "    vector_list = [BOW_test3[id] for id in BOW_test3.keys()]\n",
    "    dense_test3 = np.vstack(vector_list)    # shape (2379, 67761)\n",
    "    sparse_test3 = csr_matrix(dense_test3)  # getting the sparse matrix\n",
    "    del dense_test3 # remove from memory\n",
    "\n",
    "    vector_list = [BOW_dev[id] for id in BOW_dev.keys()]\n",
    "    dense_dev = np.vstack(vector_list)      # shape (2000, 67761)\n",
    "    sparse_dev = csr_matrix(dense_dev)      # getting the sparse matrix\n",
    "    del dense_dev # remove from memory\n",
    "\n",
    "    # train + dev together (combined)\n",
    "    vector_list1 = [BOW_train[id] for id in BOW_train.keys()]\n",
    "    vector_list2 = [BOW_dev[id] for id in BOW_dev.keys()]\n",
    "    temp1 = np.vstack(vector_list1)\n",
    "    temp2 = np.vstack(vector_list2)\n",
    "    dense_train_dev = np.vstack((temp1, temp2))     # shape (48632, 67761)\n",
    "    sparse_train_dev = csr_matrix(dense_train_dev)  # getting the sparse matrix\n",
    "    del dense_train_dev # remove from memory\n",
    "\n",
    "    if len(datasets)==7:\n",
    "        vector_list = [BOW_test4[id] for id in BOW_test4.keys()]\n",
    "        dense_test4 = np.vstack(vector_list)    # shape (1853, 67761)\n",
    "        sparse_test4 = csr_matrix(dense_test4)  #4getting the sparse matrix\n",
    "        del dense_test4 # remove from memory\n",
    "\n",
    "        vector_list = [BOW_test5[id] for id in BOW_test5.keys()]\n",
    "        dense_test5 = np.vstack(vector_list)    # shape (2379, 67761)\n",
    "        sparse_test5 = csr_matrix(dense_test5)  # getting the sparse matrix\n",
    "        del dense_test5 # remove from memory\n",
    "\n",
    "\n",
    "# save the sparse representation\n",
    "file_to_save = \"BOWsparse.pkl\"\n",
    "if not os.path.isfile(file_to_save):\n",
    "    sparse_dicts = [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev, vocabulary]\n",
    "    with open(file_to_save, 'wb') as out_file:\n",
    "        pickle.dump(sparse_dicts, out_file, protocol=-1)\n",
    "\n",
    "if len(datasets)==7:\n",
    "    file_to_save = \"BOWsparse7.pkl\"\n",
    "    sparse_dicts = [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_test4, sparse_test5, sparse_dev, sparse_train_dev, vocabulary]\n",
    "    with open(file_to_save, 'wb') as out_file:\n",
    "        pickle.dump(sparse_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: TF-IDF weighted Bag of words\n",
    "Weighted BOW vectorisation - each word in a tweet is weighted according to its TFIDF\n",
    "Please note that the following code can take up to 5-10 minutes to run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sparse processing.\n"
     ]
    }
   ],
   "source": [
    "# TFIDF feature extraction\n",
    "\n",
    "if len(datasets)==5:\n",
    "    # loading preprocessed BoW file if it exists\n",
    "    file_to_load =  \"TFIDFsparse.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            temp = pickle.load(inp_file)\n",
    "            [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev, vocabulary] = temp\n",
    "else:\n",
    "    # loading preprocessing data\n",
    "    file_to_load = \"preprocessing-plain.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            t = pickle.load(inp_file)\n",
    "            text_train, text_test1, text_test2, text_test3, text_dev = t[0], t[1], t[2], t[3], t[4]\n",
    "            txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev = t[5], t[6], t[7], t[8], t[9]\n",
    "\n",
    "    if len(datasets)==7:\n",
    "        with open(\"preprocessing-plain7.pkl\", 'rb') as inp_file:\n",
    "            t = pickle.load(inp_file)\n",
    "            text_train, text_test1, text_test2, text_test3, text_test4, text_test5, text_dev = t[0], t[1], t[2], t[3], t[4], t[5], t[6]\n",
    "            txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_test4, txtlist_test5, txtlist_dev = t[7], t[8], t[9], t[10], t[11], t[12], t[13]\n",
    "\n",
    "    # extracting the dictionary\n",
    "    freq = FreqDist()   # frequency distribution\n",
    "    txtlist_dicts = [txtlist_train, txtlist_dev]\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in tweet:\n",
    "                if not word in stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "    nums = range(1,len(freq.keys())+1)\n",
    "    vocabulary = list(freq.keys())              # creating the dictionary\n",
    "    vocabulary_array = np.array(vocabulary)     # np.array of the dictionary\n",
    "    vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out-of-vocabulary' word\n",
    "    vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "    vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words\n",
    "\n",
    "    # extracting the dictionary\n",
    "    DFfreq = FreqDist()   # document frequency distribution\n",
    "    Ntexts = len(IDs_train) + len(IDs_dev)\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in np.unique(tweet):\n",
    "                if not word in stopwords:\n",
    "                    DFfreq[word] += 1\n",
    "\n",
    "\n",
    "    # TFIDF-weighted Bag of Words for each tweet\n",
    "    TFIDF_train = {}\n",
    "    for ID, tweet in txtlist_train.items():\n",
    "        tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "        TFIDF_train[ID] = tfidf\n",
    "\n",
    "    TFIDF_test1 = {}\n",
    "    for ID, tweet in txtlist_test1.items():\n",
    "        tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "        TFIDF_test1[ID] = tfidf\n",
    "\n",
    "    TFIDF_test2 = {}\n",
    "    for ID, tweet in txtlist_test2.items():\n",
    "        tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "        TFIDF_test2[ID] = tfidf\n",
    "\n",
    "    TFIDF_test3 = {}\n",
    "    for ID, tweet in txtlist_test3.items():\n",
    "        tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "        TFIDF_test3[ID] = tfidf\n",
    "\n",
    "    TFIDF_dev = {}\n",
    "    for ID, tweet in txtlist_dev.items():\n",
    "        tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "        TFIDF_dev[ID] = tfidf\n",
    "\n",
    "    if len(datasets)==7:\n",
    "        TFIDF_test4 = {}\n",
    "        for ID, tweet in txtlist_test4.items():\n",
    "            tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "            TFIDF_test4[ID] = tfidf\n",
    "\n",
    "        TFIDF_test5 = {}\n",
    "        for ID, tweet in txtlist_test5.items():\n",
    "            tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "            TFIDF_test5[ID] = tfidf\n",
    "\n",
    "\n",
    "    print(\"Starting sparse processing.\")\n",
    "\n",
    "    # sparse TFIDF representation\n",
    "    vector_list = [TFIDF_train[id] for id in TFIDF_train.keys()]\n",
    "    TFIDFdense_train = np.vstack(vector_list)\n",
    "    TFIDFsparse_train = csr_matrix(TFIDFdense_train)  # getting the sparse matrix\n",
    "    del TFIDFdense_train # remove from memory\n",
    "\n",
    "    vector_list = [TFIDF_test1[id] for id in TFIDF_test1.keys()]\n",
    "    TFIDFdense_test1 = np.vstack(vector_list)\n",
    "    TFIDFsparse_test1 = csr_matrix(TFIDFdense_test1)  # getting the sparse matrix\n",
    "    del TFIDFdense_test1 # remove from memory\n",
    "\n",
    "    vector_list = [TFIDF_test2[id] for id in TFIDF_test2.keys()]\n",
    "    TFIDFdense_test2 = np.vstack(vector_list)\n",
    "    TFIDFsparse_test2 = csr_matrix(TFIDFdense_test2)  # getting the sparse matrix\n",
    "    del TFIDFdense_test2 # remove from memory\n",
    "\n",
    "    vector_list = [TFIDF_test3[id] for id in TFIDF_test3.keys()]\n",
    "    TFIDFdense_test3 = np.vstack(vector_list)\n",
    "    TFIDFsparse_test3 = csr_matrix(TFIDFdense_test3)  # getting the sparse matrix\n",
    "    del TFIDFdense_test3 # remove from memory\n",
    "\n",
    "    vector_list = [TFIDF_dev[id] for id in TFIDF_dev.keys()]\n",
    "    TFIDFdense_dev = np.vstack(vector_list)\n",
    "    TFIDFsparse_dev = csr_matrix(TFIDFdense_dev)      # getting the sparse matrix\n",
    "    del TFIDFdense_dev # remove from memory\n",
    "\n",
    "    vector_list1 = [TFIDF_train[id] for id in TFIDF_train.keys()]\n",
    "    vector_list2 = [TFIDF_dev[id] for id in TFIDF_dev.keys()]\n",
    "    temp1 = np.vstack(vector_list1)\n",
    "    temp2 = np.vstack(vector_list2)\n",
    "    TFIDFdense_train_dev = np.vstack((temp1, temp2))  # shape (45101, 59559)\n",
    "    TFIDFsparse_train_dev = csr_matrix(TFIDFdense_train_dev)  # getting the sparse matrix\n",
    "    del TFIDFdense_train_dev # remove from memory\n",
    "\n",
    "\n",
    "    # save the sparse representation of TFIDF features\n",
    "    if len(datasets)==5:\n",
    "        file_to_save = \"TFIDFsparse.pkl\"\n",
    "        if not os.path.isfile(file_to_save):\n",
    "            sparse_dicts = [TFIDFsparse_train, TFIDFsparse_test1, TFIDFsparse_test2, TFIDFsparse_test3, TFIDFsparse_dev, TFIDFsparse_train_dev, vocabulary]\n",
    "            with open(file_to_save, 'wb') as out_file:\n",
    "                pickle.dump(sparse_dicts, out_file, protocol=-1)\n",
    "\n",
    "    if len(datasets)==7:\n",
    "\n",
    "        vector_list = [TFIDF_test4[id] for id in TFIDF_test4.keys()]\n",
    "        TFIDFdense_test4 = np.vstack(vector_list)\n",
    "        TFIDFsparse_test4 = csr_matrix(TFIDFdense_test4)  # getting the sparse matrix\n",
    "        del TFIDFdense_test4 # remove from memory\n",
    "\n",
    "        vector_list = [TFIDF_test5[id] for id in TFIDF_test5.keys()]\n",
    "        TFIDFdense_test5 = np.vstack(vector_list)\n",
    "        TFIDFsparse_test5 = csr_matrix(TFIDFdense_test5)  # getting the sparse matrix\n",
    "        del TFIDFdense_test5 # remove from memory\n",
    "\n",
    "        file_to_save = \"TFIDFsparse7.pkl\"\n",
    "        sparse_dicts = [TFIDFsparse_train, TFIDFsparse_test1, TFIDFsparse_test2, TFIDFsparse_test3, TFIDFsparse_test4, TFIDFsparse_test5, TFIDFsparse_dev, TFIDFsparse_train_dev, vocabulary]\n",
    "        with open(file_to_save, 'wb') as out_file:\n",
    "            pickle.dump(sparse_dicts, out_file, protocol=-1)\n",
    "\n",
    "# loading preprocessed TFIDF sparse data\n",
    "# with open(\"TFIDFsparse.pkl\", 'rb') as inp_file:\n",
    "#     temp = pickle.load(inp_file)\n",
    "#     [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev] = temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: GloVe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the word vectors.\n"
     ]
    }
   ],
   "source": [
    "# APPROACH 1: Loading the word embeddings vectors from GloVE: selecting 5000 words based on frequency\n",
    "\n",
    "print('Extracting the word vectors.')\n",
    "\n",
    "## loading preprocessed embedding file if it exists (embedding matrix, word to index map, embedding dictionary)\n",
    "file_to_load =  \"embeddings.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(file_to_load, 'rb') as inp_file:\n",
    "        temp = pickle.load(inp_file)\n",
    "        [embedding_matrix, word2ID, embedding_dict] = temp\n",
    "else:\n",
    "    ## loading preprocessed data for glove\n",
    "    file_to_load = \"preprocessing-glove.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            t = pickle.load(inp_file)\n",
    "            if len(datasets)==5:\n",
    "                txtlist_train, txtlist_dev = t[5], t[9]\n",
    "            if len(datasets)==7:\n",
    "                txtlist_train, txtlist_dev = t[7], t[13]\n",
    "\n",
    "    full_embedding_dict = {}\n",
    "    glove_path = join('..','glove', 'glove.6B.100d.txt')\n",
    "    with open(glove_path, 'r', encoding='utf-8') as File:\n",
    "        for line in File:\n",
    "            vec = line.split()\n",
    "            word = vec[0]\n",
    "            coefs = np.asarray(vec[1:], dtype='float32')\n",
    "            full_embedding_dict[word] = coefs\n",
    "\n",
    "    print(f\"Extracted {len(full_embedding_dict)} word embedding vectors.\")\n",
    "\n",
    "    ## 1) finding the frequency weights\n",
    "    txtlist_dicts = [txtlist_train, txtlist_dev]\n",
    "    freq = FreqDist()   # frequency distribution\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in tweet:\n",
    "                if not word in stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "    nums = range(1, len(freq.keys())+1)\n",
    "    vocabulary = list(freq.keys())              # creating the dictionary\n",
    "    vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "    num2vocab = dict(zip(nums, vocabulary))     # word to index mapping\n",
    "\n",
    "    sorted_vocabulary = sorted([it for it in freq.items()], key=lambda data: data[1], reverse=True)\n",
    "    full_vocabulary = [ tup[0] for tup in sorted_vocabulary ]\n",
    "\n",
    "    embedding_dict = {}  # word embeddings of 6000 words from vocabulary\n",
    "    temp = 0\n",
    "    for word in full_vocabulary:\n",
    "        if word in full_embedding_dict.keys():\n",
    "            embedding_dict[word] = full_embedding_dict[word]\n",
    "            temp += 1\n",
    "        if temp == 4998:\n",
    "            break\n",
    "    vocabulary5000 = list(embedding_dict.keys())  # obtain the dictionary of 6000 most common words\n",
    "\n",
    "    print(f\"Created dictionary of {len(embedding_dict)} most common words.\")\n",
    "\n",
    "    ## extract the <OOV> vector by setting it to be the weighted avg of unused words\n",
    "    Total = np.zeros(100)\n",
    "    Sum = 0\n",
    "    for word in full_vocabulary:\n",
    "        if word not in vocabulary5000:                          # if word is not among 6000 words\n",
    "            if word in full_embedding_dict.keys():              # and it is in glove\n",
    "                Total += freq[word] * full_embedding_dict[word] # take the weighted avg\n",
    "                Sum += freq[word]\n",
    "    OOV_vector = Total / Sum\n",
    "    embedding_dict['<OOV>'] = OOV_vector\n",
    "\n",
    "    print(f\"The embedding dictionary has {len(embedding_dict)} words, the last one is: {list(embedding_dict.keys())[-1]}\")\n",
    "\n",
    "    ## Build an embedding matrix\n",
    "    word_list = list(embedding_dict.keys())\n",
    "    nums = range(1,len(word_list)+1)\n",
    "    word2ID = dict(zip(word_list, nums))     # the index of the embedding vector\n",
    "    num2vocab = dict(zip(nums, word_list))   # the index to word\n",
    "    vector_list = [embedding_dict[word] for word in word_list]\n",
    "\n",
    "    embedding_matrix = np.vstack(vector_list)\n",
    "    embedding_matrix = np.vstack((np.zeros(100), embedding_matrix))\n",
    "\n",
    "    print(f\"Created matrix with shape {embedding_matrix.shape}\")  # the first row is a dummy row\n",
    "    del full_embedding_dict # delete from memory\n",
    "    ## save the embeddings\n",
    "    with open(\"embeddings.pkl\", 'wb') as out_file:\n",
    "        temp = [embedding_matrix, word2ID, embedding_dict]\n",
    "        pickle.dump(temp, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# ALTERNATIVE APPROACH to preprocessing of embeddings: Weighted Approach - considering both semantic and frequency value\n",
    "# -> in the first approach, only frequency was the criterion to select 5000 words\n",
    "# -> but that way, we may select words which have minimal semantic value\n",
    "# -> the weighted approach will select both the semantic as well as frequency contribution\n",
    "\n",
    "\n",
    "# loading preprocessed embedding file if it exists (embedding matrix, word to index map, embedding dictionary)\n",
    "file_to_load =  \"embeddings-weighted.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(\"embeddings-weighted.pkl\", 'rb') as inp_file:\n",
    "        temp = pickle.load(inp_file)\n",
    "        [embedding_matrix, word2ID, embedding_dict] = temp\n",
    "else:\n",
    "    ## 0) loading preprocessing data for glove\n",
    "    file_to_load = \"preprocessing-glove.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            if len(datasets)==5:\n",
    "                txtlist_train, txtlist_dev = t[5], t[9]\n",
    "            if len(datasets)==7:\n",
    "                txtlist_train, txtlist_dev = t[7], t[13]\n",
    "\n",
    "    ## 1) finding the frequency weights\n",
    "    txtlist_dicts = [txtlist_train, txtlist_dev]\n",
    "    freq = FreqDist()   # frequency distribution\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in tweet:\n",
    "                if not word in stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "    nums = range(1, len(freq.keys())+1)\n",
    "    vocabulary = list(freq.keys())              # creating the dictionary\n",
    "    vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "    num2vocab = dict(zip(nums, vocabulary))     # word to index mapping\n",
    "\n",
    "    sorted_vocabulary = sorted([it for it in freq.items()], key=lambda data: data[1], reverse=True)\n",
    "    maxF = sorted_vocabulary[0][1]  # maximal frequency of a word\n",
    "    normalized_vocabulary = { word: f / maxF for (word, f) in sorted_vocabulary}  # dictionary\n",
    "\n",
    "    ## 2) use SVM coeffs (make sure to have 'SVMcoefficients.pkl' which stores clf.coef_ from SVM)\n",
    "    with open(\"SVMcoefficients.pkl\", 'rb') as inp_file:\n",
    "        SVM_coef = pickle.load(inp_file)\n",
    "\n",
    "    with open(\"BOWsparse.pkl\", 'rb') as inp_file:\n",
    "        [_, _, _, _, _, _, vocabularyBOW] = pickle.load(inp_file)\n",
    "\n",
    "    coefs = []\n",
    "    coefs_nums = []\n",
    "    for i, word in enumerate(vocabularyBOW):\n",
    "        c = np.abs(SVM_coef[2,i] - SVM_coef[0,i])  # coefficent: absolute difference between positive and negative:\n",
    "        word_coef = (word, c)                            #  ->> \"most negative/most positive\" words have higher index\n",
    "        coefs_nums.append(c)\n",
    "        coefs.append(word_coef)\n",
    "    minC, maxC = np.min(coefs_nums), np.max(coefs_nums)\n",
    "    coefs = [(word, (c - minC) / maxC) for (word, c) in coefs if word in vocabulary]     # normalize coefs\n",
    "    coefs = sorted(coefs, key=lambda data: data[1], reverse=True)  # sort coefs\n",
    "\n",
    "    ## 3) apply the weight formula: 2 * freq_value + 1 * sentiment_value ~ 2 f + 1 s\n",
    "    weighted_scores = [(word, 1 * cn + 2 * normalized_vocabulary[word]) for (word, cn) in coefs]  # list of tuples\n",
    "    sorted_weighted_vocabulary = sorted([it for it in weighted_scores], key=lambda data: data[1], reverse=True)\n",
    "\n",
    "    ## 4) load GloVe embeddings\n",
    "    full_embedding_dict = {}\n",
    "    glove_path = join('..','glove', 'glove.6B.100d.txt')\n",
    "    with open(glove_path, 'r', encoding='utf-8') as File:\n",
    "        for line in File:\n",
    "            vec = line.split()\n",
    "            word = vec[0]\n",
    "            cs = np.asarray(vec[1:], dtype='float32')\n",
    "            full_embedding_dict[word] = cs\n",
    "    print(f\"Extracted {len(full_embedding_dict)} word embedding vectors.\")\n",
    "\n",
    "    ## 5) extract the word embeddings of 5000 words based on the order from the weighted approach\n",
    "    embedding_dict = {}\n",
    "    temp = 0\n",
    "    for (word,_) in sorted_weighted_vocabulary:\n",
    "        if word in full_embedding_dict.keys():\n",
    "            embedding_dict[word] = full_embedding_dict[word]\n",
    "            temp += 1\n",
    "        if temp == 4998:\n",
    "            break\n",
    "    vocabulary5000 = list(embedding_dict.keys())  # obtain the dictionary of 5000 most common words\n",
    "\n",
    "    ## 6) extract the <OOV> vector by setting it to be the weighted avg of unused words\n",
    "    Total = np.zeros(100)\n",
    "    Sum = 0\n",
    "    for word in full_vocabulary:\n",
    "        if word not in vocabulary5000:                          # if word is not among 5000 words\n",
    "            if word in full_embedding_dict.keys():              # and it is in glove\n",
    "                Total += freq[word] * full_embedding_dict[word] # take the weighted avg\n",
    "                Sum += freq[word]\n",
    "    OOV_vector = Total / Sum\n",
    "    embedding_dict['<OOV>'] = OOV_vector\n",
    "\n",
    "    ## 7) Build the weighted-embedding matrix\n",
    "    word_list = list(embedding_dict.keys())\n",
    "    nums = range(1,len(word_list)+1)\n",
    "    word2ID = dict(zip(word_list, nums))     # the index of the embedding vector\n",
    "    num2vocab = dict(zip(nums, word_list))   # the index to word\n",
    "    vector_list = [embedding_dict[word] for word in word_list]\n",
    "    embedding_matrix = np.vstack(vector_list)\n",
    "    embedding_matrix = np.vstack((np.zeros(100), embedding_matrix))\n",
    "\n",
    "    print(f\"Created matrix with shape {embedding_matrix.shape}\")  # the first row is a dummy row\n",
    "    del full_embedding_dict # delete from memory\n",
    "\n",
    "    # save the embeddings\n",
    "    with open(\"embeddings-weighted.pkl\", 'wb') as out_file:\n",
    "        temp = [embedding_matrix, word2ID, embedding_dict]\n",
    "        pickle.dump(temp, out_file, protocol=-1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrices for torch.\n"
     ]
    }
   ],
   "source": [
    "# preparing matrices for torch: matrix_train, matrix_dev, matrix_test1, matrix_test2, matrix_test3\n",
    "\n",
    "# loading preprocessed embeddings\n",
    "embedding_file = \"embeddings-weighted.pkl\"  # alternatively use \"embeddings.pkl\"\n",
    "with open(embedding_file, 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [embedding_matrix, word2ID, embedding_dict] = temp\n",
    "\n",
    "# converting the text lists into vectors of ints\n",
    "word_list = list(embedding_dict.keys())\n",
    "if len(datasets)==5:\n",
    "    with open(\"preprocessing-glove.pkl\", 'rb') as inp_file:   # loading preprocessed data for glove\n",
    "        t = pickle.load(inp_file)\n",
    "        txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev = t[5], t[6], t[7], t[8], t[9]\n",
    "    with open(\"preprocessing-sent-ID.pkl\", 'rb') as inp_file: # loading preprocessed data\n",
    "        [sent_train, sent_test1, sent_test2, sent_test3, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev] = pickle.load(inp_file)\n",
    "\n",
    "if len(datasets)==7:\n",
    "    with open(\"preprocessing-glove7.pkl\", 'rb') as inp_file:   # loading preprocessed data for glove\n",
    "        t = pickle.load(inp_file)\n",
    "        txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_test4, txtlist_test5, txtlist_dev = t[7], t[8], t[9], t[10], t[11], t[12], t[13]\n",
    "    with open(\"preprocessing-sent-ID7.pkl\", 'rb') as inp_file: # loading preprocessed data\n",
    "        [sent_train, sent_test1, sent_test2, sent_test3, sent_test4,  sent_test5, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_test4, IDs_test5, IDs_dev] = pickle.load(inp_file)\n",
    "\n",
    "\n",
    "\n",
    "max_len = np.max([len(tweet) for tweet in txtlist_train.values()])      # longest tokenized sentence\n",
    "matrix_train = np.zeros((len(txtlist_train), max_len), dtype=np.int16)  # training datapoints\n",
    "y_train = np.zeros(len(txtlist_train), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_train.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_train[id]\n",
    "    matrix_train[i] = x\n",
    "    y_train[i] = sent2num(y)\n",
    "\n",
    "matrix_test1 = np.zeros((len(txtlist_test1), max_len), dtype=np.int16)  # training datapoints\n",
    "y_test1 = np.zeros(len(txtlist_test1), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_test1.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_test1[id]\n",
    "    matrix_test1[i] = x\n",
    "    y_test1[i] = sent2num(y)\n",
    "\n",
    "matrix_test2 = np.zeros((len(txtlist_test2), max_len), dtype=np.int16)  # training datapoints\n",
    "y_test2 = np.zeros(len(txtlist_test2), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_test2.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_test2[id]\n",
    "    matrix_test2[i] = x\n",
    "    y_test2[i] = sent2num(y)\n",
    "\n",
    "matrix_test3 = np.zeros((len(txtlist_test3), max_len), dtype=np.int16)  # training datapoints\n",
    "y_test3 = np.zeros(len(txtlist_test3), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_test3.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_test3[id]\n",
    "    matrix_test3[i] = x\n",
    "    y_test3[i] = sent2num(y)\n",
    "\n",
    "matrix_dev = np.zeros((len(txtlist_dev), max_len), dtype=np.int16)  # training datapoints\n",
    "y_dev = np.zeros(len(txtlist_dev), dtype=np.int8)  # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_dev.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_dev[id]\n",
    "    matrix_dev[i] = x\n",
    "    y_dev[i] = sent2num(y)\n",
    "\n",
    "if len(datasets)==7:\n",
    "    matrix_test4 = np.zeros((len(txtlist_test4), max_len), dtype=np.int16)  # training datapoints\n",
    "    y_test4 = np.zeros(len(txtlist_test4), dtype=np.int8)                   # training labels\n",
    "    for i, (id, text_list) in enumerate(txtlist_test4.items()):\n",
    "        x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "        y = sent_test4[id]\n",
    "        matrix_test4[i] = x\n",
    "        y_test4[i] = sent2num(y)\n",
    "\n",
    "    matrix_test5 = np.zeros((len(txtlist_test5), max_len), dtype=np.int16)  # training datapoints\n",
    "    y_test5 = np.zeros(len(txtlist_test5), dtype=np.int8)                   # training labels\n",
    "    for i, (id, text_list) in enumerate(txtlist_test5.items()):\n",
    "        x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "        y = sent_test5[id]\n",
    "        matrix_test5[i] = x\n",
    "        y_test5[i] = sent2num(y)\n",
    "\n",
    "# save the data as pickle files\n",
    "with open(\"xy_train.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_train, y_train]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_dev.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_dev, y_dev]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_test1.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_test1, y_test1]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_test2.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_test2, y_test2]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_test3.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_test3, y_test3]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "if len(datasets)==7:\n",
    "    with open(\"xy_test4.pkl\", 'wb') as out_file:\n",
    "        temp = [matrix_test4, y_test4]\n",
    "        pickle.dump(temp, out_file, protocol=-1)\n",
    "    with open(\"xy_test5.pkl\", 'wb') as out_file:\n",
    "        temp = [matrix_test5, y_test5]\n",
    "        pickle.dump(temp, out_file, protocol=-1)\n",
    "\n",
    "# load the saved data from pickle files\n",
    "with open(\"xy_train.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_train, y_train] = temp\n",
    "with open(\"xy_dev.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_dev, y_dev] = temp\n",
    "with open(\"xy_test1.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_test1, y_test1] = temp\n",
    "with open(\"xy_test2.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_test2, y_test2] = temp\n",
    "with open(\"xy_test3.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_test3, y_test3] = temp\n",
    "\n",
    "if len(datasets)==7:\n",
    "    with open(\"xy_test4.pkl\", 'rb') as inp_file:\n",
    "        temp = pickle.load(inp_file)\n",
    "        [matrix_test4, y_test4] = temp\n",
    "    with open(\"xy_test5.pkl\", 'rb') as inp_file:\n",
    "        temp = pickle.load(inp_file)\n",
    "        [matrix_test5, y_test5] = temp\n",
    "\n",
    "print(\"Saved matrices for torch.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Sentiment Classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "<2379x67761 sparse matrix of type '<class 'numpy.float64'>'\n\twith 24472 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> NEARESTNEIGHBOUR: \n",
      "../semeval-tweets/twitter-test1.txt (BOW-NearestNeighbour): 0.386\n",
      "../semeval-tweets/twitter-test2.txt (BOW-NearestNeighbour): 0.400\n",
      "../semeval-tweets/twitter-test3.txt (BOW-NearestNeighbour): 0.376\n",
      "../semeval-tweets/twitter-test4.txt (BOW-NearestNeighbour): 0.400\n",
      "../semeval-tweets/twitter-test5.txt (BOW-NearestNeighbour): 0.376\n",
      "\t\t\t\t\t\t\t\tTraining+eval time: 25.101\n",
      "--> NEARESTNEIGHBOUR: \n",
      "../semeval-tweets/twitter-test1.txt (TFIDF-NearestNeighbour): 0.404\n",
      "../semeval-tweets/twitter-test2.txt (TFIDF-NearestNeighbour): 0.439\n",
      "../semeval-tweets/twitter-test3.txt (TFIDF-NearestNeighbour): 0.415\n",
      "../semeval-tweets/twitter-test4.txt (TFIDF-NearestNeighbour): 0.439\n",
      "../semeval-tweets/twitter-test5.txt (TFIDF-NearestNeighbour): 0.415\n",
      "\t\t\t\t\t\t\t\tTraining+eval time: 24.209\n",
      "--> NAIVEBAYES: \n",
      "../semeval-tweets/twitter-test1.txt (BOW-NaiveBayes): 0.529\n",
      "../semeval-tweets/twitter-test2.txt (BOW-NaiveBayes): 0.529\n",
      "../semeval-tweets/twitter-test3.txt (BOW-NaiveBayes): 0.509\n",
      "../semeval-tweets/twitter-test4.txt (BOW-NaiveBayes): 0.529\n",
      "../semeval-tweets/twitter-test5.txt (BOW-NaiveBayes): 0.509\n",
      "\t\t\t\t\t\t\t\tTraining+eval time: 0.058\n",
      "--> NAIVEBAYES: \n",
      "../semeval-tweets/twitter-test1.txt (TFIDF-NaiveBayes): 0.507\n",
      "../semeval-tweets/twitter-test2.txt (TFIDF-NaiveBayes): 0.500\n",
      "../semeval-tweets/twitter-test3.txt (TFIDF-NaiveBayes): 0.484\n",
      "../semeval-tweets/twitter-test4.txt (TFIDF-NaiveBayes): 0.500\n",
      "../semeval-tweets/twitter-test5.txt (TFIDF-NaiveBayes): 0.484\n",
      "\t\t\t\t\t\t\t\tTraining+eval time: 0.052\n",
      "--> SVM\n",
      "../semeval-tweets/twitter-test1.txt (BOW-SVM): 0.589\n",
      "../semeval-tweets/twitter-test2.txt (BOW-SVM): 0.598\n",
      "../semeval-tweets/twitter-test3.txt (BOW-SVM): 0.543\n",
      "../semeval-tweets/twitter-test4.txt (BOW-SVM): 0.598\n",
      "../semeval-tweets/twitter-test5.txt (BOW-SVM): 0.543\n",
      "\t\t\t\t\t\t\t\tTraining+eval time: 0.307\n",
      "--> SVM\n",
      "../semeval-tweets/twitter-test1.txt (TFIDF-SVM): 0.586\n",
      "../semeval-tweets/twitter-test2.txt (TFIDF-SVM): 0.587\n",
      "../semeval-tweets/twitter-test3.txt (TFIDF-SVM): 0.541\n",
      "../semeval-tweets/twitter-test4.txt (TFIDF-SVM): 0.587\n",
      "../semeval-tweets/twitter-test5.txt (TFIDF-SVM): 0.541\n",
      "\t\t\t\t\t\t\t\tTraining+eval time: 0.352\n",
      "--> MAXENT\n",
      "../semeval-tweets/twitter-test1.txt (BOW-MaxEnt): 0.607\n",
      "../semeval-tweets/twitter-test2.txt (BOW-MaxEnt): 0.626\n",
      "../semeval-tweets/twitter-test3.txt (BOW-MaxEnt): 0.561\n",
      "../semeval-tweets/twitter-test4.txt (BOW-MaxEnt): 0.626\n",
      "../semeval-tweets/twitter-test5.txt (BOW-MaxEnt): 0.561\n",
      "\t\t\t\t\t\t\t\tTraining+eval time: 10.173\n",
      "--> MAXENT\n",
      "../semeval-tweets/twitter-test1.txt (TFIDF-MaxEnt): 0.616\n",
      "../semeval-tweets/twitter-test2.txt (TFIDF-MaxEnt): 0.626\n",
      "../semeval-tweets/twitter-test3.txt (TFIDF-MaxEnt): 0.570\n",
      "../semeval-tweets/twitter-test4.txt (TFIDF-MaxEnt): 0.626\n",
      "../semeval-tweets/twitter-test5.txt (TFIDF-MaxEnt): 0.570\n",
      "\t\t\t\t\t\t\t\tTraining+eval time: 18.424\n",
      "--> LSTM\n",
      "../semeval-tweets/twitter-test1.txt (LSTM): 0.592\n",
      "../semeval-tweets/twitter-test2.txt (LSTM): 0.602\n",
      "../semeval-tweets/twitter-test3.txt (LSTM): 0.542\n",
      "../semeval-tweets/twitter-test4.txt (LSTM): 0.617\n",
      "../semeval-tweets/twitter-test5.txt (LSTM): 0.535\n",
      "--> LSTM-HIDDEN\n",
      "../semeval-tweets/twitter-test1.txt (LSTM-hidden): 0.588\n",
      "../semeval-tweets/twitter-test2.txt (LSTM-hidden): 0.622\n",
      "../semeval-tweets/twitter-test3.txt (LSTM-hidden): 0.561\n",
      "../semeval-tweets/twitter-test4.txt (LSTM-hidden): 0.617\n",
      "../semeval-tweets/twitter-test5.txt (LSTM-hidden): 0.535\n"
     ]
    }
   ],
   "source": [
    "# Build and evaluate traditional and LSTM sentiment classifiers.\n",
    "\n",
    "if len(datasets)==5:\n",
    "    with open(\"preprocessing-sent-ID.pkl\", 'rb') as inp_file: # loading preprocessed data\n",
    "        [sent_train, sent_test1, sent_test2, sent_test3, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev] = pickle.load(inp_file)\n",
    "if len(datasets)==7:\n",
    "    with open(\"preprocessing-sent-ID7.pkl\", 'rb') as inp_file: # loading preprocessed data\n",
    "        [sent_train, sent_test1, sent_test2, sent_test3, sent_test4,  sent_test5, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_test4, IDs_test5, IDs_dev] = pickle.load(inp_file)\n",
    "\n",
    "\n",
    "for classifier in ['NearestNeighbour', 'NaiveBayes', 'SVM', 'MaxEnt', 'LSTM', 'LSTM-hidden']:#, 'LSTM-hidden', 'LSTM-10K']:\n",
    "    for features in ['BOW', 'TFIDF']:\n",
    "\n",
    "        if features == 'BOW':\n",
    "            if len(datasets)==5:\n",
    "                with open(\"BOWsparse.pkl\", 'rb') as inp_file:\n",
    "                    [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev, vocabulary]  = pickle.load(inp_file)\n",
    "            if len(datasets)==7:\n",
    "                with open(\"BOWsparse7.pkl\", 'rb') as inp_file:\n",
    "                    [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_test4, sparse_test5, sparse_dev, sparse_train_dev, vocabulary] = pickle.load(inp_file)\n",
    "\n",
    "\n",
    "            Xtrain = sparse_train_dev                      # combining the two datasets\n",
    "            ID_train = list(IDs_train.values())            # list of IDs in train set\n",
    "            ID_dev = list(IDs_dev.values())                # list of IDs in dev set\n",
    "            ID_train_dev = ID_train + ID_dev               # combined train, dev\n",
    "            sn_train = [sent_train[id] for id in ID_train] # training labels train\n",
    "            sn_dev = [sent_dev[id] for id in ID_dev]       # training labels dev\n",
    "            Ytrain = np.array(sn_train + sn_dev)           # combining both labels\n",
    "            Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])                      # numerical labels for train\n",
    "            y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])  # numerical labels for test1\n",
    "            y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])  # numerical labels for test2\n",
    "            y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])  # numerical labels for test3\n",
    "            testset_sparse_samples = [sparse_test1, sparse_test2, sparse_test3]           # list of testing examples\n",
    "            testset_IDs = [list(IDs_test1.values()), list(IDs_test2.values()), list(IDs_test3.values())]  # IDs in test1, test2, test3\n",
    "\n",
    "            if len(datasets)==7:\n",
    "                y_test4_numeric = np.array([sent2num(sent) for sent in sent_test4.values()])  # numerical labels for test4\n",
    "                y_test5_numeric = np.array([sent2num(sent) for sent in sent_test5.values()])  # numerical labels for test5\n",
    "                testset_sparse_samples = [sparse_test1, sparse_test2, sparse_test3, sparse_test4, sparse_test5]\n",
    "                testset_IDs = [list(IDs_test1.values()), list(IDs_test2.values()), list(IDs_test3.values()), list(IDs_test4.values()), list(IDs_test5.values())]\n",
    "\n",
    "\n",
    "        if features == 'TFIDF':\n",
    "            if len(datasets) == 5:\n",
    "                with open(\"TFIDFsparse.pkl\", 'rb') as inp_file:\n",
    "                    [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev, vocabulary] = pickle.load(inp_file)\n",
    "\n",
    "            if len(datasets)==7:\n",
    "                with open(\"TFIDFsparse7.pkl\", 'rb') as inp_file:\n",
    "                    [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_test4, sparse_test5, sparse_dev, sparse_train_dev, vocabulary] = pickle.load(inp_file)\n",
    "\n",
    "            Xtrain = sparse_train_dev                      # combining the two datasets\n",
    "            ID_train = list(IDs_train.values())            # list of IDs in train set\n",
    "            ID_dev = list(IDs_dev.values())                # list of IDs in dev set\n",
    "            ID_train_dev = ID_train + ID_dev               # combined train, dev\n",
    "            sn_train = [sent_train[id] for id in ID_train] # training labels train\n",
    "            sn_dev = [sent_dev[id] for id in ID_dev]       # training labels dev\n",
    "            Ytrain = np.array(sn_train + sn_dev)           # combining both labels\n",
    "            Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])                      # numerical labels for train\n",
    "            y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])  # numerical labels for test1\n",
    "            y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])  # numerical labels for test2\n",
    "            y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])  # numerical labels for test3\n",
    "            testset_sparse_samples = [sparse_test1, sparse_test2, sparse_test3]           # list of testing examples\n",
    "            testset_IDs = [list(IDs_test1.values()), list(IDs_test2.values()), list(IDs_test3.values())]  # IDs in test1, test2, test3\n",
    "\n",
    "            if len(datasets)==7:\n",
    "                y_test4_numeric = np.array([sent2num(sent) for sent in sent_test4.values()])  # numerical labels for test4\n",
    "                y_test5_numeric = np.array([sent2num(sent) for sent in sent_test5.values()])  # numerical labels for test5\n",
    "                testset_sparse_samples = [sparse_test1, sparse_test2, sparse_test3, sparse_test4, sparse_test5]\n",
    "                testset_IDs = [list(IDs_test1.values()), list(IDs_test2.values()), list(IDs_test3.values()), list(IDs_test4.values()), list(IDs_test5.values())]\n",
    "\n",
    "\n",
    "        # Creation and training of the classifiers\n",
    "        if classifier == 'NearestNeighbour':\n",
    "            print('--> ' + classifier.upper() + ': ')\n",
    "            t0 = time.time()                # timing the run\n",
    "            clf = KNeighborsClassifier(n_neighbors=9, metric='cosine', weights='uniform')  # the best params selected by GridSearch\n",
    "            clf.fit(Xtrain, Ytrain_numeric)\n",
    "\n",
    "        elif classifier == 'NaiveBayes':\n",
    "            print('--> ' + classifier.upper() + ': ')\n",
    "            t0 = time.time()                # timing the run\n",
    "            clf = MultinomialNB(alpha = 0.4)# the best params selected by GridSearch\n",
    "            clf.fit(Xtrain, Ytrain_numeric)\n",
    "\n",
    "        elif classifier == 'SVM':\n",
    "            print('--> ' + classifier.upper())\n",
    "            t0 = time.time() # timing the run\n",
    "            clf = LinearSVC(C=0.0200, class_weight='balanced', max_iter=2000, tol=0.01)\n",
    "            clf.fit(Xtrain,Ytrain_numeric)\n",
    "\n",
    "        elif classifier == 'MaxEnt':\n",
    "            print('--> ' + classifier.upper())\n",
    "            tt0 = time.time() # timing the run\n",
    "            clf = LogisticRegression(C=0.15, tol=0.001, class_weight='balanced', max_iter=1000, multi_class='multinomial')\n",
    "            clf.fit(Xtrain,Ytrain_numeric)\n",
    "\n",
    "        # Prediction performance of the classical classifiers\n",
    "        testsets = datasets[1:4]\n",
    "        if len(datasets)==7:\n",
    "            testsets = datasets[1:6]\n",
    "        if classifier != 'LSTM' and 'LSTM' not in classifier:\n",
    "            for i in range(len(testsets)):\n",
    "                testset = testsets[i]\n",
    "                X, IDs = testset_sparse_samples[i], testset_IDs[i]\n",
    "                y_pred_numeric = clf.predict(X)\n",
    "                y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "                pred_dict = dict(zip(IDs, y_pred))\n",
    "                evaluate(pred_dict, testset, classifier=features + '-' + classifier)\n",
    "            t1 = time.time()                # timing the run\n",
    "            print(f\"\\t\\t\\t\\t\\t\\t\\t\\tTraining+eval time: {t1-t0:.3f}\")\n",
    "\n",
    "    # a basic implementation of LSTM recurrent neural network:\n",
    "    if classifier == 'LSTM':\n",
    "        print('--> ' + classifier.upper())\n",
    "\n",
    "        device = torch.device('cpu')                     # setting up the device\n",
    "        random_seed_setup(123)                           # setting up the seed\n",
    "        train_dataset, dev_dataset = tweet_dataset(\"xy_train.pkl\"), tweet_dataset(\"xy_dev.pkl\") # loading WEIGHTED data\n",
    "\n",
    "        # Hyperparameters\n",
    "        input_size = 100\n",
    "        seq_len = 32\n",
    "        n_layers = 1\n",
    "        n_classes = 3\n",
    "        hidden_size = 100\n",
    "        n_epochs = 9\n",
    "        batch_size = 64\n",
    "        learning_rate = 0.001\n",
    "        HYPER = {'hidden_size': hidden_size,'n_layers': n_layers,'n_epochs': n_epochs,'batch_size': batch_size,'learning_rate': learning_rate}\n",
    "\n",
    "        # data loaders\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        dev_loader = DataLoader(dataset=dev_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        # Fully connected neural network with one hidden layer\n",
    "        class RNN(nn.Module):                                                                # -> adjusted the embeddings-weighted\n",
    "            def __init__(self, input_size, hidden_size, n_layers, n_classes, embedding_matrix_file=\"embeddings-weighted.pkl\"):\n",
    "                super(RNN, self).__init__()\n",
    "\n",
    "                # load preprocessed embedding matrix\n",
    "                with open(embedding_matrix_file, 'rb') as inp_file:\n",
    "                    [embedding_matrix, _, _] = pickle.load(inp_file)  # [embedding_matrix, word2ID, embedding_dict]\n",
    "                    embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "\n",
    "                n_embeddings, embedding_dim = embedding_matrix.size()  # (5000, 100) ~ (n_vocab, glove_dim)\n",
    "                embedding_layer = nn.Embedding(n_embeddings, embedding_dim)\n",
    "                embedding_layer.load_state_dict({'weight': embedding_matrix})\n",
    "                embedding_layer.weight.requires_grad = False\n",
    "\n",
    "                self.n_layers = n_layers\n",
    "                self.embedding = embedding_layer\n",
    "                self.hidden_size = hidden_size\n",
    "                self.n_embeddings = n_embeddings\n",
    "                self.embedding_dim = embedding_dim\n",
    "                self.lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)  #  x: (batch_size, seq, input_size)\n",
    "                self.fc = nn.Linear(hidden_size, n_classes)  # classification layer\n",
    "\n",
    "\n",
    "                #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True) # alternative to lstm\n",
    "\n",
    "            def forward(self, x_batch):\n",
    "                init_weights = torch.rand(self.n_layers, x_batch.size(0), self.hidden_size).to(device)\n",
    "                init_cell = torch.rand(self.n_layers, x_batch.size(0), self.hidden_size).to(device)\n",
    "\n",
    "                x_batch =  x_batch[:,0:cut_off(x_batch)]  # truncate the zeros at the end of the batch\n",
    "                x_to_glove = self.embedding(x_batch)\n",
    "                out, _ = self.lstm(x_to_glove, (init_weights,init_cell))\n",
    "                # (64, 32, 100) ~ (batch_size, seq, hidden_dim)\n",
    "                out = out[:,-1,:]  # take only the last time step in the sequence\n",
    "                # (64, 100) ~ (batch_size, hidden_dim)\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "\n",
    "        model = RNN(input_size, hidden_size, n_layers, n_classes).to(device)\n",
    "\n",
    "        # loading preprocessed embedding file if it exists (embedding matrix, word to index map, embedding dictionary)\n",
    "        if os.path.isfile('lstm.pt'):  # loading the pytorch pretrained model\n",
    "            model.load_state_dict(torch.load(\"lstm.pt\"))\n",
    "        else:  # otherwise we train the model\n",
    "\n",
    "            # Loss and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            L = []            # training loss history\n",
    "            L_validation = [] # validation loss history\n",
    "            i_val = []        # validation indexes\n",
    "\n",
    "            # Train the model\n",
    "            n_total_steps = len(train_loader)\n",
    "            for epoch in tqdm(range(n_epochs)):\n",
    "                for i, (xs, ys) in enumerate(train_loader):\n",
    "                    # originally  (100, 32) ~ (samples, max_len_seq)\n",
    "                    # after embedding: (100, 32, 100) ~ (samples, seqs, glove_dim)\n",
    "                    xs = xs.to(device)  # xs will be (10, 32) ~ (batch, seq_len)\n",
    "                    ys = ys.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(xs)\n",
    "                    loss = criterion(outputs, ys)\n",
    "                    L.append(loss.item())\n",
    "\n",
    "                    # Backward and optimize\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # do the validation error analysis\n",
    "                with torch.no_grad():\n",
    "                    ys_true, ys_pred, L_val = [], [], []\n",
    "                    for  j, (xs, ys) in enumerate(dev_loader):\n",
    "                        preds = model(xs)\n",
    "                        loss = criterion(preds, ys)\n",
    "                        L_val.append(loss)\n",
    "                        ys_true.append(ys)\n",
    "                        ys_pred.append(preds.argmax(dim=-1))\n",
    "                    ys_true = torch.cat(ys_true)\n",
    "                    ys_pred = torch.cat(ys_pred)\n",
    "                    mean_val_loss = torch.tensor(L_val).mean()\n",
    "                    val_accuracy = accuracy_score(ys_true.detach().numpy(), ys_pred.detach().numpy())\n",
    "                    L_validation.append(mean_val_loss)  # append the validation loss vector\n",
    "                    i_val.append((epoch + 1) * (i + 1))             # record the iteration number for plotting\n",
    "\n",
    "            # reporting the train and validation performance\n",
    "            [print(f\"{p}:{n}  \" , end='' ) for (p,n) in HYPER.items()]; print('')\n",
    "            print(f\"  Train loss: \\t {L[-1]:.3f} \\t\\tValidation loss: {L_validation[-1]:.3f}  \\tValidation acc:  {val_accuracy:.3f} \")\n",
    "            plt.figure(figsize=(12,8)); plt.plot(L, label=\"train loss\");plt.xlabel('iterations');plt.ylabel('Loss');plt.grid()\n",
    "            plt.plot(i_val, L_validation, 'o:', label=\"val loss\"); plt.legend(); plt.show()\n",
    "\n",
    "            torch.save(model.state_dict(), \"lstm.pt\")  # saving the model so that it can be reloaded\n",
    "\n",
    "        #evaluation over testsets:\n",
    "        with torch.no_grad():\n",
    "            with open('xy_test1.pkl', 'rb') as inp_file:\n",
    "                [xs, ys] = pickle.load(inp_file)\n",
    "\n",
    "                y_pred1 = []\n",
    "                for x in xs:\n",
    "                    t = torch.from_numpy(np.array([x]))\n",
    "                    x = toTensor(t, dtype=torch.long, requires_grad=False)\n",
    "                    pr = model(x)\n",
    "                    y_pred1.append(int(pr.argmax()))\n",
    "\n",
    "            with open('xy_test2.pkl', 'rb') as inp_file:\n",
    "                [xs, ys] = pickle.load(inp_file)\n",
    "\n",
    "                y_pred2 = []\n",
    "                for x in xs:\n",
    "                    t = torch.from_numpy(np.array([x]))\n",
    "                    x = toTensor(t, dtype=torch.long, requires_grad=False)\n",
    "                    pr = model(x)\n",
    "                    y_pred2.append(int(pr.argmax()))\n",
    "\n",
    "            with open('xy_test3.pkl', 'rb') as inp_file:\n",
    "                [xs, ys] = pickle.load(inp_file)\n",
    "\n",
    "                y_pred3 = []\n",
    "                for x in xs:\n",
    "                    t = torch.from_numpy(np.array([x]))\n",
    "                    x = toTensor(t, dtype=torch.long, requires_grad=False)\n",
    "                    pr = model(x)\n",
    "                    y_pred3.append(int(pr.argmax()))\n",
    "\n",
    "            if len(datasets) == 7:\n",
    "                with open('xy_test4.pkl', 'rb') as inp_file:\n",
    "                    [xs, ys] = pickle.load(inp_file)\n",
    "\n",
    "                    y_pred4 = []\n",
    "                    for x in xs:\n",
    "                        t = torch.from_numpy(np.array([x]))\n",
    "                        x = toTensor(t, dtype=torch.long, requires_grad=False)\n",
    "                        pr = model(x)\n",
    "                        y_pred4.append(int(pr.argmax()))\n",
    "\n",
    "                with open('xy_test5.pkl', 'rb') as inp_file:\n",
    "                    [xs, ys] = pickle.load(inp_file)\n",
    "\n",
    "                    y_pred5 = []\n",
    "                    for x in xs:\n",
    "                        t = torch.from_numpy(np.array([x]))\n",
    "                        x = toTensor(t, dtype=torch.long, requires_grad=False)\n",
    "                        pr = model(x)\n",
    "                        y_pred5.append(int(pr.argmax()))\n",
    "\n",
    "            # F1 score:\n",
    "            y_pred_numeric1 = np.array(y_pred1)-1\n",
    "            y_pred_numeric2 = np.array(y_pred2)-1\n",
    "            y_pred_numeric3 = np.array(y_pred3)-1\n",
    "            test_pred1 = [num2sent(num) for num in y_pred_numeric1]\n",
    "            test_pred2 = [num2sent(num) for num in y_pred_numeric2]\n",
    "            test_pred3 = [num2sent(num) for num in y_pred_numeric3]\n",
    "            pred_dict1 = dict(zip(testset_IDs[0], test_pred1))\n",
    "            pred_dict2 = dict(zip(testset_IDs[1], test_pred2))\n",
    "            pred_dict3 = dict(zip(testset_IDs[2], test_pred3))\n",
    "            evaluate(pred_dict1, testsets[0], classifier=classifier)\n",
    "            evaluate(pred_dict2, testsets[1], classifier=classifier)\n",
    "            evaluate(pred_dict3, testsets[2], classifier=classifier)\n",
    "            if len(datasets)==7:\n",
    "                y_pred_numeric4 = np.array(y_pred4)-1\n",
    "                y_pred_numeric5 = np.array(y_pred5)-1\n",
    "                test_pred4 = [num2sent(num) for num in y_pred_numeric4]\n",
    "                test_pred5 = [num2sent(num) for num in y_pred_numeric5]\n",
    "                pred_dict4 = dict(zip(testset_IDs[3], test_pred4))\n",
    "                pred_dict5 = dict(zip(testset_IDs[4], test_pred5))\n",
    "                evaluate(pred_dict4, testsets[3], classifier=classifier)\n",
    "                evaluate(pred_dict5, testsets[4], classifier=classifier)\n",
    "\n",
    "\n",
    "\n",
    "    # LSTM Recurrent neural network with an additional hidden layer\n",
    "    if classifier == 'LSTM-hidden':\n",
    "        print('--> ' + classifier.upper())\n",
    "\n",
    "        device = torch.device('cpu')                     # setting up the device\n",
    "        random_seed_setup(123)                           # setting up the seed\n",
    "        train_dataset, dev_dataset = tweet_dataset(\"xy_train.pkl\"), tweet_dataset(\"xy_dev.pkl\") # loading WEIGHTED data\n",
    "\n",
    "        # Hyperparameters\n",
    "        input_size = 100\n",
    "        seq_len = 32\n",
    "        hidden_size = 64\n",
    "        hidden_size2 = 32\n",
    "        n_layers = 1\n",
    "        n_classes = 3\n",
    "        n_epochs = 12\n",
    "        batch_size = 128\n",
    "        learning_rate = 0.001\n",
    "        HYPER = {'hidden_size': [hidden_size,hidden_size2],'n_layers': n_layers,'n_epochs': n_epochs,'batch_size': batch_size,'learning_rate': learning_rate}\n",
    "\n",
    "        # data loaders\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        dev_loader = DataLoader(dataset=dev_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        # Fully connected neural network with one hidden layer\n",
    "        class lstm_hidden(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, hidden_size2, n_layers, n_classes, embedding_matrix_file=\"embeddings-weighted.pkl\"):\n",
    "                super(lstm_hidden, self).__init__()\n",
    "\n",
    "                # load preprocessed embedding matrix\n",
    "                with open(embedding_matrix_file, 'rb') as inp_file:\n",
    "                    [embedding_matrix, _, _] = pickle.load(inp_file)\n",
    "                    embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "\n",
    "\n",
    "                n_embeddings, embedding_dim = embedding_matrix.size()  # (5000, 100) ~ (n_vocab, glove_dim)\n",
    "                embedding_layer = nn.Embedding(n_embeddings, embedding_dim)\n",
    "                embedding_layer.load_state_dict({'weight': embedding_matrix})\n",
    "                embedding_layer.weight.requires_grad = False\n",
    "\n",
    "                self.n_layers = n_layers\n",
    "                self.hidden_size = hidden_size\n",
    "                self.embedding = embedding_layer\n",
    "                self.n_embeddings = n_embeddings\n",
    "                self.embedding_dim = embedding_dim\n",
    "                self.lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)  #  x: (batch_size, seq, input_size)\n",
    "                self.fc1 = nn.Linear(hidden_size, hidden_size2)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.fc2 = nn.Linear(hidden_size2, n_classes)  # classification layer\n",
    "\n",
    "            def forward(self, x_batch):\n",
    "                init_weights = torch.rand(self.n_layers, x_batch.size(0), self.hidden_size).to(device)\n",
    "                init_cell = torch.rand(self.n_layers, x_batch.size(0), self.hidden_size).to(device)\n",
    "\n",
    "                x_batch =  x_batch[:,0:cut_off(x_batch)]  # truncate the zeros at the end of the batch\n",
    "                x_to_glove = self.embedding(x_batch)\n",
    "                out, _ = self.lstm(x_to_glove, (init_weights,init_cell))\n",
    "                # (10, 32, 60) ~ (batch_size, seq, hidden_dim)\n",
    "                out = out[:,-1,:]  # take only the last time step in the sequence\n",
    "                # (10, 60) ~ (batch_size, hidden_dim)\n",
    "                out = self.fc1(out)\n",
    "                out = self.relu(out)\n",
    "                out = self.fc2(out)\n",
    "                return out\n",
    "\n",
    "        model = lstm_hidden(input_size, hidden_size, hidden_size2, n_layers, n_classes).to(device)\n",
    "\n",
    "        # loading preprocessed embedding file if it exists (embedding matrix, word to index map, embedding dictionary)\n",
    "        if os.path.isfile('lstm-hidden.pt'):  # loading the pytorch pretrained model\n",
    "            model.load_state_dict(torch.load(\"lstm-hidden.pt\"))\n",
    "        else:  # otherwise we train the model\n",
    "\n",
    "            # Loss and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            L = []            # training loss history\n",
    "            L_validation = [] # validation loss history\n",
    "            i_val = []        # validation indexes\n",
    "\n",
    "            # Train the model\n",
    "            n_total_steps = len(train_loader)\n",
    "            for epoch in tqdm(range(n_epochs)):\n",
    "                for i, (xs, ys) in enumerate(train_loader):\n",
    "                    # originally  (100, 32) ~ (samples, max_len_seq)\n",
    "                    # after embedding: (100, 32, 100) ~ (samples, seqs, glove_dim)\n",
    "                    xs = xs.to(device)  # xs will be (10, 32) ~ (batch, seq_len)\n",
    "                    ys = ys.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(xs)\n",
    "                    loss = criterion(outputs, ys)\n",
    "                    L.append(loss.item())\n",
    "\n",
    "                    # Backward and optimize\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # do the validation error analysis\n",
    "                with torch.no_grad():\n",
    "                    ys_true, ys_pred, L_val = [], [], []\n",
    "                    for  j, (xs, ys) in enumerate(dev_loader):\n",
    "                        preds = model(xs)\n",
    "                        loss = criterion(preds, ys)\n",
    "                        L_val.append(loss)\n",
    "                        ys_true.append(ys)\n",
    "                        ys_pred.append(preds.argmax(dim=-1))\n",
    "                    ys_true = torch.cat(ys_true)\n",
    "                    ys_pred = torch.cat(ys_pred)\n",
    "                    mean_val_loss = torch.tensor(L_val).mean()\n",
    "                    val_accuracy = accuracy_score(ys_true.detach().numpy(), ys_pred.detach().numpy())\n",
    "                    L_validation.append(mean_val_loss)              # append the validation loss vector\n",
    "                    i_val.append((epoch + 1) * (i + 1))             # record the iteration number for plotting\n",
    "\n",
    "            # reporting the train and validation performance\n",
    "            [print(f\"{p}:{n}  \" , end='' ) for (p,n) in HYPER.items()]; print('')\n",
    "            print(f\"  Train loss: \\t {L[-1]:.3f} \\t\\tValidation loss: {L_validation[-1]:.3f}  \\tValidation acc:  {val_accuracy:.3f} \")\n",
    "            plt.figure(figsize=(12,8)); plt.plot(L, label=\"train loss\");plt.xlabel('iterations');plt.ylabel('Loss');plt.grid()\n",
    "            plt.plot(i_val, L_validation, 'o:', label=\"val loss\"); plt.legend(); plt.show()\n",
    "\n",
    "            torch.save(model.state_dict(), \"lstm-hidden.pt\")  # saving the model so that it can be reloaded\n",
    "\n",
    "        #evaluation over testsets:\n",
    "        with torch.no_grad():\n",
    "            with open('xy_test1.pkl', 'rb') as inp_file:\n",
    "                temp = pickle.load(inp_file)\n",
    "                [xs, ys] = temp\n",
    "\n",
    "                y_pred1 = []\n",
    "                for x in xs:\n",
    "                    t = torch.from_numpy(np.array([x]))\n",
    "                    x = toTensor(t, dtype=torch.long, requires_grad=False)\n",
    "                    pr = model(x)\n",
    "                    y_pred1.append(int(pr.argmax()))\n",
    "\n",
    "            with open('xy_test2.pkl', 'rb') as inp_file:\n",
    "                temp = pickle.load(inp_file)\n",
    "                [xs, ys] = temp\n",
    "\n",
    "                y_pred2 = []\n",
    "                for x in xs:\n",
    "                    t = torch.from_numpy(np.array([x]))\n",
    "                    x = toTensor(t, dtype=torch.long, requires_grad=False)\n",
    "                    pr = model(x)\n",
    "                    y_pred2.append(int(pr.argmax()))\n",
    "\n",
    "            with open('xy_test3.pkl', 'rb') as inp_file:\n",
    "                temp = pickle.load(inp_file)\n",
    "                [xs, ys] = temp\n",
    "\n",
    "                y_pred3 = []\n",
    "                for x in xs:\n",
    "                    t = torch.from_numpy(np.array([x]))\n",
    "                    x = toTensor(t, dtype=torch.long, requires_grad=False)\n",
    "                    pr = model(x)\n",
    "                    y_pred3.append(int(pr.argmax()))\n",
    "\n",
    "            # F1 score:\n",
    "            y_pred_numeric1 = np.array(y_pred1)-1\n",
    "            y_pred_numeric2 = np.array(y_pred2)-1\n",
    "            y_pred_numeric3 = np.array(y_pred3)-1\n",
    "            test_pred1 = [num2sent(num) for num in y_pred_numeric1]\n",
    "            test_pred2 = [num2sent(num) for num in y_pred_numeric2]\n",
    "            test_pred3 = [num2sent(num) for num in y_pred_numeric3]\n",
    "            pred_dict1 = dict(zip(testset_IDs[0], test_pred1))\n",
    "            pred_dict2 = dict(zip(testset_IDs[1], test_pred2))\n",
    "            pred_dict3 = dict(zip(testset_IDs[2], test_pred3))\n",
    "            evaluate(pred_dict1, testsets[0], classifier=classifier)\n",
    "            evaluate(pred_dict2, testsets[1], classifier=classifier)\n",
    "            evaluate(pred_dict3, testsets[2], classifier=classifier)\n",
    "            if len(datasets)==7:\n",
    "                y_pred_numeric4 = np.array(y_pred4)-1\n",
    "                y_pred_numeric5 = np.array(y_pred5)-1\n",
    "                test_pred4 = [num2sent(num) for num in y_pred_numeric4]\n",
    "                test_pred5 = [num2sent(num) for num in y_pred_numeric5]\n",
    "                pred_dict4 = dict(zip(testset_IDs[3], test_pred4))\n",
    "                pred_dict5 = dict(zip(testset_IDs[4], test_pred5))\n",
    "                evaluate(pred_dict4, testsets[3], classifier=classifier)\n",
    "                evaluate(pred_dict5, testsets[4], classifier=classifier)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor(0.9002),\n tensor(0.8696),\n tensor(0.8771),\n tensor(0.8451),\n tensor(0.8561),\n tensor(0.8447),\n tensor(0.8220),\n tensor(0.8323),\n tensor(0.8486),\n tensor(0.8353)]"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 10)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-50-f4c9e9176e68>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"preprocessing-sent-ID.pkl\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0minp_file\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;31m# loading preprocessed data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    128\u001B[0m     \u001B[0mtemp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 129\u001B[0;31m     [sent_train, sent_test1, sent_test2, sent_test3, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3,\n\u001B[0m\u001B[1;32m    130\u001B[0m      IDs_dev] = temp\n\u001B[1;32m    131\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 10)"
     ]
    }
   ],
   "source": [
    "# 10K - word embeddings\n",
    "\n",
    "# ALTERNATIVE APPROACH to preprocessing of embeddings: Weighted Approach - considering both semantic and frequency value\n",
    "# -> in the first approach, only frequency was the criterion to select 5000 words\n",
    "# -> but that way, we may select words which have minimal semantic value\n",
    "# -> the weighted approach will select both the semantic as well as frequency contribution\n",
    "\n",
    "\n",
    "# loading preprocessed embedding file if it exists (embedding matrix, word to index map, embedding dictionary)\n",
    "file_to_load =  \"embeddings-weighted.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(\"embeddings-weighted.pkl\", 'rb') as inp_file:\n",
    "        temp = pickle.load(inp_file)\n",
    "        [embedding_matrix, word2ID, embedding_dict] = temp\n",
    "else:\n",
    "    ## 0) loading preprocessing data for glove\n",
    "    file_to_load = \"preprocessing-glove.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            t = pickle.load(inp_file)\n",
    "            text_train, text_test1, text_test2, text_test3, text_dev = t[0], t[1], t[2], t[3], t[4]\n",
    "            txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev = t[5], t[6], t[7], t[8], t[9]\n",
    "\n",
    "    ## 1) finding the frequency weights\n",
    "    txtlist_dicts = [txtlist_train, txtlist_dev]\n",
    "    freq = FreqDist()   # frequency distribution\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in tweet:\n",
    "                if not word in stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "    nums = range(1, len(freq.keys())+1)\n",
    "    vocabulary = list(freq.keys())              # creating the dictionary\n",
    "    vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "    num2vocab = dict(zip(nums, vocabulary))     # word to index mapping\n",
    "\n",
    "    sorted_vocabulary = sorted([it for it in freq.items()], key=lambda data: data[1], reverse=True)\n",
    "    maxF = sorted_vocabulary[0][1]  # maximal frequency of a word\n",
    "    normalized_vocabulary = { word: f / maxF for (word, f) in sorted_vocabulary}  # dictionary\n",
    "\n",
    "    ## 2) use SVM coeffs (make sure to have 'SVMcoefficients.pkl' which stores clf.coef_ from SVM)\n",
    "    with open(\"SVMcoefficients.pkl\", 'rb') as inp_file:\n",
    "        SVM_coef = pickle.load(inp_file)\n",
    "    coefs = []\n",
    "    coefs_nums = []\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        c = np.abs(SVM_coef[2,i] - SVM_coef[0,i])  # coefficent: absolute difference between positive and negative:\n",
    "        word_coef = (word, c)                            #  ->> \"most negative/most positive\" words have higher index\n",
    "        coefs_nums.append(c)\n",
    "        coefs.append(word_coef)\n",
    "    minC, maxC = np.min(coefs_nums), np.max(coefs_nums)\n",
    "    coefs = [(word, (c - minC) / maxC) for (word, c) in coefs]     # normalize coefs\n",
    "    coefs = sorted(coefs, key=lambda data: data[1], reverse=True)  # sort coefs\n",
    "\n",
    "    ## 3) apply the weight formula: 2 * freq_value + 1 * sentiment_value ~ 2 f + 1 s\n",
    "    weighted_scores = [(word, 1 * cn + 2 * normalized_vocabulary[word]) for (word, cn) in coefs]  # list of tuples\n",
    "    sorted_weighted_vocabulary = sorted([it for it in weighted_scores], key=lambda data: data[1], reverse=True)\n",
    "\n",
    "    ## 4) load GloVe embeddings\n",
    "    full_embedding_dict = {}\n",
    "    glove_path = join('..','glove', 'glove.6B.100d.txt')\n",
    "    with open(glove_path, 'r', encoding='utf-8') as File:\n",
    "        for line in File:\n",
    "            vec = line.split()\n",
    "            word = vec[0]\n",
    "            coefs = np.asarray(vec[1:], dtype='float32')\n",
    "            full_embedding_dict[word] = coefs\n",
    "    print(f\"Extracted {len(full_embedding_dict)} word embedding vectors.\")\n",
    "\n",
    "    ## 5) extract the word embeddings of 5000 words based on the order from the weighted approach\n",
    "    embedding_dict = {}\n",
    "    temp = 0\n",
    "    for (word,_) in sorted_weighted_vocabulary:\n",
    "        if word in full_embedding_dict.keys():\n",
    "            embedding_dict[word] = full_embedding_dict[word]\n",
    "            temp += 1\n",
    "        if temp == 4998:\n",
    "            break\n",
    "    vocabulary5000 = list(embedding_dict.keys())  # obtain the dictionary of 5000 most common words\n",
    "\n",
    "    ## 6) extract the <OOV> vector by setting it to be the weighted avg of unused words\n",
    "    Total = np.zeros(100)\n",
    "    Sum = 0\n",
    "    for word in full_vocabulary:\n",
    "        if word not in vocabulary5000:                          # if word is not among 5000 words\n",
    "            if word in full_embedding_dict.keys():              # and it is in glove\n",
    "                Total += freq[word] * full_embedding_dict[word] # take the weighted avg\n",
    "                Sum += freq[word]\n",
    "    OOV_vector = Total / Sum\n",
    "    embedding_dict['<OOV>'] = OOV_vector\n",
    "\n",
    "    ## 7) Build the weighted-embedding matrix\n",
    "    word_list = list(embedding_dict.keys())\n",
    "    nums = range(1,len(word_list)+1)\n",
    "    word2ID = dict(zip(word_list, nums))     # the index of the embedding vector\n",
    "    num2vocab = dict(zip(nums, word_list))   # the index to word\n",
    "    vector_list = [embedding_dict[word] for word in word_list]\n",
    "    embedding_matrix = np.vstack(vector_list)\n",
    "    embedding_matrix = np.vstack((np.zeros(100), embedding_matrix))\n",
    "\n",
    "    print(f\"Created matrix with shape {embedding_matrix.shape}\")  # the first row is a dummy row\n",
    "    del full_embedding_dict # delete from memory\n",
    "\n",
    "    # save the embeddings\n",
    "    with open(\"embeddings-weighted.pkl\", 'wb') as out_file:\n",
    "        temp = [embedding_matrix, word2ID, embedding_dict]\n",
    "        pickle.dump(temp, out_file, protocol=-1)\n",
    "\n",
    "\n",
    "# preparing matrices for torch: matrix_train, matrix_dev, matrix_test1, matrix_test2, matrix_test3\n",
    "\n",
    "# loading preprocessed embeddings\n",
    "embedding_file = \"embeddings-weighted.pkl\"  # alternatively use \"embeddings.pkl\"\n",
    "with open(embedding_file, 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [embedding_matrix, word2ID, embedding_dict] = temp\n",
    "\n",
    "# converting the text lists into vectors of ints\n",
    "word_list = list(embedding_dict.keys())\n",
    "\n",
    "with open(\"preprocessing-glove.pkl\", 'rb') as inp_file:   # loading preprocessed data for glove\n",
    "    t = pickle.load(inp_file)\n",
    "    text_train, text_test1, text_test2, text_test3, text_dev = t[0], t[1], t[2], t[3], t[4]\n",
    "    txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev = t[5], t[6], t[7], t[8], t[9]\n",
    "\n",
    "with open(\"preprocessing-sent-ID.pkl\", 'rb') as inp_file: # loading preprocessed data\n",
    "    temp = pickle.load(inp_file)\n",
    "    [sent_train, sent_test1, sent_test2, sent_test3, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3,\n",
    "     IDs_dev] = temp\n",
    "\n",
    "max_len = np.max([len(tweet) for tweet in txtlist_train.values()])      # longest tokenized sentence\n",
    "matrix_train = np.zeros((len(txtlist_train), max_len), dtype=np.int16)  # training datapoints\n",
    "y_train = np.zeros(len(txtlist_train), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_train.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_train[id]\n",
    "    matrix_train[i] = x\n",
    "    y_train[i] = sent2num(y)\n",
    "\n",
    "matrix_test1 = np.zeros((len(txtlist_test1), max_len), dtype=np.int16)  # training datapoints\n",
    "y_test1 = np.zeros(len(txtlist_test1), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_test1.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_test1[id]\n",
    "    matrix_test1[i] = x\n",
    "    y_test1[i] = sent2num(y)\n",
    "\n",
    "matrix_test2 = np.zeros((len(txtlist_test2), max_len), dtype=np.int16)  # training datapoints\n",
    "y_test2 = np.zeros(len(txtlist_test2), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_test2.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_test2[id]\n",
    "    matrix_test2[i] = x\n",
    "    y_test2[i] = sent2num(y)\n",
    "\n",
    "matrix_test3 = np.zeros((len(txtlist_test3), max_len), dtype=np.int16)  # training datapoints\n",
    "y_test3 = np.zeros(len(txtlist_test3), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_test3.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_test3[id]\n",
    "    matrix_test3[i] = x\n",
    "    y_test3[i] = sent2num(y)\n",
    "\n",
    "matrix_dev = np.zeros((len(txtlist_dev), max_len), dtype=np.int16)  # training datapoints\n",
    "y_dev = np.zeros(len(txtlist_dev), dtype=np.int8)  # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_dev.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_dev[id]\n",
    "    matrix_dev[i] = x\n",
    "    y_dev[i] = sent2num(y)\n",
    "\n",
    "# save the data as pickle files\n",
    "with open(\"xy_train.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_train, y_train]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_dev.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_dev, y_dev]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_test1.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_test1, y_test1]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_test2.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_test2, y_test2]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_test3.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_test3, y_test3]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "\n",
    "# load the saved data from pickle files\n",
    "with open(\"xy_train.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_train, y_train] = temp\n",
    "with open(\"xy_dev.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_dev, y_dev] = temp\n",
    "with open(\"xy_test1.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_test1, y_test1] = temp\n",
    "with open(\"xy_test2.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_test2, y_test2] = temp\n",
    "with open(\"xy_test3.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_test3, y_test3] = temp\n",
    "\n",
    "print(\"Saved 10K matrices for torch.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
