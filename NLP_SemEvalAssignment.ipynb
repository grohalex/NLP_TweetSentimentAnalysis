{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required modules for preprocessing\n",
    "import os\n",
    "from os.path import join\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords as Stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# import required modules for classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import required modules for RNN classification\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Load training set, dev set and testing set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from: \n",
      "\t../semeval-tweets/twitter-training-data.txt\n",
      "\t../semeval-tweets/twitter-test1.txt\n",
      "\t../semeval-tweets/twitter-test2.txt\n",
      "\t../semeval-tweets/twitter-test3.txt\n",
      "\t../semeval-tweets/twitter-dev-data.txt\n"
     ]
    }
   ],
   "source": [
    "# Load training set, dev set and testing set\n",
    "\n",
    "dataDir = '../semeval-tweets'  # change to the proper directory\n",
    "datasetStrings = ['twitter-training-data.txt', 'twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt', 'twitter-dev-data.txt']\n",
    "datasets = [join(dataDir, t) for t in datasetStrings]\n",
    "print(f\"Extracting data from: \\n\\t{datasets[0]}\\n\\t{datasets[1]}\\n\\t{datasets[2]}\\n\\t{datasets[3]}\\n\\t{datasets[4]}\")\n",
    "\n",
    "tweet_IDs = {}          # init dictionary with tweet IDs\n",
    "tweet_sentiments = {}   # init dictionary with sentiments\n",
    "tweet_texts = {}        # init dictionary with tweet texts\n",
    "\n",
    "for DatasetString in datasets:\n",
    "    data_ID, data_sent, data_text  = {}, {}, {}    # temp dictionaries\n",
    "    with open(DatasetString, 'r', encoding='utf8') as f1:\n",
    "        for i, line in enumerate(f1):\n",
    "            fields = line.split('\\t')\n",
    "            data_ID[i] = fields[0]            # tweet IDs\n",
    "            data_sent[fields[0]] = fields[1]  # sentiments\n",
    "            data_text[fields[0]] = fields[2]  # tweet text\n",
    "    tweet_IDs[DatasetString] = data_ID\n",
    "    tweet_sentiments[DatasetString] = data_sent\n",
    "    tweet_texts[DatasetString] = data_text\n",
    "\n",
    "# sentiment dictionaries\n",
    "sent_train = tweet_sentiments[datasets[0]]\n",
    "sent_test1 = tweet_sentiments[datasets[1]]\n",
    "sent_test2 = tweet_sentiments[datasets[2]]\n",
    "sent_test3 = tweet_sentiments[datasets[3]]\n",
    "sent_dev = tweet_sentiments[datasets[4]]\n",
    "\n",
    "# tweet text dictionaries\n",
    "text_train = tweet_texts[datasets[0]]\n",
    "text_test1 = tweet_texts[datasets[1]]\n",
    "text_test2 = tweet_texts[datasets[2]]\n",
    "text_test3 = tweet_texts[datasets[3]]\n",
    "text_dev = tweet_texts[datasets[4]]\n",
    "\n",
    "# tweet IDs dictionaries\n",
    "IDs_train = tweet_IDs[datasets[0]]\n",
    "IDs_test1 = tweet_IDs[datasets[1]]\n",
    "IDs_test2 = tweet_IDs[datasets[2]]\n",
    "IDs_test3 = tweet_IDs[datasets[3]]\n",
    "IDs_dev = tweet_IDs[datasets[4]]\n",
    "\n",
    "# saving the sentiments and IDs as a pickle file\n",
    "if not os.path.isfile(\"preprocessing-sent-ID.pkl\"):\n",
    "    temp = [sent_train, sent_test1, sent_test2, sent_test3, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev]\n",
    "    with open(\"preprocessing-sent-ID.pkl\", 'wb') as out_file:\n",
    "        pickle.dump(temp, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# auxiliary functions\n",
    "\n",
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    reading the testset and return a dictionary with: ID -> sentiment\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}  # init the dictionary\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "        n = acc['tp'] + acc['fn']\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "        itemcount += n\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)\n",
    "    return semevalmacrof1\n",
    "\n",
    "# removing stop words variables\n",
    "stopwords = Stopwords.words('english')\n",
    "stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its BoW representation as np.array\n",
    "def text2BOW(text_list, vocabulary, vocab2num, stopwords):\n",
    "    BOW_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in text_list:\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                BOW_vec[vocab2num[word]-1] += 1\n",
    "            else:\n",
    "                BOW_vec[vocab2num['<OOV>']-1] += 1\n",
    "    return BOW_vec\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its TFIDF representation as np.array\n",
    "def text2TFIDF(text_list, vocabulary, vocab2num, stopwords, DFfreq, Ntexts):\n",
    "    TFIDF_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in np.unique(text_list):\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                if DFfreq[word] == 0:    ###\n",
    "                    print('oh no:', word)###\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / DFfreq[word])\n",
    "                TFIDF_vec[vocab2num[word]-1] = tf * idf\n",
    "            else:\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / 0.000001 )\n",
    "                TFIDF_vec[vocab2num['<OOV>']-1] = tf * idf\n",
    "    return TFIDF_vec\n",
    "\n",
    "# convenience ftion for sentiment -> num\n",
    "def sent2num(sent):\n",
    "    if sent == 'negative':\n",
    "        return -1\n",
    "    if sent == 'neutral':\n",
    "        return 0\n",
    "    if sent == 'positive':\n",
    "        return 1\n",
    "\n",
    "# convenience ftion for num -> sentiment\n",
    "def num2sent(num):\n",
    "    if num == -1:\n",
    "        return 'negative'\n",
    "    if num == 0:\n",
    "        return 'neutral'\n",
    "    if num == 1:\n",
    "        return 'positive'\n",
    "\n",
    "# convert list of tokens (tweet) to an array of indexes\n",
    "def text_list2array(text_list, vocabulary_list, word2index_dict, max_length):\n",
    "    output_array = np.zeros(max_length, dtype=np.int16)\n",
    "    for i, word in enumerate(text_list):\n",
    "        if word in vocabulary_list:\n",
    "            output_array[i] = word2ID[word]     # update the index in vocab\n",
    "        else:\n",
    "            output_array[i] = word2ID['<OOV>']  # provide the index of OOV\n",
    "    return output_array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Data Preprocessing\n",
    "* lowercase text\n",
    "* regex cleaning\n",
    "   * Remove URLs\n",
    "   * Process emoticons\n",
    "   * Remove non-alphanumeric characters (leave hashtags and usernames)\n",
    "   * Process usernames and hashtags\n",
    "   * Remove numbers that are fully made of digits\n",
    "   * (Remove words with only 1 character)\n",
    "* Tokenisation\n",
    "* POS tagging\n",
    "* Lemmatization\n",
    "* Saving the processed output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "## Preprocessing 1: Plain - keeping all usernames, hashtags and emojis -> this preprocessing is for BOW and TFIDF-weighted BOW\n",
    "\n",
    "# loading preprocessed datasets - if you already have the preprocessed file\n",
    "file_to_load = \"preprocessing-plain.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(file_to_load, 'rb') as inp_file:\n",
    "        temp_dicts = pickle.load(inp_file)\n",
    "        txt_dicts = temp_dicts[0:5]\n",
    "        txtlist_dicts = temp_dicts[5:]\n",
    "\n",
    "else:\n",
    "    top100 = ['com', 'net', 'org', 'jp', 'de', 'uk', 'fr', 'br', 'it', 'ru', 'es', 'me', 'gov', 'pl', 'ca', 'au', 'cn', 'co', 'in', 'nl', 'edu', 'info', 'eu', 'ch', 'id', 'at', 'kr', 'cz', 'mx', 'be', 'tv', 'se', 'tr', 'tw', 'al', 'ua', 'ir', 'vn', 'cl', 'sk', 'ly', 'cc', 'to', 'no', 'fi', 'us', 'pt', 'dk', 'ar', 'hu', 'tk', 'gr', 'il', 'news', 'ro', 'my', 'biz', 'ie', 'za', 'nz', 'sg', 'ee', 'th', 'io', 'xyz', 'pe', 'bg', 'hk', 'rs', 'lt', 'link', 'ph', 'club', 'si', 'site', 'mobi', 'by', 'cat', 'wiki', 'la', 'ga', 'xxx', 'cf', 'hr', 'ng', 'jobs', 'online', 'kz', 'ug', 'gq', 'ae', 'is', 'lv', 'pro', 'fm', 'tips', 'ms', 'sa', 'app', 'lat']\n",
    "\n",
    "    # emoticons extracted from the tweets\n",
    "    emoticons = ['%)', ':&', '8-)', '=/', ':c', ':#', ':)))', ';)', 'd:', '=3', ':O', '8D', 'oO', ':o)', '*)', 'QQ', ':S', '=)', 'D8', ':]', 'O:)', 'XD', 'Q_Q', \":'(\", ':$', ':3', ':L', 'XP', ':-(', ':(', ':-)', ':-))', 'o.O', ':*', '0:3', ';;', ':D', ';D', '=]', ':@', ':)', ':))', ':/', '>:)', ':P', ':-)))', ';]', '^_^', \":')\", ':x', 'D:', ':^)', ':|', ';_;', '=p', ':b', '=D', ':o', 'DX']\n",
    "    emoticon_strings = ['emoticon' + str(num) for num in range(len(emoticons))]\n",
    "    emoticon2string = dict(zip(emoticons, emoticon_strings))\n",
    "    string2emoticon = dict(zip(emoticon_strings, emoticons))\n",
    "\n",
    "\n",
    "    ID_dicts = [IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev]\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev]\n",
    "    txtlist_dicts = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  # init the lemmatizer\n",
    "    POSconvert = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "\n",
    "    for i, IDdict in enumerate(ID_dicts):\n",
    "        output = txt_dicts[i]\n",
    "        output_txt = {}\n",
    "        for id in IDdict.values():\n",
    "            text = output[id].lower()\n",
    "\n",
    "            # replace/delete all URLs starting with 'http' and 'www'\n",
    "            new_text = re.sub(\"http[^\\s]*\", '', text)\n",
    "            new_text = re.sub(\"www[^\\s]*\", '', new_text)\n",
    "\n",
    "            # delete all URLs which have one of 100 most common extensions ('.com', '.net', ...)\n",
    "            for ext in top100:\n",
    "                re_string = \"[^\\s]*\\.\" + ext + \"[^\\s]*\"\n",
    "                new_text = re.sub(re_string, '', new_text)\n",
    "\n",
    "            # replace all emoticons with an emoticon string:  #emoticon42\n",
    "            for em in emoticons:\n",
    "                re_string = '\\s' + re.escape(em) + '\\s'\n",
    "                replace_string = ' ' + emoticon2string[em] + ' '\n",
    "                new_text = re.sub(re_string, replace_string, new_text)\n",
    "\n",
    "            # removing '&amp'\n",
    "            new_text = re.sub('&amp','', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars except for '# and @'\n",
    "            new_text = re.sub('[^\\w\\s@#]','', new_text)\n",
    "\n",
    "            # remove strings with '#' not on the beginning (to keep only hashtags)\n",
    "            new_text = re.sub('\\s[\\w]+#[\\w]*','', new_text)\n",
    "\n",
    "            # numbers fully made of digits\n",
    "            new_text = re.sub('\\s[\\d]+\\s','', new_text)\n",
    "\n",
    "            # remove words with only 1 character\n",
    "            new_text = re.sub('\\\\b\\\\w{1}\\\\b','', new_text)\n",
    "\n",
    "            # remove newline chars -> just aesthetics for printing, it doesn't matter with tokenizer\n",
    "            new_text = new_text.replace('\\n', ' ')\n",
    "\n",
    "            # replace a multiple spaces with a single space -> just aesthetics for printing\n",
    "            new_text = re.sub('\\s+',' ', new_text)\n",
    "\n",
    "            # do not delete @usernames\n",
    "            # do not delete #hashtags\n",
    "\n",
    "\n",
    "            # using the lemmatizer\n",
    "            txt_list = nltk.word_tokenize(new_text)     # tokenise the tweet\n",
    "            for k, word in enumerate(txt_list):         # fixing the separation of hashtags by the tokenizer\n",
    "                if word == '#' or word == '@':\n",
    "                    if k < len(txt_list) - 1:\n",
    "                        txt_list[k] = txt_list[k] + txt_list[k+1]\n",
    "                        txt_list.pop(k+1)\n",
    "            POS = nltk.pos_tag(txt_list)                  # POS tags from nltk\n",
    "            WordNetPOS = [POSconvert(P[1]) for P in POS]  # POS tags for lemmatizer\n",
    "            for j in range(len(txt_list)):\n",
    "                word = txt_list[j]\n",
    "                lemmatized = lemmatizer.lemmatize(word, WordNetPOS[j])  # process each token/word one by one\n",
    "                if lemmatized in emoticon_strings:                      # put the emoticons back in\n",
    "                    lemmatized = string2emoticon[lemmatized]\n",
    "                txt_list[j] = lemmatized                                # update the word in the txt_list\n",
    "\n",
    "            # UPDATE the dictionary\n",
    "            output_txt[id] = ' '.join(txt_list)\n",
    "            output[id] = txt_list\n",
    "\n",
    "        txt_dicts[i] = output_txt\n",
    "        txtlist_dicts.append(output)\n",
    "\n",
    "text_train = txt_dicts[0]\n",
    "text_test1 = txt_dicts[1]\n",
    "text_test2 = txt_dicts[2]\n",
    "text_test3 = txt_dicts[3]\n",
    "text_dev = txt_dicts[4]\n",
    "txtlist_train = txtlist_dicts[0]\n",
    "txtlist_test1 = txtlist_dicts[1]\n",
    "txtlist_test2 = txtlist_dicts[2]\n",
    "txtlist_test3 = txtlist_dicts[3]\n",
    "txtlist_dev = txtlist_dicts[4]\n",
    "\n",
    "# saving preprocessing.pkl\n",
    "file_to_save = \"preprocessing-plain.pkl\"\n",
    "if not os.path.isfile(file_to_save):\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev, txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev]\n",
    "    with open(file_to_save, 'wb') as out_file:\n",
    "        pickle.dump(txt_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felt privilege to play foo fighter song on guitar today with one of the plectrum from the gig on saturday\n",
      "@aaqibafzaal pakistan may be an islamic country but der be lot true muslim in india who love their country and can sacrifice all for it\n",
      "happy birthday to the coolest golfer in bali @tjvictoriacnd :) may you become cooler and cooler everyday stay humble little sister xx\n",
      "@simpplya tmills be go to tucson but the 29th and it on thursday :(\n",
      "hmmmmm where be the #blacklivesmatter when matter like this rise kid be disgrace\n",
      "@hypable all good im excite about 3rd season find home on netflix just want to make sure the reader have the news a it develop\n",
      "told my mom want to stay in hotel for my 18th with people but my birthday on valentine :-)) lucky me\n",
      "1st thing do after baggage claim be get up to date with @ronnaandbeverly bad blood ronna bev style make be home bit more okay\n",
      "bobby jindal want you to assimilate to heritage of the old confederacy even though his parent be from january dot dot dot\n",
      "@coolcat1304 watch itthe 1st time votedit in the nta award best factual program mite needwatch rest on itvplayer\n",
      "@lilbeast03 im sad that naruto manga be over and ill probably cry when the anime end kishimotosan be come to the u in october\n",
      "feel down this monday check out this guy who dress a baby prince george for week it might cheer you up\n",
      "huge play by #georgia to get out of the shadow of their own goal line on 2nd down lambert to sony for ayard gain very accurate pas\n",
      "kasich seek gop presidential nomination john kasich announce his run for the white house intuesday\n",
      "why do this horse have the same look of fear in it eye when next to marine le pen a romney do when next to tru\n",
      "ben carson be consider for the same cabinet position jack bauer once have mister carson youre no jack bauer\n",
      "john kasichs tone of surrender may not have faze the biased buckeye audience but it certainly bother the gop base @tperkins\n",
      "hommage to gary carter and the expo day tomorrow at rcup dollar hot dog andtix at thelevel #youppiwillbehere\n",
      "@pocketvolcano take it you hear ric flair be at dreamwave in jan\n",
      "@renuudesai yes mambetter google it you know some people dont have any work may create it a big scenethiz happen for many celebtc\n",
      "colour #3dprinting be en vogue for luxury eyewear maker @safilo1934\n",
      "@immortaltech dublin this saturday get ta get on the guinness\n",
      "@transferdicky antiimmigration sentiment be merely piece in bigger puzzle\n",
      "really like holly holm but be she ready for rousey thought\n",
      "which mean if andrade win she could get rousey next\n",
      "why do the medium keep give this brain dead moron air time former alaska gov sarah palin say sunday she\n",
      "@daniboothang @indeliblemarq__ myyr old cousin didnt know ice cube be rapperjust an superstar actor from the friday movie lol\n",
      "club remix next saturday night allstar will be in the building @geezyallstar @crazyronallstar\n",
      "@talk2cleo @kerrymacuska thats right lady im blame everything on kerry cleo this show may well eclipse the number of @jakeneedham\n",
      "cliff avril leave detroit lion game with back injury detroit lion defensive end cliff avril leave sunday ga\n",
      "make me sick to think we the american taxpayer will be pay for secret service detail for trump melania for\n",
      "@jimyeoman @pedallingveg bollock then theres ian brady myra hindley some bad hombre from that city see what do there\n",
      "my topare probably granny chiyo sakura v sasori guy v kissame the 2nd time and naruto v pain\n",
      "the guy at dunkin just say see you tomorrow because he know that im there every day\n",
      "watch the lookalikes teaser on sunday brunch be the only one who think the david beckham lookalike be definitely not lookalike\n",
      "dont think chelsea should appeal for torres red card let sturridge play tomorrow it the carling cup not pl\n",
      "@jonathamingo in other related news you know theres no park tomorrow\n",
      "an older article cite the legal issue behind the dakota access pipeline #dapl\n",
      "today life lesson courtesy of alabama if youre gon na play in texas you get ta have fiddle in the band\n",
      "we a people must do whatever it take to fight obamas lawlessness communist bent on destroy our heritage now not tomorrow\n",
      "you have to watch michael moore in trumplandi catch it on sho2\n",
      "my middle daughter just tell me 5th grade presentation in her class be on seth rollins #school #wwe #education\n",
      "mack sure you check out @carolinakidz1 and @trilla_guapo this saturday @club bodi\n",
      "uk release of star war episode vii the force awaken to be day earlier than expect dec\n",
      "nialls go to beindays no one gon na beill lock him in forever211 prepares squad cmon\n",
      "oracle set the date for it first quarter fiscal yearearnings announcement redwood shore ca mar\n",
      "@jayjbooth such close call cheer pal do well first month at ibm do miss the friday call though haha hope youre well\n",
      "@mdavisbot @justanactor sweet jesus #thewalkingdead\n",
      "@petestavros @megynkelly well good for you\n",
      "phone to consider if you really want the galaxy note via @yahoo\n",
      "report in nigeria say a many aspeople may have be kill after raid on village in the northeast by suspected boko haram\n",
      "@planetmoney be you sure of the bet which one can make in vega eg gatorade katy perry think those may only be available offshore\n",
      "the sunday daily the rock hulk hogan pay the price for racist rant dwayne the rock johnson pix #prkdr\n",
      "russia denies deal with iran over military cooperation in meeting on wednesday break news buzz\n",
      "at least we wont face de bruyne for wolfsburg in the champion league or against city when they get barca in the 2nd round\n",
      "oxygen vh1 get the most rating on monday night lol we aint play no game cant wait till tonight\n",
      "like how the bbc always load against brexit vd on now\n",
      "@juliomc69 it just get me hot people trash may for be cocky but not rousey\n",
      "after many month of wait and cancel wembley date along the way finally get to see foo fighter tomorrow excite time\n",
      "@illessa base on the 1st ep mr robot seem good series to go on look to respect the audience in the same way a hannibal least\n",
      "@kanakmanidixit you be free to join yakub tomorrow if you be feel guilty dont mind be use freedom of speech :)\n",
      "remember this one weekend have see fat nick pouya on friday disneyland on saturday and the beach my friend on sunday lit\n",
      "royal birthday @work happy 2nd prince george from all of u across the pond @constitution park pool\n",
      "cant do this 3rd day of eid and im back to work\n",
      "kris bryant line out to lf to end the top of the 1st strand runner on 1st 2nd with no score between the #cubs brewer\n",
      "the only thing scarier than kanye west run for president be that there slight chance kim kardashian may be the first lady\n",
      "blue jaysgames out with david price on the mound tomorrow\n",
      "the exchange club of winona and the eagle club will be host tater for tot to raise money for child abuse prevention on october 31st\n",
      "want to be batman on halloween but im go to be busy on that day even when it saturday tttt\n",
      "frank ocean can go to hell now idgaf about him no more when baby drop her album\n",
      "if didnt have train tomorrow id be sick and at the jason aldean concert #maybenexttime\n",
      "@yahoonews billionaire woman who make fortune in private business which be normalway not in goverment post a per repubicans dems\n",
      "happy 2nd birthday prince george kensington palace celebrate the young royal 2nd birthday with another\n",
      "what be the fafsa and pell grant begin college what be the fafsa and pell grant october2012 cate\n",
      "be on guard #blacklivesmatter be attack police hillary podesta soros all behind this they lose #blm need to be arrest #pizzagate\n",
      "brock lesnars return to msg to air live on the wwe network it be announce on sunday night during summersla\n",
      "galaxy notebanned in u flight violator will be send to prison\n",
      "direct donate link for that senate seat dems can still win\n",
      "snapburning the flag should be #maga #boycottcnn #boycottmsnbc #boycottnfl\n",
      "nicki do not have the time to be shady she will deadass drop the tea on ya lap and let you get 3rd degree burn\n",
      "march 3rddo you think kevin rudd be superficial yesvotes novotes rudd be reveal a failure #auspol\n",
      "cristiano ronaldo only have four friend at #realmadrid\n",
      "transient night with stranger can be hold forever in music leonard cohen on the inspiration behind song\n",
      "with shawn death anniversary on the 21st markingyrs without him well my heart and crystal feel heavy know if he be still here\n",
      "so notice on syfy website you can vote if april life or dy in sharknadoessentially they announce shaknadowhat great day\n",
      "@clarkhoward to add to your budget smartphone list may introduce to you the new moto 3rd gen which be unlocked and start at\n",
      "@mabasalamah assalaam it be my 1st eid away from family hope you have nice eid and all family be good happy eid to you too thanks\n",
      "this may be hard to believe be watch seinfelds the contest for the very first time\n",
      "chad sentence boko haram member to death for june attack recent month however have see the group stage\n",
      "last day of work at walgreens tomorrow cry tear of joy\n",
      "@the_overtones look like you guy have fun in sunny scotlandlooks a if therell be some hungovertones today #hadto\n",
      "#news today inaugust 24th nirvana play at the today inaugust 24th nirvana #keepgrunge\n",
      "go to see jason aldean tomorrow with the best friend kinda very excite\n",
      "ghetto at the center of the world chungking mansion hong kong\n",
      "all want for my birthday tomorrow be to go see jurassic world again thats literally the only thing want\n",
      "you cant shit talk kpop every group be so in sync and vocally on point it may not be your thing but it talent\n",
      "just meet the player for the 1st time this roger johnson fella walk around like he own the place so debagged him flick his helmet\n",
      "with the 1st pick in the classic movie lotto mark bonnell selects north by northwest #classicmovielotto\n",
      "janet street porter shock viewer with her comment about prince george well do jsp bang on right\n",
      "be announce an award the same a present it bc be announc itbut what about the actual golden globe show in january\n"
     ]
    }
   ],
   "source": [
    "# checking the preprocessed output\n",
    "for id in list(IDs_train.values())[0:100]:\n",
    "    print(text_train[id])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## Preprocessing 2: GloVe - replacing usernames with 'username', hashtags with 'hashtag' and keeping only GloVe emoticons\n",
    "\n",
    "# loading preprocessed datasets - if you already have the preprocessed file\n",
    "file_to_load = \"preprocessing-glove.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(file_to_load, 'rb') as inp_file:\n",
    "        temp_dicts = pickle.load(inp_file)\n",
    "        txt_dicts = temp_dicts[0:5]\n",
    "        txtlist_dicts = temp_dicts[5:]\n",
    "\n",
    "else:\n",
    "    top100 = ['com', 'net', 'org', 'jp', 'de', 'uk', 'fr', 'br', 'it', 'ru', 'es', 'me', 'gov', 'pl', 'ca', 'au', 'cn', 'co', 'in', 'nl', 'edu', 'info', 'eu', 'ch', 'id', 'at', 'kr', 'cz', 'mx', 'be', 'tv', 'se', 'tr', 'tw', 'al', 'ua', 'ir', 'vn', 'cl', 'sk', 'ly', 'cc', 'to', 'no', 'fi', 'us', 'pt', 'dk', 'ar', 'hu', 'tk', 'gr', 'il', 'news', 'ro', 'my', 'biz', 'ie', 'za', 'nz', 'sg', 'ee', 'th', 'io', 'xyz', 'pe', 'bg', 'hk', 'rs', 'lt', 'link', 'ph', 'club', 'si', 'site', 'mobi', 'by', 'cat', 'wiki', 'la', 'ga', 'xxx', 'cf', 'hr', 'ng', 'jobs', 'online', 'kz', 'ug', 'gq', 'ae', 'is', 'lv', 'pro', 'fm', 'tips', 'ms', 'sa', 'app', 'lat']\n",
    "\n",
    "    # emoticons in the glove embeddings\n",
    "    glove_emoticons = [';)', '=)', ':]', ':3', ':(', ':-)', '0:3', ':@', ':)', ':|', '=p']\n",
    "    glove_emoticon_strings = ['emoticon' + str(num) for num in range(len(emoticons))]\n",
    "    emoticon2string = dict(zip(glove_emoticons, glove_emoticon_strings))\n",
    "    string2emoticon = dict(zip(glove_emoticon_strings, glove_emoticons))\n",
    "\n",
    "    ID_dicts = [IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev]\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev]\n",
    "    txtlist_dicts = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  # init the lemmatizer\n",
    "    POSconvert = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "\n",
    "    for i, IDdict in enumerate(ID_dicts):\n",
    "        output = txt_dicts[i]\n",
    "        output_txt = {}\n",
    "        for id in IDdict.values():\n",
    "            text = output[id].lower()\n",
    "\n",
    "            # replace/delete all URLs starting with 'http' and 'www'\n",
    "            new_text = re.sub(\"http[^\\s]*\", '', text)\n",
    "            new_text = re.sub(\"www[^\\s]*\", '', new_text)\n",
    "\n",
    "            # delete all URLs which have one of 100 most common extensions ('.com', '.net', ...)\n",
    "            for ext in top100:\n",
    "                re_string = \"[^\\s]*\\.\" + ext + \"[^\\s]*\"\n",
    "                new_text = re.sub(re_string, '', new_text)\n",
    "\n",
    "            #replace all emoticons with an emoticon string:  #emoticon42\n",
    "            for em in glove_emoticons:\n",
    "                re_string = '\\s' + re.escape(em) + '\\s'\n",
    "                replace_string = ' ' + emoticon2string[em] + ' '\n",
    "                new_text = re.sub(re_string, replace_string, new_text)\n",
    "\n",
    "            # removing '&amp'\n",
    "            new_text = re.sub('&amp','', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars except for '# and @'\n",
    "            new_text = re.sub('[^\\w\\s@#]','', new_text)\n",
    "\n",
    "            # replace all @usernames with 'username'\n",
    "            new_text = re.sub('\\s@[^\\s]+',' username', new_text)  # middle\n",
    "            new_text = re.sub('^@[^\\s]+','username', new_text)    # start\n",
    "\n",
    "            # remove strings with '#' not on the beginning (to keep only hashtags)\n",
    "            new_text = re.sub('\\s[\\w]+#[\\w]*','', new_text)\n",
    "\n",
    "            # replace #hashtags with 'hashtag' and '#hashtag1 #hashtag2' with 'hashtags'\n",
    "            new_text = re.sub('#[^\\s]*\\s',' hashtag ', new_text)\n",
    "            new_text = re.sub('\\s#[^\\s]*$',' hashtag ', new_text)\n",
    "            new_text = re.sub('(\\s+hashtag){2,}', ' hashtags', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars\n",
    "            new_text = re.sub('[^\\w\\s]','', new_text)\n",
    "\n",
    "            # numbers fully made of digits\n",
    "            new_text = re.sub('\\s[\\d]+\\s','', new_text)\n",
    "\n",
    "            # remove words with only 1 character\n",
    "            new_text = re.sub('\\\\b\\\\w{1}\\\\b','', new_text)\n",
    "\n",
    "            # remove newline chars\n",
    "            new_text = new_text.replace('\\n', ' ')\n",
    "\n",
    "            # replace a multiple spaces with a single space\n",
    "            new_text = re.sub('\\s+',' ', new_text)\n",
    "\n",
    "            # using the lemmatizer\n",
    "            txt_list = nltk.word_tokenize(new_text)       # tokenise the tweet\n",
    "            POS = nltk.pos_tag(txt_list)                  # POS tag the tweet\n",
    "            WordNetPOS = [POSconvert(P[1]) for P in POS]  # convert POS tags to use in lemmatizer\n",
    "            for j in range(len(txt_list)):\n",
    "                word = txt_list[j]\n",
    "                lemmatized = lemmatizer.lemmatize(word, WordNetPOS[j])  # process each token/word one by one\n",
    "                if lemmatized in glove_emoticon_strings:                # replace the emoticon strings\n",
    "                    lemmatized = string2emoticon[lemmatized]\n",
    "                txt_list[j] = lemmatized                                # update the word in the txt_list\n",
    "\n",
    "            # UPDATE the dictionary\n",
    "            output_txt[id] = ' '.join(txt_list)\n",
    "            output[id] = txt_list\n",
    "\n",
    "        txt_dicts[i] = output_txt\n",
    "        txtlist_dicts.append(output)\n",
    "\n",
    "text_train = txt_dicts[0]\n",
    "text_test1 = txt_dicts[1]\n",
    "text_test2 = txt_dicts[2]\n",
    "text_test3 = txt_dicts[3]\n",
    "text_dev = txt_dicts[4]\n",
    "txtlist_train = txtlist_dicts[0]\n",
    "txtlist_test1 = txtlist_dicts[1]\n",
    "txtlist_test2 = txtlist_dicts[2]\n",
    "txtlist_test3 = txtlist_dicts[3]\n",
    "txtlist_dev = txtlist_dicts[4]\n",
    "\n",
    "# saving the preprocessed dictionaries as preprocessing-glove.pkl\n",
    "file_to_save = \"preprocessing-glove.pkl\"\n",
    "if not os.path.isfile(file_to_save):\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev, txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev]\n",
    "    with open(file_to_save, 'wb') as out_file:\n",
    "        pickle.dump(txt_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felt privilege to play foo fighter song on guitar today with one of the plectrum from the gig on saturday\n",
      "username pakistan may be an islamic country but der be lot true muslim in india who love their country and can sacrifice all for it\n",
      "happy birthday to the coolest golfer in bali username :) may you become cooler and cooler everyday stay humble little sister xx\n",
      "username tmills be go to tucson but the 29th and it on thursday\n",
      "hmmmmm where be the hashtag when matter like this rise kid be disgrace\n",
      "username all good im excite about 3rd season find home on netflix just want to make sure the reader have the news it develop\n",
      "told my mom want to stay in hotel for my 18th with people but my birthday on valentine lucky me\n",
      "1st thing do after baggage claim be get up to date with username bad blood ronna bev style make be home bit more okay\n",
      "bobby jindal want you to assimilate to heritage of the old confederacy even though his parent be from january dot dot dot\n",
      "username watch itthe 1st time votedit in the nta award best factual program mite needwatch rest on itvplayer\n",
      "username im sad that naruto manga be over and ill probably cry when the anime end kishimotosan be come to the in october\n",
      "feel down this monday check out this guy who dress baby prince george for week it might cheer you up\n",
      "huge play by hashtag to get out of the shadow of their own goal line on 2nd down lambert to sony for ayard gain very accurate pa\n",
      "kasich seek gop presidential nomination john kasich announce his run for the white house intuesday\n",
      "why do this horse have the same look of fear in it eye when next to marine le pen romney do when next to tru\n",
      "ben carson be consider for the same cabinet position jack bauer once have mister carson youre no jack bauer\n",
      "john kasichs tone of surrender may not have faze the biased buckeye audience but it certainly bother the gop base username\n",
      "hommage to gary carter and the expo day tomorrow at rcup dollar hot dog andtix at thelevel hashtag\n",
      "username take it you hear ric flair be at dreamwave in jan\n",
      "username yes mambetter google it you know some people dont have any work may create it big scenethiz happen for many celebtc\n",
      "colour hashtag be en vogue for luxury eyewear maker username\n",
      "username dublin this saturday get ta get on the guinness\n",
      "username antiimmigration sentiment be merely piece in bigger puzzle\n",
      "really like holly holm but be she ready for rousey thought\n",
      "which mean if andrade win she could get rousey next\n",
      "why do the medium keep give this brain dead moron air time former alaska gov sarah palin say sunday she\n",
      "username username myyr old cousin didnt know ice cube be rapperjust an superstar actor from the friday movie lol\n",
      "club remix next saturday night allstar will be in the building username username\n",
      "username username thats right lady im blame everything on kerry cleo this show may well eclipse the number of username\n",
      "cliff avril leave detroit lion game with back injury detroit lion defensive end cliff avril leave sunday ga\n",
      "make me sick to think we the american taxpayer will be pay for secret service detail for trump melania for\n",
      "username username bollock then theres ian brady myra hindley some bad hombre from that city see what do there\n",
      "my topare probably granny chiyo sakura sasori guy kissame the 2nd time and naruto pain\n",
      "the guy at dunkin just say see you tomorrow because he know that im there every day\n",
      "watch the lookalikes teaser on sunday brunch be the only one who think the david beckham lookalike be definitely not lookalike\n",
      "dont think chelsea should appeal for torres red card let sturridge play tomorrow it the carling cup not pl\n",
      "username in other related news you know theres no park tomorrow\n",
      "an older article cite the legal issue behind the dakota access pipeline hashtag\n",
      "today life lesson courtesy of alabama if youre gon na play in texas you get ta have fiddle in the band\n",
      "we people must do whatever it take to fight obamas lawlessness communist bent on destroy our heritage now not tomorrow\n",
      "you have to watch michael moore in trumplandi catch it on sho2\n",
      "my middle daughter just tell me 5th grade presentation in her class be on seth rollins hashtags\n",
      "mack sure you check out username and username this saturday username bodi\n",
      "uk release of star war episode vii the force awaken to be day earlier than expect dec\n",
      "nialls go to beindays no one gon na beill lock him in forever211 prepares squad cmon\n",
      "oracle set the date for it first quarter fiscal yearearnings announcement redwood shore ca mar\n",
      "username such close call cheer pal do well first month at ibm do miss the friday call though haha hope youre well\n",
      "username username sweet jesus hashtag\n",
      "username username well good for you\n",
      "phone to consider if you really want the galaxy note via username\n",
      "report in nigeria say many aspeople may have be kill after raid on village in the northeast by suspected boko haram\n",
      "username be you sure of the bet which one can make in vega eg gatorade katy perry think those may only be available offshore\n",
      "the sunday daily the rock hulk hogan pay the price for racist rant dwayne the rock johnson pix hashtag\n",
      "russia denies deal with iran over military cooperation in meeting on wednesday break news buzz\n",
      "at least we wont face de bruyne for wolfsburg in the champion league or against city when they get barca in the 2nd round\n",
      "oxygen vh1 get the most rating on monday night lol we aint play no game cant wait till tonight\n",
      "like how the bbc always load against brexit vd on now\n",
      "username it just get me hot people trash may for be cocky but not rousey\n",
      "after many month of wait and cancel wembley date along the way finally get to see foo fighter tomorrow excite time\n",
      "username base on the 1st ep mr robot seem good series to go on look to respect the audience in the same way hannibal least\n",
      "username you be free to join yakub tomorrow if you be feel guilty dont mind be use freedom of speech\n",
      "remember this one weekend have see fat nick pouya on friday disneyland on saturday and the beach my friend on sunday lit\n",
      "royal birthday username happy 2nd prince george from all of across the pond username park pool\n",
      "cant do this 3rd day of eid and im back to work\n",
      "kris bryant line out to lf to end the top of the 1st strand runner on 1st 2nd with no score between the hashtag brewer\n",
      "the only thing scarier than kanye west run for president be that there slight chance kim kardashian may be the first lady\n",
      "blue jaysgames out with david price on the mound tomorrow\n",
      "the exchange club of winona and the eagle club will be host tater for tot to raise money for child abuse prevention on october 31st\n",
      "want to be batman on halloween but im go to be busy on that day even when it saturday tttt\n",
      "frank ocean can go to hell now idgaf about him no more when baby drop her album\n",
      "if didnt have train tomorrow id be sick and at the jason aldean concert hashtag\n",
      "username billionaire woman who make fortune in private business which be normalway not in goverment post per repubicans dems\n",
      "happy 2nd birthday prince george kensington palace celebrate the young royal 2nd birthday with another\n",
      "what be the fafsa and pell grant begin college what be the fafsa and pell grant october2012 cate\n",
      "be on guard hashtag be attack police hillary podesta soros all behind this they lose hashtag need to be arrest hashtag\n",
      "brock lesnars return to msg to air live on the wwe network it be announce on sunday night during summersla\n",
      "galaxy notebanned in flight violator will be send to prison\n",
      "direct donate link for that senate seat dems can still win\n",
      "snapburning the flag should be hashtags\n",
      "nicki do not have the time to be shady she will deadass drop the tea on ya lap and let you get 3rd degree burn\n",
      "march 3rddo you think kevin rudd be superficial yesvotes novotes rudd be reveal failure hashtag\n",
      "cristiano ronaldo only have four friend at hashtag\n",
      "transient night with stranger can be hold forever in music leonard cohen on the inspiration behind song\n",
      "with shawn death anniversary on the 21st markingyrs without him well my heart and crystal feel heavy know if he be still here\n",
      "so notice on syfy website you can vote if april life or dy in sharknadoessentially they announce shaknadowhat great day\n",
      "username to add to your budget smartphone list may introduce to you the new moto 3rd gen which be unlocked and start at\n",
      "username assalaam it be my 1st eid away from family hope you have nice eid and all family be good happy eid to you too thanks\n",
      "this may be hard to believe be watch seinfelds the contest for the very first time\n",
      "chad sentence boko haram member to death for june attack recent month however have see the group stage\n",
      "last day of work at walgreens tomorrow cry tear of joy\n",
      "username look like you guy have fun in sunny scotlandlooks if therell be some hungovertones today hashtag\n",
      "hashtag today inaugust 24th nirvana play at the today inaugust 24th nirvana hashtag\n",
      "go to see jason aldean tomorrow with the best friend kinda very excite\n",
      "ghetto at the center of the world chungking mansion hong kong\n",
      "all want for my birthday tomorrow be to go see jurassic world again thats literally the only thing want\n",
      "you cant shit talk kpop every group be so in sync and vocally on point it may not be your thing but it talent\n",
      "just meet the player for the 1st time this roger johnson fella walk around like he own the place so debagged him flick his helmet\n",
      "with the 1st pick in the classic movie lotto mark bonnell selects north by northwest hashtag\n",
      "janet street porter shock viewer with her comment about prince george well do jsp bang on right\n",
      "be announce an award the same present it bc be announc itbut what about the actual golden globe show in january\n"
     ]
    }
   ],
   "source": [
    "# checking the preprocessed output\n",
    "for id in list(IDs_train.values())[0:100]:\n",
    "    print(text_train[id])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: Bag of words\n",
    "Bag of words vectorisation: please note that the following code can take up to 5-10 minutes to run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Bag of Words (BoW) feature extraction - my implementation:\n",
    "\n",
    "# loading preprocessed BoW file if it exists\n",
    "file_to_load =  \"BOWsparse.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(file_to_load, 'rb') as inp_file:\n",
    "        temp = pickle.load(inp_file)  # BOWsparse-plain.pkl has only 6 parts, doesn't have vocabularyin it\n",
    "        [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev, vocabulary] = temp\n",
    "else:\n",
    "    # loading preprocessing data\n",
    "    file_to_load = \"preprocessing-plain.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            t = pickle.load(inp_file)\n",
    "            text_train, text_test1, text_test2, text_test3, text_dev = t[0], t[1], t[2], t[3], t[4]\n",
    "            txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev = t[5], t[6], t[7], t[8], t[9]\n",
    "\n",
    "    ## 1) removing stop words\n",
    "    stopwords = Stopwords.words('english')\n",
    "    stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "    ## 2) extracting the dictionary/vocabulary\n",
    "    freq = FreqDist()   # frequency distribution\n",
    "    txtlist_dicts = [txtlist_train, txtlist_dev]\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in tweet:\n",
    "                if not word in stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "    nums = range(1, len(freq.keys())+1)\n",
    "    vocabulary = list(freq.keys())              # creating the dictionary\n",
    "    vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out of vocabulary' word\n",
    "    vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "    vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words -> len: 69742\n",
    "\n",
    "    BOW_train = {}\n",
    "    for ID, tweet in txtlist_train.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "        BOW_train[ID] = BOW\n",
    "\n",
    "    BOW_test1 = {}\n",
    "    for ID, tweet in txtlist_test1.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "        BOW_test1[ID] = BOW\n",
    "\n",
    "    BOW_test2 = {}\n",
    "    for ID, tweet in txtlist_test2.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "        BOW_test2[ID] = BOW\n",
    "\n",
    "    BOW_test3 = {}\n",
    "    for ID, tweet in txtlist_test3.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "        BOW_test3[ID] = BOW\n",
    "\n",
    "    BOW_dev = {}\n",
    "    for ID, tweet in txtlist_dev.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num)\n",
    "        BOW_dev[ID] = BOW\n",
    "\n",
    "\n",
    "    print(\"Starting sparse processing.\")\n",
    "\n",
    "    # sparse representation -> BOW vectors are sparse, so sparse representation saves memory and time\n",
    "    vector_list = [BOW_train[id] for id in BOW_train.keys()]\n",
    "    dense_train = np.vstack(vector_list)    # shape (45101, 67761)\n",
    "    sparse_train = csr_matrix(dense_train)  # getting the sparse matrix\n",
    "    del dense_train # remove from memory\n",
    "\n",
    "    vector_list = [BOW_test1[id] for id in BOW_test1.keys()]\n",
    "    dense_test1 = np.vstack(vector_list)    # shape (3531, 67761)\n",
    "    sparse_test1 = csr_matrix(dense_test1)  # getting the sparse matrix\n",
    "    del dense_test1 # remove from memory\n",
    "\n",
    "    vector_list = [BOW_test2[id] for id in BOW_test2.keys()]\n",
    "    dense_test2 = np.vstack(vector_list)    # shape (1853, 67761)\n",
    "    sparse_test2 = csr_matrix(dense_test2)  # getting the sparse matrix\n",
    "    del dense_test2 # remove from memory\n",
    "\n",
    "    vector_list = [BOW_test3[id] for id in BOW_test3.keys()]\n",
    "    dense_test3 = np.vstack(vector_list)    # shape (2379, 67761)\n",
    "    sparse_test3 = csr_matrix(dense_test3)  # getting the sparse matrix\n",
    "    del dense_test3 # remove from memory\n",
    "\n",
    "    vector_list = [BOW_dev[id] for id in BOW_dev.keys()]\n",
    "    dense_dev = np.vstack(vector_list)      # shape (2000, 67761)\n",
    "    sparse_dev = csr_matrix(dense_dev)      # getting the sparse matrix\n",
    "    del dense_dev # remove from memory\n",
    "\n",
    "    # train + dev together (combined)\n",
    "    vector_list1 = [BOW_train[id] for id in BOW_train.keys()]\n",
    "    vector_list2 = [BOW_dev[id] for id in BOW_dev.keys()]\n",
    "    temp1 = np.vstack(vector_list1)\n",
    "    temp2 = np.vstack(vector_list2)\n",
    "    dense_train_dev = np.vstack((temp1, temp2))     # shape (48632, 67761)\n",
    "    sparse_train_dev = csr_matrix(dense_train_dev)  # getting the sparse matrix\n",
    "    del dense_train_dev # remove from memory\n",
    "\n",
    "\n",
    "# save the sparse representation\n",
    "file_to_save = \"BOWsparse.pkl\"\n",
    "if not os.path.isfile(file_to_save):\n",
    "    sparse_dicts = [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev, vocabulary]\n",
    "    with open(file_to_save, 'wb') as out_file:\n",
    "        pickle.dump(sparse_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: TF-IDF weighted Bag of words\n",
    "Weighted BOW vectorisation - each word in a tweet is weighted according to its TFIDF\n",
    "Please note that the following code can take up to 5-10 minutes to run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# TFIDF feature extraction\n",
    "\n",
    "# loading preprocessed BoW file if it exists\n",
    "file_to_load =  \"TFIDFsparse.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(file_to_load, 'rb') as inp_file:\n",
    "        temp = pickle.load(inp_file)\n",
    "        [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev, vocabulary] = temp\n",
    "else:\n",
    "    # loading preprocessing data\n",
    "    file_to_load = \"preprocessing-plain.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            t = pickle.load(inp_file)\n",
    "            text_train, text_test1, text_test2, text_test3, text_dev = t[0], t[1], t[2], t[3], t[4]\n",
    "            txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev = t[5], t[6], t[7], t[8], t[9]\n",
    "\n",
    "    # extracting the dictionary\n",
    "    freq = FreqDist()   # frequency distribution\n",
    "    txtlist_dicts = [txtlist_train, txtlist_dev]\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in tweet:\n",
    "                if not word in stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "    nums = range(1,len(freq.keys())+1)\n",
    "    vocabulary = list(freq.keys())              # creating the dictionary\n",
    "    vocabulary_array = np.array(vocabulary)     # np.array of the dictionary\n",
    "    vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out-of-vocabulary' word\n",
    "    vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "    vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words\n",
    "\n",
    "    # extracting the dictionary\n",
    "    DFfreq = FreqDist()   # document frequency distribution\n",
    "    Ntexts = len(IDs_train) + len(IDs_dev)\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in np.unique(tweet):\n",
    "                if not word in stopwords:\n",
    "                    DFfreq[word] += 1\n",
    "\n",
    "\n",
    "    # TFIDF-weighted Bag of Words for each tweet\n",
    "    TFIDF_train = {}\n",
    "    for ID, tweet in txtlist_train.items():\n",
    "        tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "        TFIDF_train[ID] = tfidf\n",
    "\n",
    "    TFIDF_test1 = {}\n",
    "    for ID, tweet in txtlist_test1.items():\n",
    "        tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "        TFIDF_test1[ID] = tfidf\n",
    "\n",
    "    TFIDF_test2 = {}\n",
    "    for ID, tweet in txtlist_test2.items():\n",
    "        tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "        TFIDF_test2[ID] = tfidf\n",
    "\n",
    "    TFIDF_test3 = {}\n",
    "    for ID, tweet in txtlist_test3.items():\n",
    "        tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "        TFIDF_test3[ID] = tfidf\n",
    "\n",
    "    TFIDF_dev = {}\n",
    "    for ID, tweet in txtlist_dev.items():\n",
    "        tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, vocab2num=vocab2num, DFfreq=DFfreq, Ntexts=Ntexts)\n",
    "        TFIDF_dev[ID] = tfidf\n",
    "\n",
    "    print(\"Starting sparse processing.\")\n",
    "\n",
    "    # sparse TFIDF representation\n",
    "    vector_list = [TFIDF_train[id] for id in TFIDF_train.keys()]\n",
    "    TFIDFdense_train = np.vstack(vector_list)\n",
    "    TFIDFsparse_train = csr_matrix(TFIDFdense_train)  # getting the sparse matrix\n",
    "    del TFIDFdense_train # remove from memory\n",
    "\n",
    "    vector_list = [TFIDF_test1[id] for id in TFIDF_test1.keys()]\n",
    "    TFIDFdense_test1 = np.vstack(vector_list)\n",
    "    TFIDFsparse_test1 = csr_matrix(TFIDFdense_test1)  # getting the sparse matrix\n",
    "    del TFIDFdense_test1 # remove from memory\n",
    "\n",
    "    vector_list = [TFIDF_test2[id] for id in TFIDF_test2.keys()]\n",
    "    TFIDFdense_test2 = np.vstack(vector_list)\n",
    "    TFIDFsparse_test2 = csr_matrix(TFIDFdense_test2)  # getting the sparse matrix\n",
    "    del TFIDFdense_test2 # remove from memory\n",
    "\n",
    "    vector_list = [TFIDF_test3[id] for id in TFIDF_test3.keys()]\n",
    "    TFIDFdense_test3 = np.vstack(vector_list)\n",
    "    TFIDFsparse_test3 = csr_matrix(TFIDFdense_test3)  # getting the sparse matrix\n",
    "    del TFIDFdense_test3 # remove from memory\n",
    "\n",
    "    vector_list = [TFIDF_dev[id] for id in TFIDF_dev.keys()]\n",
    "    TFIDFdense_dev = np.vstack(vector_list)\n",
    "    TFIDFsparse_dev = csr_matrix(TFIDFdense_dev)      # getting the sparse matrix\n",
    "    del TFIDFdense_dev # remove from memory\n",
    "\n",
    "    vector_list1 = [TFIDF_train[id] for id in TFIDF_train.keys()]\n",
    "    vector_list2 = [TFIDF_dev[id] for id in TFIDF_dev.keys()]\n",
    "    temp1 = np.vstack(vector_list1)\n",
    "    temp2 = np.vstack(vector_list2)\n",
    "    TFIDFdense_train_dev = np.vstack((temp1, temp2))  # shape (45101, 59559)\n",
    "    TFIDFsparse_train_dev = csr_matrix(TFIDFdense_train_dev)  # getting the sparse matrix\n",
    "    del TFIDFdense_train_dev # remove from memory\n",
    "\n",
    "    # save the sparse representation of TFIDF features\n",
    "    file_to_save = \"TFIDFsparse.pkl\"\n",
    "    if not os.path.isfile(file_to_save):\n",
    "        sparse_dicts = [TFIDFsparse_train, TFIDFsparse_test1, TFIDFsparse_test2, TFIDFsparse_test3, TFIDFsparse_dev, TFIDFsparse_train_dev, vocabulary]\n",
    "        with open(file_to_save, 'wb') as out_file:\n",
    "            pickle.dump(sparse_dicts, out_file, protocol=-1)\n",
    "\n",
    "# loading preprocessed TFIDF sparse data\n",
    "# with open(\"TFIDFsparse.pkl\", 'rb') as inp_file:\n",
    "#     temp = pickle.load(inp_file)\n",
    "#     [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev] = temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: GloVe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the word vectors.\n"
     ]
    }
   ],
   "source": [
    "# APPROACH 1: Loading the word embeddings vectors from GloVE: selecting 5000 words based on frequency\n",
    "\n",
    "print('Extracting the word vectors.')\n",
    "\n",
    "## loading preprocessed embedding file if it exists (embedding matrix, word to index map, embedding dictionary)\n",
    "file_to_load =  \"embeddings.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(file_to_load, 'rb') as inp_file:\n",
    "        temp = pickle.load(inp_file)\n",
    "        [embedding_matrix, word2ID, embedding_dict] = temp\n",
    "else:\n",
    "    ## loading preprocessed data for glove\n",
    "    file_to_load = \"preprocessing-glove.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            t = pickle.load(inp_file)\n",
    "            text_train, text_test1, text_test2, text_test3, text_dev = t[0], t[1], t[2], t[3], t[4]\n",
    "            txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev = t[5], t[6], t[7], t[8], t[9]\n",
    "\n",
    "    full_embedding_dict = {}\n",
    "    glove_path = join('..','glove', 'glove.6B.100d.txt')\n",
    "    with open(glove_path, 'r', encoding='utf-8') as File:\n",
    "        for line in File:\n",
    "            vec = line.split()\n",
    "            word = vec[0]\n",
    "            coefs = np.asarray(vec[1:], dtype='float32')\n",
    "            full_embedding_dict[word] = coefs\n",
    "\n",
    "    print(f\"Extracted {len(full_embedding_dict)} word embedding vectors.\")\n",
    "\n",
    "    sorted_vocabulary = sorted([it for it in freq.items()], key=lambda data: data[1], reverse=True)\n",
    "    full_vocabulary = [ tup[0] for tup in sorted_vocabulary ]\n",
    "\n",
    "    embedding_dict = {}  # word embeddings of 6000 words from vocabulary\n",
    "    temp = 0\n",
    "    for word in full_vocabulary:\n",
    "        if word in full_embedding_dict.keys():\n",
    "            embedding_dict[word] = full_embedding_dict[word]\n",
    "            temp += 1\n",
    "        if temp == 4998:\n",
    "            break\n",
    "    vocabulary5000 = list(embedding_dict.keys())  # obtain the dictionary of 6000 most common words\n",
    "\n",
    "    print(f\"Created dictionary of {len(embedding_dict)} most common words.\")\n",
    "\n",
    "    ## extract the <OOV> vector by setting it to be the weighted avg of unused words\n",
    "    Total = np.zeros(100)\n",
    "    Sum = 0\n",
    "    for word in full_vocabulary:\n",
    "        if word not in vocabulary5000:                          # if word is not among 6000 words\n",
    "            if word in full_embedding_dict.keys():              # and it is in glove\n",
    "                Total += freq[word] * full_embedding_dict[word] # take the weighted avg\n",
    "                Sum += freq[word]\n",
    "    OOV_vector = Total / Sum\n",
    "    embedding_dict['<OOV>'] = OOV_vector\n",
    "\n",
    "    print(f\"The embedding dictionary has {len(embedding_dict)} words, the last one is: {list(embedding_dict.keys())[-1]}\")\n",
    "\n",
    "    ## Build an embedding matrix\n",
    "    word_list = list(embedding_dict.keys())\n",
    "    nums = range(1,len(word_list)+1)\n",
    "    word2ID = dict(zip(word_list, nums))     # the index of the embedding vector\n",
    "    num2vocab = dict(zip(nums, word_list))   # the index to word\n",
    "    vector_list = [embedding_dict[word] for word in word_list]\n",
    "\n",
    "    embedding_matrix = np.vstack(vector_list)\n",
    "    embedding_matrix = np.vstack((np.zeros(100), embedding_matrix))\n",
    "\n",
    "    print(f\"Created matrix with shape {embedding_matrix.shape}\")  # the first row is a dummy row\n",
    "    del full_embedding_dict # delete from memory\n",
    "    ## save the embeddings\n",
    "    with open(\"embeddings.pkl\", 'wb') as out_file:\n",
    "        temp = [embedding_matrix, word2ID, embedding_dict]\n",
    "        pickle.dump(temp, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 400000 word embedding vectors.\n",
      "Created matrix with shape (5000, 100)\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE APPROACH to preprocessing of embeddings: Weighted Approach - considering both semantic and frequency value\n",
    "# -> in the first approach, only frequency was the criterion to select 5000 words\n",
    "# -> but that way, we may select words which have minimal semantic value\n",
    "# -> the weighted approach will select both the semantic as well as frequency contribution\n",
    "\n",
    "\n",
    "# loading preprocessed embedding file if it exists (embedding matrix, word to index map, embedding dictionary)\n",
    "file_to_load =  \"embeddings-weighted.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(\"embeddings-weighted.pkl\", 'rb') as inp_file:\n",
    "        temp = pickle.load(inp_file)\n",
    "        [embedding_matrix, word2ID, embedding_dict] = temp\n",
    "else:\n",
    "    ## 0) loading preprocessing data for glove\n",
    "    file_to_load = \"preprocessing-glove.pkl\"\n",
    "    if os.path.isfile(file_to_load):\n",
    "        with open(file_to_load, 'rb') as inp_file:\n",
    "            t = pickle.load(inp_file)\n",
    "            text_train, text_test1, text_test2, text_test3, text_dev = t[0], t[1], t[2], t[3], t[4]\n",
    "            txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev = t[5], t[6], t[7], t[8], t[9]\n",
    "\n",
    "    ## 1) finding the frequency weights\n",
    "    txtlist_dicts = [txtlist_train, txtlist_dev]\n",
    "    freq = FreqDist()   # frequency distribution\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in tweet:\n",
    "                if not word in stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "    nums = range(1, len(freq.keys())+1)\n",
    "    vocabulary = list(freq.keys())              # creating the dictionary\n",
    "    vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "    num2vocab = dict(zip(nums, vocabulary))     # word to index mapping\n",
    "\n",
    "    sorted_vocabulary = sorted([it for it in freq.items()], key=lambda data: data[1], reverse=True)\n",
    "    maxF = sorted_vocabulary[0][1]  # maximal frequency of a word\n",
    "    normalized_vocabulary = { word: f / maxF for (word, f) in sorted_vocabulary}  # dictionary\n",
    "\n",
    "    ## 2) use SVM coeffs (make sure to have 'SVMcoefficients.pkl' which stores clf.coef_ from SVM)\n",
    "    with open(\"SVMcoefficients.pkl\", 'rb') as inp_file:\n",
    "        SVM_coef = pickle.load(inp_file)\n",
    "    coefs = []\n",
    "    coefs_nums = []\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        c = np.abs(SVM_coef[2,i] - SVM_coef[0,i])  # coefficent: absolute difference between positive and negative:\n",
    "        word_coef = (word, c)                            #  ->> \"most negative/most positive\" words have higher index\n",
    "        coefs_nums.append(c)\n",
    "        coefs.append(word_coef)\n",
    "    minC, maxC = np.min(coefs_nums), np.max(coefs_nums)\n",
    "    coefs = [(word, (c - minC) / maxC) for (word, c) in coefs]     # normalize coefs\n",
    "    coefs = sorted(coefs, key=lambda data: data[1], reverse=True)  # sort coefs\n",
    "\n",
    "    ## 3) apply the weight formula: 2 * freq_value + 1 * sentiment_value ~ 2 f + 1 s\n",
    "    weighted_scores = [(word, 1 * cn + 2 * normalized_vocabulary[word]) for (word, cn) in coefs]  # list of tuples\n",
    "    sorted_weighted_vocabulary = sorted([it for it in weighted_scores], key=lambda data: data[1], reverse=True)\n",
    "\n",
    "    ## 4) load GloVe embeddings\n",
    "    full_embedding_dict = {}\n",
    "    glove_path = join('..','glove', 'glove.6B.100d.txt')\n",
    "    with open(glove_path, 'r', encoding='utf-8') as File:\n",
    "        for line in File:\n",
    "            vec = line.split()\n",
    "            word = vec[0]\n",
    "            coefs = np.asarray(vec[1:], dtype='float32')\n",
    "            full_embedding_dict[word] = coefs\n",
    "    print(f\"Extracted {len(full_embedding_dict)} word embedding vectors.\")\n",
    "\n",
    "    ## 5) extract the word embeddings of 5000 words based on the order from the weighted approach\n",
    "    embedding_dict = {}\n",
    "    temp = 0\n",
    "    for (word,_) in sorted_weighted_vocabulary:\n",
    "        if word in full_embedding_dict.keys():\n",
    "            embedding_dict[word] = full_embedding_dict[word]\n",
    "            temp += 1\n",
    "        if temp == 4998:\n",
    "            break\n",
    "    vocabulary5000 = list(embedding_dict.keys())  # obtain the dictionary of 5000 most common words\n",
    "\n",
    "    ## 6) extract the <OOV> vector by setting it to be the weighted avg of unused words\n",
    "    Total = np.zeros(100)\n",
    "    Sum = 0\n",
    "    for word in full_vocabulary:\n",
    "        if word not in vocabulary5000:                          # if word is not among 5000 words\n",
    "            if word in full_embedding_dict.keys():              # and it is in glove\n",
    "                Total += freq[word] * full_embedding_dict[word] # take the weighted avg\n",
    "                Sum += freq[word]\n",
    "    OOV_vector = Total / Sum\n",
    "    embedding_dict['<OOV>'] = OOV_vector\n",
    "\n",
    "    ## 7) Build the weighted-embedding matrix\n",
    "    word_list = list(embedding_dict.keys())\n",
    "    nums = range(1,len(word_list)+1)\n",
    "    word2ID = dict(zip(word_list, nums))     # the index of the embedding vector\n",
    "    num2vocab = dict(zip(nums, word_list))   # the index to word\n",
    "    vector_list = [embedding_dict[word] for word in word_list]\n",
    "    embedding_matrix = np.vstack(vector_list)\n",
    "    embedding_matrix = np.vstack((np.zeros(100), embedding_matrix))\n",
    "\n",
    "    print(f\"Created matrix with shape {embedding_matrix.shape}\")  # the first row is a dummy row\n",
    "    del full_embedding_dict # delete from memory\n",
    "\n",
    "    # save the embeddings\n",
    "    with open(\"embeddings-weighted.pkl\", 'wb') as out_file:\n",
    "        temp = [embedding_matrix, word2ID, embedding_dict]\n",
    "        pickle.dump(temp, out_file, protocol=-1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrices for torch.\n"
     ]
    }
   ],
   "source": [
    "# preparing matrices for torch: matrix_train, matrix_dev, matrix_test1, matrix_test2, matrix_test3\n",
    "\n",
    "# loading preprocessed embeddings\n",
    "embedding_file = \"embeddings-weighted.pkl\"  # alternatively use \"embeddings.pkl\"\n",
    "with open(embedding_file, 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [embedding_matrix, word2ID, embedding_dict] = temp\n",
    "\n",
    "# converting the text lists into vectors of ints\n",
    "word_list = list(embedding_dict.keys())\n",
    "\n",
    "with open(\"preprocessing-glove.pkl\", 'rb') as inp_file:   # loading preprocessed data for glove\n",
    "    t = pickle.load(inp_file)\n",
    "    text_train, text_test1, text_test2, text_test3, text_dev = t[0], t[1], t[2], t[3], t[4]\n",
    "    txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev = t[5], t[6], t[7], t[8], t[9]\n",
    "\n",
    "with open(\"preprocessing-sent-ID.pkl\", 'rb') as inp_file: # loading preprocessed data\n",
    "    temp = pickle.load(inp_file)\n",
    "    [sent_train, sent_test1, sent_test2, sent_test3, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3,\n",
    "     IDs_dev] = temp\n",
    "\n",
    "max_len = np.max([len(tweet) for tweet in txtlist_train.values()])      # longest tokenized sentence\n",
    "matrix_train = np.zeros((len(txtlist_train), max_len), dtype=np.int16)  # training datapoints\n",
    "y_train = np.zeros(len(txtlist_train), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_train.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_train[id]\n",
    "    matrix_train[i] = x\n",
    "    y_train[i] = sent2num(y)\n",
    "\n",
    "matrix_test1 = np.zeros((len(txtlist_test1), max_len), dtype=np.int16)  # training datapoints\n",
    "y_test1 = np.zeros(len(txtlist_test1), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_test1.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_test1[id]\n",
    "    matrix_test1[i] = x\n",
    "    y_test1[i] = sent2num(y)\n",
    "\n",
    "matrix_test2 = np.zeros((len(txtlist_test2), max_len), dtype=np.int16)  # training datapoints\n",
    "y_test2 = np.zeros(len(txtlist_test2), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_test2.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_test2[id]\n",
    "    matrix_test2[i] = x\n",
    "    y_test2[i] = sent2num(y)\n",
    "\n",
    "matrix_test3 = np.zeros((len(txtlist_test3), max_len), dtype=np.int16)  # training datapoints\n",
    "y_test3 = np.zeros(len(txtlist_test3), dtype=np.int8)                   # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_test3.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_test3[id]\n",
    "    matrix_test3[i] = x\n",
    "    y_test3[i] = sent2num(y)\n",
    "\n",
    "matrix_dev = np.zeros((len(txtlist_dev), max_len), dtype=np.int16)  # training datapoints\n",
    "y_dev = np.zeros(len(txtlist_dev), dtype=np.int8)  # training labels\n",
    "for i, (id, text_list) in enumerate(txtlist_dev.items()):\n",
    "    x = text_list2array(text_list, vocabulary_list=word_list, word2index_dict=word2ID, max_length=max_len)\n",
    "    y = sent_dev[id]\n",
    "    matrix_dev[i] = x\n",
    "    y_dev[i] = sent2num(y)\n",
    "\n",
    "# save the data as pickle files\n",
    "with open(\"xy_train.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_train, y_train]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_dev.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_dev, y_dev]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_test1.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_test1, y_test1]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_test2.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_test2, y_test2]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "with open(\"xy_test3.pkl\", 'wb') as out_file:\n",
    "    temp = [matrix_test3, y_test3]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "\n",
    "# load the saved data from pickle files\n",
    "with open(\"xy_train.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_train, y_train] = temp\n",
    "with open(\"xy_dev.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_dev, y_dev] = temp\n",
    "with open(\"xy_test1.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_test1, y_test1] = temp\n",
    "with open(\"xy_test2.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_test2, y_test2] = temp\n",
    "with open(\"xy_test3.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [matrix_test3, y_test3] = temp\n",
    "\n",
    "print(\"Saved matrices for torch.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Sentiment Classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> NEARESTNEIGHBOUR: training time: 0.004\n",
      "../semeval-tweets/twitter-test1.txt (BOW-NearestNeighbour): 0.386\n",
      "../semeval-tweets/twitter-test2.txt (BOW-NearestNeighbour): 0.400\n",
      "../semeval-tweets/twitter-test3.txt (BOW-NearestNeighbour): 0.376\n",
      "--> NEARESTNEIGHBOUR: training time: 0.007\n",
      "../semeval-tweets/twitter-test1.txt (TFIDF-NearestNeighbour): 0.404\n",
      "../semeval-tweets/twitter-test2.txt (TFIDF-NearestNeighbour): 0.439\n",
      "../semeval-tweets/twitter-test3.txt (TFIDF-NearestNeighbour): 0.415\n"
     ]
    }
   ],
   "source": [
    "# classical classifiers data loading:\n",
    "\n",
    "\n",
    "with open(\"preprocessing-sent-ID.pkl\", 'rb') as inp_file: # loading preprocessed data\n",
    "    temp = pickle.load(inp_file)\n",
    "    [sent_train, sent_test1, sent_test2, sent_test3, sent_dev, IDs_train, IDs_test1, IDs_test2, IDs_test3,\n",
    "     IDs_dev] = temp\n",
    "\n",
    "\n",
    "for classifier in ['NearestNeighbour', 'NaiveBayes']:#,'SVM', 'MaxEnt']:\n",
    "    for features in ['BOW', 'TFIDF']:\n",
    "\n",
    "        if features == 'BOW':\n",
    "            with open(\"BOWsparse.pkl\", 'rb') as inp_file:\n",
    "                temp = pickle.load(inp_file)  # BOWsparse-plain.pkl has only 6 parts, doesn't have vocabularyin it\n",
    "                [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev, vocabulary] = temp\n",
    "\n",
    "            Xtrain = sparse_train_dev                      # combining the two datasets\n",
    "            ID_train = list(IDs_train.values())            # list of IDs in train set\n",
    "            ID_dev = list(IDs_dev.values())                # list of IDs in dev set\n",
    "            ID_train_dev = ID_train + ID_dev               # combined train, dev\n",
    "            sn_train = [sent_train[id] for id in ID_train] # training labels train\n",
    "            sn_dev = [sent_dev[id] for id in ID_dev]       # training labels dev\n",
    "            Ytrain = np.array(sn_train + sn_dev)           # combining both labels\n",
    "            Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])                      # numerical labels for train\n",
    "            y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])  # numerical labels for test1\n",
    "            y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])  # numerical labels for test2\n",
    "            y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])  # numerical labels for test3\n",
    "            testset_sparse_samples = [sparse_test1, sparse_test2, sparse_test3]           # list of testing examples\n",
    "            testset_IDs = [list(IDs_test1.values()), list(IDs_test2.values()), list(IDs_test3.values())]  # IDs in test1, test2, test3\n",
    "\n",
    "        if features == 'TFIDF':\n",
    "            with open(\"TFIDFsparse.pkl\", 'rb') as inp_file:\n",
    "                temp = pickle.load(inp_file)\n",
    "                [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev, vocabulary] = temp\n",
    "\n",
    "            Xtrain = sparse_train_dev                      # combining the two datasets\n",
    "            ID_train = list(IDs_train.values())            # list of IDs in train set\n",
    "            ID_dev = list(IDs_dev.values())                # list of IDs in dev set\n",
    "            ID_train_dev = ID_train + ID_dev               # combined train, dev\n",
    "            sn_train = [sent_train[id] for id in ID_train] # training labels train\n",
    "            sn_dev = [sent_dev[id] for id in ID_dev]       # training labels dev\n",
    "            Ytrain = np.array(sn_train + sn_dev)           # combining both labels\n",
    "            Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])                      # numerical labels for train\n",
    "            y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])  # numerical labels for test1\n",
    "            y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])  # numerical labels for test2\n",
    "            y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])  # numerical labels for test3\n",
    "            testset_sparse_samples = [sparse_test1, sparse_test2, sparse_test3]           # list of testing examples\n",
    "            testset_IDs = [list(IDs_test1.values()), list(IDs_test2.values()), list(IDs_test3.values())]  # IDs in test1, test2, test3\n",
    "\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if classifier == 'NearestNeighbour':\n",
    "            print('--> ' + classifier.upper() + ': ', end='')\n",
    "            t0 = time.time()                # timing the run\n",
    "            clf = KNeighborsClassifier(n_neighbors=9, metric='cosine', weights='uniform')  # the best params selected by GridSearch\n",
    "            clf.fit(Xtrain, Ytrain_numeric)\n",
    "            t1 = time.time()                # timing the run\n",
    "            print(f\"training time: {t1-t0:.3f}\")\n",
    "\n",
    "\n",
    "        elif classifier == 'NaiveBayes':\n",
    "            print('--> ' + classifier.upper() + ': ', end='')\n",
    "            t0 = time.time()                # timing the run\n",
    "            clf = MultinomialNB(alpha = 0.4)# the best params selected by GridSearch\n",
    "            clf.fit(Xtrain, Ytrain_numeric)\n",
    "            t1 = time.time()                # timing the run\n",
    "            print(f\"training time: {t1-t0:.3f}\")\n",
    "        elif classifier == 'SVM':\n",
    "            # write the classifier 3 here\n",
    "            print('--> ' + classifier.upper(), end='')\n",
    "        elif classifier == 'LSTM':\n",
    "            # write the LSTM classifier here\n",
    "            print('--> ' + classifier.upper(), end='')\n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "\n",
    "        # Prediction performance of the classifiers\n",
    "        testsets = datasets[1:4]\n",
    "        for i in range(len(testsets)):\n",
    "            testset = testsets[i]\n",
    "            X, IDs = testset_sparse_samples[i], testset_IDs[i]\n",
    "            y_pred_numeric = clf.predict(X)\n",
    "            y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "            pred_dict = dict(zip(IDs, y_pred))\n",
    "            evaluate(pred_dict, testset, classifier=features + '-' + classifier)\n",
    "\n",
    "\n",
    "# # evaluation test2\n",
    "# ID_test2 = list(BOW_test2.keys())\n",
    "#\n",
    "# y_pred_numeric = clf.predict(sparse_test2)\n",
    "# y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "# pred_dict = dict(zip(ID_test2, y_pred))\n",
    "# s2 = evaluate(pred_dict, '../semeval-tweets/twitter-test2.txt', classifier=\"KNN\")  # best score 0.435\n",
    "#\n",
    "# # evaluation test3\n",
    "# ID_test3 = list(BOW_test3.keys())\n",
    "# y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])\n",
    "# y_pred_numeric = clf.predict(sparse_test3)\n",
    "# y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "# pred_dict = dict(zip(ID_test3, y_pred))\n",
    "# s3 = evaluate(pred_dict, '../semeval-tweets/twitter-test3.txt', classifier=\"KNN\")  # best score 0.435\n",
    "# t1 = time.time()  # timing the run\n",
    "# print('overall run time:', t1-t0)\n",
    "# print(\"average F1 score:\", (s1 + s2 + s3)/3)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
