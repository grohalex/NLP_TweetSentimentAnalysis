{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a *Jupyter notebook* for the coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, donâ€™t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n",
    "You may import more packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages here\n",
    "import re\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../semeval-tweets/twitter-test1.txt', '../semeval-tweets/twitter-test2.txt', '../semeval-tweets/twitter-test3.txt']\n"
     ]
    }
   ],
   "source": [
    "# Define test sets\n",
    "dataDir = '../semeval-tweets'\n",
    "testsetStrings = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']\n",
    "testsets = [join(dataDir, t) for t in testsetStrings]\n",
    "print(testsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght: 139\n",
      "102313285628711403\n",
      "neutral\n",
      "\"Bing one-ups knowledge graph, hires Encyclopaedia Britannica to supply results:   It may have retired from the cut-throat world of pr...\"\n",
      "\n",
      "Lenght: 139\n",
      "653274888624828198\n",
      "neutral\n",
      "\"On Thursday, concealed-carry gun license holders will be given a new right in the state of Oklahoma: the ability... http://t.co/oSgGHKi1\"\n",
      "\n",
      "Lenght: 137\n",
      "420747042670198316\n",
      "negative\n",
      "Miyagi just got banned from yoga. He was caught him sniffing the sphincter of the girl in front of him. There may be police involvement!\n",
      "\n",
      "Lenght: 135\n",
      "822064800445716046\n",
      "neutral\n",
      "Join us tonight at Boston Pizza - Centre on Barton for THURSDAY NIGHT FOOTBALL! Tonight the Chiefs take on the... http://t.co/iegTxPQv\n",
      "\n",
      "Lenght: 139\n",
      "055480020953212084\n",
      "neutral\n",
      "\"#FX NEW YORK, Oct 18 (Reuters) - The Federal Reserve provided $4.701 billion of liquidity to the ... http://t.co/BJhIQTtO #EUR #AUD #CAD\"\n",
      "\n",
      "Lenght: 132\n",
      "429443270273347255\n",
      "neutral\n",
      "\"13 April 1996, History is made, as the MetroStars and the Los Angeles Galaxy meet at the Rose Bowl in Pasadena, California, (1/3)\"\n",
      "\n",
      "Lenght: 135\n",
      "220323262844863802\n",
      "neutral\n",
      "Make sure you tune into The Saga exclusively on Temple's only student run radio WHIP at iHeartRadio Monday through Thursday's at 9pm!!\n",
      "\n",
      "Lenght: 135\n",
      "266953303729385574\n",
      "positive\n",
      "\"Saturday Nov 17th, 2012 Q Bar and Grill is the Place to be!  UFC 90's Event with Special Guests:  DJ BOBBY D... http://t.co/z96soXQf\"\n",
      "\n",
      "Lenght: 134\n",
      "267372990522222442\n",
      "negative\n",
      "@Waka_BacaFlame I'm already on my 3rd year at Colton it wouldn't make sense to graduate from gt after spending all this time here >.<\n",
      "\n",
      "Lenght: 131\n",
      "578446202147364926\n",
      "neutral\n",
      "\"Ephesians 6:11 (NEB) - Ephesians 6:11 (NEB) Put on all the armour which God provides, so that you may be... http://t.co/cEt5WKVK\"\n",
      "\n",
      "Lenght: 141\n",
      "810763059461202071\n",
      "neutral\n",
      "Andando in bicicletta a Roma -- si puo' dividere (share) pure li! Shareable: Changing Bike Culture in the Eternal City: http://t.co/nT3c7ryI\n",
      "\n",
      "Lenght: 132\n",
      "885726184364930528\n",
      "positive\n",
      ".@Q1047 Would luv if you could make an announcement about Chris Rene concert @theroxy this Thursday!! Thanks!! http://t.co/aKzopIrd\n",
      "\n",
      "Lenght: 141\n",
      "960689731242555262\n",
      "negative\n",
      "So now it seems Daav(Conspiracy) of Yadav have back fired...and if true/proved may create troubles for Akhilesh Yadav and Samajwadi Party...\n",
      "\n",
      "Lenght: 139\n",
      "491718215382409200\n",
      "neutral\n",
      "\"Charlie Rose with Desmond Tutu; Bill Joy (October 5, 1999): The Archbishop of Cape Town, South Africa and Nobel ... http://t.co/BVoS0dnd\"\n",
      "\n",
      "Lenght: 143\n",
      "317970942816831516\n",
      "neutral\n",
      "\"Remember folks. Grabbed 3 more earnings plays today. CELG,CME and HSY all calls for tomorrow!!! 5/5 for the week   $CME http://t.co/ZcmjGsha\"\n",
      "\n",
      "Lenght: 137\n",
      "281466922061596703\n",
      "neutral\n",
      "\"Deadly Israeli strike, fire from Gaza mar truce: A deadly Israeli airstrike into the Gaza Strip and rocket and... http://t.co/E0h27Mcg\"\n",
      "\n",
      "Lenght: 137\n",
      "076653802834383376\n",
      "positive\n",
      "PST Keane delivers best MLS goal from Saturday: The Los Angeles Galaxy were on top of things early tonight again... http://t.co/aVYSaVOo\n",
      "\n",
      "Lenght: 142\n",
      "629377153435614908\n",
      "negative\n",
      "\"Heather is like the serpent in the garden trying 2 entice info from McBain! Michael Easton, dont ever go away that long! Watching Tues #GH.\"\n",
      "\n",
      "Lenght: 142\n",
      "255090901012423499\n",
      "neutral\n",
      "\"Southampton's Richard Bland is -9 after his 2nd round in the Madeira Islands Open on the European Tour. Currently tied 4th, 3 off the lead.\"\n",
      "\n",
      "Lenght: 135\n",
      "581663993931072193\n",
      "neutral\n",
      "\"Overexposed Tour, Maroon 5 With The Cab: Through The Eyes Of A Drummer - As you may have heard, The Cab just... http://t.co/H3c7kQsw\"\n",
      "\n",
      "Lenght: 136\n",
      "854756032273426749\n",
      "neutral\n",
      "Remember to join the Springboks at 3pm tomorrow at Orlando Stadium to watch their open training session as they... http://t.co/vaCN6EZ0\n",
      "\n",
      "Lenght: 137\n",
      "385766324957619690\n",
      "positive\n",
      "@MichelleLMyers #BreakingDawnPart2 The Saga may end but the love and friendships will last 4ever #LoveIsLouderthanRollingTheFinalCredits\n",
      "\n",
      "Lenght: 134\n",
      "844654517192428026\n",
      "positive\n",
      "\"Happy 102 years to L.A. artist Tyrus Wong! May the road rise to meet you, may the wind be always at your back: http://t.co/e8SB5S90\"\n",
      "\n",
      "Lenght: 137\n",
      "233928065818229615\n",
      "neutral\n",
      "Federal Reserve stands firm on bond-buying program: The Federal Reserve said on Wednesday that it is s... http://t.co/KnRMVgLu #business\n",
      "\n",
      "Lenght: 139\n",
      "226170643240610310\n",
      "positive\n",
      "\"Bersani, supports stability agreement to secure government: (AGI) Rome, October 24 - Tomorrow the agreement on t... http://t.co/ew0JWQgD\"\n",
      "\n",
      "Lenght: 141\n",
      "282416648112742867\n",
      "neutral\n",
      "\"About to watch Kony 2012 video. Despite 100m views, there are quite a number here watching for 1st time. Interested in the reactions.#yuga\"\n",
      "\n",
      "Lenght: 139\n",
      "790411654414429445\n",
      "neutral\n",
      "\"LG may update the Optimus 2X to ICS after all, releases source code: The saga of the elusive ICS update for the ... http://t.co/g0hT0NtX\"\n",
      "\n",
      "Lenght: 136\n",
      "712768996713302192\n",
      "positive\n",
      "Saturday night Adult Swim (Cowboy Bebop + Ghost in the Shell) reminds me how much I adore Yoko Kanno's music. Such an amazing composer.\n",
      "\n",
      "Lenght: 136\n",
      "170040141580222961\n",
      "neutral\n",
      "Just one of the songs to expect during THE FRAY's concert in Smart Araneta Coliseum on November 10. This is off... http://t.co/BUVMOlcM\n",
      "\n",
      "Lenght: 139\n",
      "836541798391946839\n",
      "neutral\n",
      "\"Seven Penny Stocks on the Move with Heavy Volume, April 24: CSOC shares have traded as high as $.18 over the pas... http://t.co/OX7oCx72\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the structure of the dataset\n",
    "with open(testsets[0], 'r', encoding='utf8') as f1:\n",
    "    i = 0\n",
    "    for line in f1:\n",
    "        fields = line.split('\\t')\n",
    "        if i < 30:\n",
    "            length = len(fields[2])\n",
    "            if length > 130:\n",
    "                print(f\"Lenght: {length}\")\n",
    "                print(fields[0])  # 1st column - tweet ID\n",
    "                print(fields[1])  # 2nd column - tweet sentiment\n",
    "                print(fields[2])  # 3rd column - tweet text\n",
    "                i += 1\n",
    "\n",
    "# preprocessing questions and notes: \n",
    "  # -> what about removing the @usernames, is it advisable?\n",
    "  # -> need to remove URLs!\n",
    "  # -> There is a lot of noise/mistakes in the data and absence of interpunction.\n",
    "  # -> what about adding of the starting token?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    reading the testset and return a dictionary with: ID -> sentiment\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}  # init the dictionary\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)\n",
    "    return semevalmacrof1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            positive  negative  neutral\n",
      "positive    1.000     0.000     0.000     \n",
      "negative    0.000     1.000     0.000     \n",
      "neutral     0.000     0.000     1.000     \n",
      "\n",
      "../semeval-tweets/twitter-test1.txt (PerfectClassifier): 1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the evaluation functions\n",
    "tweetDict = read_test('../semeval-tweets/twitter-test1.txt')\n",
    "confusion(tweetDict, '../semeval-tweets/twitter-test1.txt', \"PerfectClassifier\")\n",
    "evaluate(tweetDict, '../semeval-tweets/twitter-test1.txt', \"PerfectClassifier\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, dev set and testing set\n",
    "\n",
    "dataDir = '../semeval-tweets'  # change to the proper directory\n",
    "datasetStrings = ['twitter-training-data.txt', 'twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt', 'twitter-dev-data.txt']\n",
    "datasets = [join(dataDir, t) for t in datasetStrings]\n",
    "\n",
    "tweet_IDs = {}          # init dictionary with tweet IDs\n",
    "tweet_sentiments = {}   # init dictionary with sentiments\n",
    "tweet_texts = {}        # init dictionary with tweet texts\n",
    "\n",
    "for DatasetString in datasets:\n",
    "    data_ID, data_sent, data_text  = {}, {}, {}    # temp dictionaries\n",
    "    with open(DatasetString, 'r', encoding='utf8') as f1:\n",
    "        for i, line in enumerate(f1):\n",
    "            fields = line.split('\\t')\n",
    "            data_ID[i] = fields[0]            # tweet IDs\n",
    "            data_sent[fields[0]] = fields[1]  # sentiments\n",
    "            data_text[fields[0]] = fields[2]  # tweet text\n",
    "    tweet_IDs[DatasetString] = data_ID\n",
    "    tweet_sentiments[DatasetString] = data_sent\n",
    "    tweet_texts[DatasetString] = data_text\n",
    "\n",
    "# sentiment dictionaries\n",
    "sent_train = tweet_sentiments[datasets[0]]\n",
    "sent_test1 = tweet_sentiments[datasets[1]]\n",
    "sent_test2 = tweet_sentiments[datasets[2]]\n",
    "sent_test3 = tweet_sentiments[datasets[3]]\n",
    "sent_dev = tweet_sentiments[datasets[4]]\n",
    "\n",
    "# tweet text dictionaries\n",
    "text_train = tweet_texts[datasets[0]]\n",
    "text_test1 = tweet_texts[datasets[1]]\n",
    "text_test2 = tweet_texts[datasets[2]]\n",
    "text_test3 = tweet_texts[datasets[3]]\n",
    "text_dev = tweet_texts[datasets[4]]\n",
    "\n",
    "# tweet IDs dictionaries\n",
    "IDs_train = tweet_IDs[datasets[0]]\n",
    "IDs_test1 = tweet_IDs[datasets[1]]\n",
    "IDs_test2 = tweet_IDs[datasets[2]]\n",
    "IDs_test3 = tweet_IDs[datasets[3]]\n",
    "IDs_dev = tweet_IDs[datasets[4]]\n",
    "\n",
    "\n",
    "## examples and tests\n",
    "# id = IDs_train[0]\n",
    "# id_dev = IDs_dev[0]\n",
    "# id1 = IDs_test1[0]\n",
    "# id2 = IDs_test2[0]\n",
    "# id3 = IDs_test3[0]\n",
    "# print(f\"-ID:{id} \\n-TEXT:{text_train[id]}-SENTIMENT: {sent_train[id]}\\n\")\n",
    "# print(f\"-ID:{id_dev} \\n-TEXT:{text_dev[id_dev]}-SENTIMENT: {sent_dev[id_dev]}\\n\")\n",
    "# print(f\"-ID:{id1} \\n-TEXT:{text_test1[id1]}-SENTIMENT: {sent_test1[id1]}\\n\")\n",
    "# print(f\"-ID:{id2} \\n-TEXT:{text_test2[id2]}-SENTIMENT: {sent_test2[id2]}\\n\")\n",
    "# print(f\"-ID:{id3} \\n-TEXT:{text_test3[id3]}-SENTIMENT: {sent_test3[id3]}\\n\")\n",
    "# print(len(IDs_train.keys()), len(text_train.keys()), len(sent_train.keys()))  # 45101\n",
    "# print(len(IDs_test1.keys()), len(text_test1.keys()), len(sent_test1.keys()))  # 3531\n",
    "# print(len(IDs_test2.keys()), len(text_test2.keys()), len(sent_test2.keys()))  # 1853\n",
    "# print(len(IDs_test3.keys()), len(text_test3.keys()), len(sent_test3.keys()))  # 2379\n",
    "# print(len(IDs_dev.keys()), len(text_dev.keys()), len(sent_dev.keys()))        # 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "#### Order of preprocessing\n",
    "* lowercase text\n",
    "* regex cleaning\n",
    "   * Remove URLs\n",
    "   * Remove non-alphanumeric characters (leave hashtags and usernames)\n",
    "   * Remove numbers that are fully made of digits\n",
    "   * (Remove words with only 1 character)\n",
    "\n",
    " #### Preprocessing questions and notes:\n",
    "   -> what about removing the @usernames, is it advisable?\n",
    "   -> need to remove URLs!\n",
    "   -> There is a lot of noise/mistakes in the data and absence of interpunction.\n",
    "   -> what about adding of the starting token?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "if os.path.isfile(\"preprocessing.pkl\"):  # loading preprocessed datasets\n",
    "    with open('preprocessing-plain.pkl', 'rb') as inp_file:\n",
    "        temp_dicts = pickle.load(inp_file)\n",
    "        txt_dicts = temp_dicts[0:5]\n",
    "        txtlist_dicts = temp_dicts[5:]\n",
    "\n",
    "else:\n",
    "    top100 = ['com', 'net', 'org', 'jp', 'de', 'uk', 'fr', 'br', 'it', 'ru', 'es', 'me', 'gov', 'pl', 'ca', 'au', 'cn', 'co', 'in', 'nl', 'edu', 'info', 'eu', 'ch', 'id', 'at', 'kr', 'cz', 'mx', 'be', 'tv', 'se', 'tr', 'tw', 'al', 'ua', 'ir', 'vn', 'cl', 'sk', 'ly', 'cc', 'to', 'no', 'fi', 'us', 'pt', 'dk', 'ar', 'hu', 'tk', 'gr', 'il', 'news', 'ro', 'my', 'biz', 'ie', 'za', 'nz', 'sg', 'ee', 'th', 'io', 'xyz', 'pe', 'bg', 'hk', 'rs', 'lt', 'link', 'ph', 'club', 'si', 'site', 'mobi', 'by', 'cat', 'wiki', 'la', 'ga', 'xxx', 'cf', 'hr', 'ng', 'jobs', 'online', 'kz', 'ug', 'gq', 'ae', 'is', 'lv', 'pro', 'fm', 'tips', 'ms', 'sa', 'app', 'lat']\n",
    "\n",
    "    emoticons = ['%)', ':&', '8-)', '=/', ':c', ':#', ':)))', ';)', 'd:', '=3', ':O', '8D', 'oO', ':o)', '*)', 'QQ', ':S', '=)', 'D8', ':]', 'O:)', 'XD', 'Q_Q', \":'(\", ':$', ':3', ':L', 'XP', ':-(', ':(', ':-)', ':-))', 'o.O', ':*', '0:3', ';;', ':D', ';D', '=]', ':@', ':)', ':))', ':/', '>:)', ':P', ':-)))', ';]', '^_^', \":')\", ':x', 'D:', ':^)', ':|', ';_;', '=p', ':b', '=D', ':o', 'DX']\n",
    "    emoticon_strings = ['#emoticon' + str(num) for num in range(len(emoticons))]\n",
    "    emoticon2string = dict(zip(emoticons, emoticon_strings))\n",
    "    string2emoticon = dict(zip(emoticon_strings, emoticons))\n",
    "\n",
    "    ID_dicts = [IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev]\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev]\n",
    "    txtlist_dicts = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  # init the lemmatizer\n",
    "    POSconvert = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "\n",
    "    for i, IDdict in enumerate(ID_dicts):\n",
    "        output = txt_dicts[i]\n",
    "        output_txt = {}\n",
    "        for id in IDdict.values():\n",
    "            text = output[id].lower()\n",
    "\n",
    "            # replace/delete all URLs starting with 'http' and 'www'\n",
    "            new_text = re.sub(\"http[^\\s]*\", '', text)\n",
    "            new_text = re.sub(\"www[^\\s]*\", '', new_text)\n",
    "\n",
    "            # delete all URLs which have one of 100 most common extensions ('.com', '.net', ...)\n",
    "            for ext in top100:\n",
    "                re_string = \"[^\\s]*\\.\" + ext + \"[^\\s]*\"\n",
    "                new_text = re.sub(re_string, '', new_text)\n",
    "\n",
    "            # replace all emoticons with an emoticon string:  #emoticon42\n",
    "            # for em in emoticons:\n",
    "            #     re_string = '\\s' + re.escape(em) + '\\s'\n",
    "            #     replace_string = ' ' + emoticon2string[em] + ' '\n",
    "            #     new_text = re.sub(re_string, replace_string, new_text)\n",
    "\n",
    "            # removing '&amp'\n",
    "            new_text = re.sub('&amp','', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars except for '# and @'\n",
    "            new_text = re.sub('[^\\w\\s@#]','', new_text)\n",
    "\n",
    "            #replace all @usernames with 'username'\n",
    "            new_text = re.sub('\\s@[^\\s]+',' username', new_text)  # middle\n",
    "            new_text = re.sub('^@[^\\s]+','username', new_text)    # start\n",
    "\n",
    "            # remove strings with '#' not on the beginning (to keep only hashtags)\n",
    "            new_text = re.sub('\\s[\\w]+#[\\w]*','', new_text)\n",
    "\n",
    "            # numbers fully made of digits\n",
    "            new_text = re.sub('\\s[\\d]+\\s','', new_text)\n",
    "\n",
    "            # remove words with only 1 character\n",
    "            new_text = re.sub('\\\\b\\\\w{1}\\\\b','', new_text)\n",
    "\n",
    "            # remove newline chars\n",
    "            new_text = new_text.replace('\\n', ' ')\n",
    "\n",
    "            # replace a multiple spaces with a single space\n",
    "            new_text = re.sub('\\s+',' ', new_text)\n",
    "\n",
    "            # using the lemmatizer\n",
    "            txt_list = nltk.word_tokenize(new_text)\n",
    "            for k, word in enumerate(txt_list):  # fixing the separation of hashtags by the tokenizer\n",
    "                if word == '#' or word == '@':\n",
    "                    if k < len(txt_list) - 1:\n",
    "                        txt_list[k] = txt_list[k] + txt_list[k+1]\n",
    "                        txt_list.pop(k+1)\n",
    "            POS = nltk.pos_tag(txt_list)                  # POS tags from nltk\n",
    "            WordNetPOS = [POSconvert(P[1]) for P in POS]  # POS tags for lemmatizer\n",
    "            for j in range(len(txt_list)):\n",
    "                word = txt_list[j]\n",
    "                lemmatized = lemmatizer.lemmatize(word, WordNetPOS[j])  # process each token/word one by one\n",
    "                txt_list[j] = lemmatized  # update the word in the txt_list\n",
    "\n",
    "            ## UPDATE the dictionary\n",
    "            output_txt[id] = ' '.join(txt_list)\n",
    "            output[id] = txt_list\n",
    "\n",
    "        txt_dicts[i] = output_txt\n",
    "        txtlist_dicts.append(output)\n",
    "\n",
    "text_train = txt_dicts[0]\n",
    "text_test1 = txt_dicts[1]\n",
    "text_test2 = txt_dicts[2]\n",
    "text_test3 = txt_dicts[3]\n",
    "text_dev = txt_dicts[4]\n",
    "txtlist_train = txtlist_dicts[0]\n",
    "txtlist_test1 = txtlist_dicts[1]\n",
    "txtlist_test2 = txtlist_dicts[2]\n",
    "txtlist_test3 = txtlist_dicts[3]\n",
    "txtlist_dev = txtlist_dicts[4]\n",
    "\n",
    "# saving preprocessing.pkl\n",
    "if not os.path.isfile(\"preprocessing.pkl\"):\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev, txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev]\n",
    "    with open('preprocessing.pkl', 'wb') as out_file:\n",
    "        pickle.dump(txt_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felt privilege to play foo fighter song on guitar today with one of the plectrum from the gig on saturday\n",
      "@aaqibafzaal pakistan may be an islamic country but der be lot true muslim in india who love their country and can sacrifice all for it\n",
      "happy birthday to the coolest golfer in bali @tjvictoriacnd may you become cooler and cooler everyday stay humble little sister xx\n",
      "@simpplya tmills be go to tucson but the 29th and it on thursday\n",
      "hmmmmm where be the #blacklivesmatter when matter like this rise kid be disgrace\n",
      "@hypable all good im excite about 3rd season find home on netflix just want to make sure the reader have the news a it develop\n",
      "told my mom want to stay in hotel for my 18th with people but my birthday on valentine lucky me\n",
      "1st thing do after baggage claim be get up to date with @ronnaandbeverly bad blood ronna bev style make be home bit more okay\n",
      "bobby jindal want you to assimilate to heritage of the old confederacy even though his parent be from january dot dot dot\n",
      "@coolcat1304 watch itthe 1st time votedit in the nta award best factual program mite needwatch rest on itvplayer\n",
      "@lilbeast03 im sad that naruto manga be over and ill probably cry when the anime end kishimotosan be come to the u in october\n",
      "feel down this monday check out this guy who dress a baby prince george for week it might cheer you up\n",
      "huge play by #georgia to get out of the shadow of their own goal line on 2nd down lambert to sony for ayard gain very accurate pas\n",
      "kasich seek gop presidential nomination john kasich announce his run for the white house intuesday\n",
      "why do this horse have the same look of fear in it eye when next to marine le pen a romney do when next to tru\n",
      "ben carson be consider for the same cabinet position jack bauer once have mister carson youre no jack bauer\n",
      "john kasichs tone of surrender may not have faze the biased buckeye audience but it certainly bother the gop base @tperkins\n",
      "hommage to gary carter and the expo day tomorrow at rcup dollar hot dog andtix at thelevel #youppiwillbehere\n",
      "@pocketvolcano take it you hear ric flair be at dreamwave in jan\n",
      "@renuudesai yes mambetter google it you know some people dont have any work may create it a big scenethiz happen for many celebtc\n",
      "colour #3dprinting be en vogue for luxury eyewear maker @safilo1934\n",
      "@immortaltech dublin this saturday get ta get on the guinness\n",
      "@transferdicky antiimmigration sentiment be merely piece in bigger puzzle\n",
      "really like holly holm but be she ready for rousey thought\n",
      "which mean if andrade win she could get rousey next\n",
      "why do the medium keep give this brain dead moron air time former alaska gov sarah palin say sunday she\n",
      "@daniboothang @indeliblemarq__ myyr old cousin didnt know ice cube be rapperjust an superstar actor from the friday movie lol\n",
      "club remix next saturday night allstar will be in the building @geezyallstar @crazyronallstar\n",
      "@talk2cleo @kerrymacuska thats right lady im blame everything on kerry cleo this show may well eclipse the number of @jakeneedham\n",
      "cliff avril leave detroit lion game with back injury detroit lion defensive end cliff avril leave sunday ga\n",
      "make me sick to think we the american taxpayer will be pay for secret service detail for trump melania for\n",
      "@jimyeoman @pedallingveg bollock then theres ian brady myra hindley some bad hombre from that city see what do there\n",
      "my topare probably granny chiyo sakura v sasori guy v kissame the 2nd time and naruto v pain\n",
      "the guy at dunkin just say see you tomorrow because he know that im there every day\n",
      "watch the lookalikes teaser on sunday brunch be the only one who think the david beckham lookalike be definitely not lookalike\n",
      "dont think chelsea should appeal for torres red card let sturridge play tomorrow it the carling cup not pl\n",
      "@jonathamingo in other related news you know theres no park tomorrow\n",
      "an older article cite the legal issue behind the dakota access pipeline #dapl\n",
      "today life lesson courtesy of alabama if youre gon na play in texas you get ta have fiddle in the band\n",
      "we a people must do whatever it take to fight obamas lawlessness communist bent on destroy our heritage now not tomorrow\n",
      "you have to watch michael moore in trumplandi catch it on sho2\n",
      "my middle daughter just tell me 5th grade presentation in her class be on seth rollins #school #wwe #education\n",
      "mack sure you check out @carolinakidz1 and @trilla_guapo this saturday @club bodi\n",
      "uk release of star war episode vii the force awaken to be day earlier than expect dec\n",
      "nialls go to beindays no one gon na beill lock him in forever211 prepares squad cmon\n",
      "oracle set the date for it first quarter fiscal yearearnings announcement redwood shore ca mar\n",
      "@jayjbooth such close call cheer pal do well first month at ibm do miss the friday call though haha hope youre well\n",
      "@mdavisbot @justanactor sweet jesus #thewalkingdead\n",
      "@petestavros @megynkelly well good for you\n",
      "phone to consider if you really want the galaxy note via @yahoo\n",
      "report in nigeria say a many aspeople may have be kill after raid on village in the northeast by suspected boko haram\n",
      "@planetmoney be you sure of the bet which one can make in vega eg gatorade katy perry think those may only be available offshore\n",
      "the sunday daily the rock hulk hogan pay the price for racist rant dwayne the rock johnson pix #prkdr\n",
      "russia denies deal with iran over military cooperation in meeting on wednesday break news buzz\n",
      "at least we wont face de bruyne for wolfsburg in the champion league or against city when they get barca in the 2nd round\n",
      "oxygen vh1 get the most rating on monday night lol we aint play no game cant wait till tonight\n",
      "like how the bbc always load against brexit vd on now\n",
      "@juliomc69 it just get me hot people trash may for be cocky but not rousey\n",
      "after many month of wait and cancel wembley date along the way finally get to see foo fighter tomorrow excite time\n",
      "@illessa base on the 1st ep mr robot seem good series to go on look to respect the audience in the same way a hannibal least\n",
      "@kanakmanidixit you be free to join yakub tomorrow if you be feel guilty dont mind be use freedom of speech\n",
      "remember this one weekend have see fat nick pouya on friday disneyland on saturday and the beach my friend on sunday lit\n",
      "royal birthday @work happy 2nd prince george from all of u across the pond @constitution park pool\n",
      "cant do this 3rd day of eid and im back to work\n",
      "kris bryant line out to lf to end the top of the 1st strand runner on 1st 2nd with no score between the #cubs brewer\n",
      "the only thing scarier than kanye west run for president be that there slight chance kim kardashian may be the first lady\n",
      "blue jaysgames out with david price on the mound tomorrow\n",
      "the exchange club of winona and the eagle club will be host tater for tot to raise money for child abuse prevention on october 31st\n",
      "want to be batman on halloween but im go to be busy on that day even when it saturday tttt\n",
      "frank ocean can go to hell now idgaf about him no more when baby drop her album\n",
      "if didnt have train tomorrow id be sick and at the jason aldean concert #maybenexttime\n",
      "@yahoonews billionaire woman who make fortune in private business which be normalway not in goverment post a per repubicans dems\n",
      "happy 2nd birthday prince george kensington palace celebrate the young royal 2nd birthday with another\n",
      "what be the fafsa and pell grant begin college what be the fafsa and pell grant october2012 cate\n",
      "be on guard #blacklivesmatter be attack police hillary podesta soros all behind this they lose #blm need to be arrest #pizzagate\n",
      "brock lesnars return to msg to air live on the wwe network it be announce on sunday night during summersla\n",
      "galaxy notebanned in u flight violator will be send to prison\n",
      "direct donate link for that senate seat dems can still win\n",
      "snapburning the flag should be #maga #boycottcnn #boycottmsnbc #boycottnfl\n",
      "nicki do not have the time to be shady she will deadass drop the tea on ya lap and let you get 3rd degree burn\n",
      "march 3rddo you think kevin rudd be superficial yesvotes novotes rudd be reveal a failure #auspol\n",
      "cristiano ronaldo only have four friend at #realmadrid\n",
      "transient night with stranger can be hold forever in music leonard cohen on the inspiration behind song\n",
      "with shawn death anniversary on the 21st markingyrs without him well my heart and crystal feel heavy know if he be still here\n",
      "so notice on syfy website you can vote if april life or dy in sharknadoessentially they announce shaknadowhat great day\n",
      "@clarkhoward to add to your budget smartphone list may introduce to you the new moto 3rd gen which be unlocked and start at\n",
      "@mabasalamah assalaam it be my 1st eid away from family hope you have nice eid and all family be good happy eid to you too thanks\n",
      "this may be hard to believe be watch seinfelds the contest for the very first time\n",
      "chad sentence boko haram member to death for june attack recent month however have see the group stage\n",
      "last day of work at walgreens tomorrow cry tear of joy\n",
      "@the_overtones look like you guy have fun in sunny scotlandlooks a if therell be some hungovertones today #hadto\n",
      "#news today inaugust 24th nirvana play at the today inaugust 24th nirvana #keepgrunge\n",
      "go to see jason aldean tomorrow with the best friend kinda very excite\n",
      "ghetto at the center of the world chungking mansion hong kong\n",
      "all want for my birthday tomorrow be to go see jurassic world again thats literally the only thing want\n",
      "you cant shit talk kpop every group be so in sync and vocally on point it may not be your thing but it talent\n",
      "just meet the player for the 1st time this roger johnson fella walk around like he own the place so debagged him flick his helmet\n",
      "with the 1st pick in the classic movie lotto mark bonnell selects north by northwest #classicmovielotto\n",
      "janet street porter shock viewer with her comment about prince george well do jsp bang on right\n",
      "be announce an award the same a present it bc be announc itbut what about the actual golden globe show in january\n"
     ]
    }
   ],
   "source": [
    "# checking the preprocessed output\n",
    "for id in list(IDs_train.values())[0:100]:\n",
    "    print(text_train[id])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: Bag of words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-41-fb2ce43bb459>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0mBOW_test2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mID\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtweet\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtxtlist_test2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m     \u001B[0mBOW\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtext2BOW\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtweet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvocabulary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstopwords\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstopwords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     55\u001B[0m     \u001B[0mBOW_test2\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mID\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBOW\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-41-fb2ce43bb459>\u001B[0m in \u001B[0;36mtext2BOW\u001B[0;34m(text_list, vocabulary, stopwords)\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtext_list\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mstopwords\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m             \u001B[0;32mif\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     30\u001B[0m                 \u001B[0mBOW_vec\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mvocab2num\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Bag of Words - my implementation:\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords as Stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# 1) removing stop words\n",
    "stopwords = Stopwords.words('english')\n",
    "stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "# 2) extracting the dictionary/vocabulary\n",
    "freq = FreqDist()   # frequency distribution\n",
    "for Dict in txtlist_dicts:\n",
    "    for tweet in Dict.values():\n",
    "        for word in tweet:\n",
    "            if not word in stopwords:\n",
    "                freq[word] += 1\n",
    "\n",
    "nums = range(len(freq.keys()))\n",
    "vocabulary = list(freq.keys())              # creating the dictionary\n",
    "vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out of vocabulary' word\n",
    "vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words -> len: 69742\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its BoW representation as np.array\n",
    "def text2BOW(text_list, vocabulary, stopwords):\n",
    "    BOW_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in text_list:\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                BOW_vec[vocab2num[word]] += 1\n",
    "            else:\n",
    "                BOW_vec[vocab2num['<OOV>']] += 1\n",
    "    return BOW_vec\n",
    "\n",
    "# if os.path.isfile(\"BOWs.pkl\"):  # loading preprocessed datasets\n",
    "#     with open('BOWs.pkl', 'rb') as inp_file:\n",
    "#         ll = pickle.load(inp_file)\n",
    "#         BOW_train, BOW_test1, BOW_test2, BOW_test3, BOW_dev = ll[0], ll[1], ll[2], ll[3], ll[4]\n",
    "#\n",
    "# else:\n",
    "# Bag of Words (BOW) for each tweet\n",
    "BOW_train = {}\n",
    "for ID, tweet in txtlist_train.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_train[ID] = BOW\n",
    "\n",
    "BOW_test1 = {}\n",
    "for ID, tweet in txtlist_test1.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_test1[ID] = BOW\n",
    "\n",
    "BOW_test2 = {}\n",
    "for ID, tweet in txtlist_test2.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_test2[ID] = BOW\n",
    "\n",
    "BOW_test3 = {}\n",
    "for ID, tweet in txtlist_test3.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_test3[ID] = BOW\n",
    "\n",
    "BOW_dev = {}\n",
    "for ID, tweet in txtlist_dev.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_dev[ID] = BOW\n",
    "\n",
    "\n",
    "# saving BOWs.pkl: very large file - maybe not the best idea to save it?\n",
    "    # if not os.path.isfile(\"BOWs.pkl\"):\n",
    "    #     BOW_dicts = [BOW_train, BOW_test1, BOW_test2, BOW_test3, BOW_dev]\n",
    "    #     with open(\"BOWs.pkl\", 'wb') as out_file:\n",
    "    #         pickle.dump(BOW_dicts, out_file, protocol=-1)\n",
    "\n",
    "print(\"Starting sparse processing.\")\n",
    "\n",
    "# sparse representation\n",
    "vector_list = [BOW_train[id] for id in BOW_train.keys()]\n",
    "dense_train = np.vstack(vector_list)    # shape (45101, 59559)\n",
    "sparse_train = csr_matrix(dense_train)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [BOW_test1[id] for id in BOW_test1.keys()]\n",
    "dense_test1 = np.vstack(vector_list)    # shape (3531, 59559)\n",
    "sparse_test1 = csr_matrix(dense_test1)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [BOW_test2[id] for id in BOW_test2.keys()]\n",
    "dense_test2 = np.vstack(vector_list)    # shape (1853, 59559)\n",
    "sparse_test2 = csr_matrix(dense_test2)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [BOW_test3[id] for id in BOW_test3.keys()]\n",
    "dense_test3 = np.vstack(vector_list)    # shape (2379, 59559)\n",
    "sparse_test3 = csr_matrix(dense_test3)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [BOW_dev[id] for id in BOW_dev.keys()]\n",
    "dense_dev = np.vstack(vector_list)      # shape (2000, 59559)\n",
    "sparse_dev = csr_matrix(dense_dev)      # getting the sparse matrix\n",
    "\n",
    "# train + dev together (combined)\n",
    "vector_list1 = [BOW_train[id] for id in BOW_train.keys()]\n",
    "vector_list2 = [BOW_dev[id] for id in BOW_dev.keys()]\n",
    "temp1 = np.vstack(vector_list1)\n",
    "temp2 = np.vstack(vector_list2)\n",
    "dense_train_dev = np.vstack((temp1, temp2))  # shape (45101, 59559)\n",
    "sparse_train_dev = csr_matrix(dense_train_dev)  # getting the sparse matrix\n",
    "\n",
    "# save the sparse representation\n",
    "sparse_dicts = [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev]\n",
    "with open(\"BOWsparse.pkl\", 'wb') as out_file:\n",
    "    pickle.dump(sparse_dicts, out_file, protocol=-1)\n",
    "\n",
    "#if we want to load sparse representation\n",
    "with open(\"BOWsparse-noEmo.pkl\", 'wb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev] = temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "with open(\"BOWsparse-plain.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev] = temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-42-02e57f66a314>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mBOW_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: TF-IDF weighted Bag of words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sparse processing.\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords as Stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# stop words\n",
    "stopwords = Stopwords.words('english')\n",
    "stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "# extracting the dictionary\n",
    "freq = FreqDist()   # frequency distribution\n",
    "for Dict in txtlist_dicts:\n",
    "    for tweet in Dict.values():\n",
    "        for word in tweet:\n",
    "            if not word in stopwords:\n",
    "                freq[word] += 1\n",
    "\n",
    "nums = range(len(freq.keys()))\n",
    "vocabulary = list(freq.keys())              # creating the dictionary\n",
    "vocabulary_array = np.array(vocabulary)     # np.array of the dictionary\n",
    "vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out of vocabulary' word\n",
    "vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words -> len: 69742\n",
    "\n",
    "# extracting the dictionary\n",
    "DFfreq = FreqDist()   # document frequency distribution\n",
    "Ntexts = len(IDs_train) + len(IDs_test1) + len(IDs_test2) + len(IDs_test3) + len(IDs_dev)\n",
    "for Dict in txtlist_dicts:\n",
    "    for tweet in Dict.values():\n",
    "        for word in np.unique(tweet):\n",
    "            if not word in stopwords:\n",
    "                DFfreq[word] += 1\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its TFIDF representation as np.array\n",
    "def text2TFIDF(text_list, vocabulary, stopwords, Ntexts):\n",
    "    TFIDF_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in np.unique(text_list):\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / DFfreq[word])\n",
    "                TFIDF_vec[vocab2num[word]] = tf * idf\n",
    "            else:\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / 0.000001 )\n",
    "                TFIDF_vec[vocab2num['<OOV>']] = tf * idf\n",
    "    return TFIDF_vec\n",
    "\n",
    "\n",
    "# TFIDF-weighted Bag of Words for each tweet\n",
    "TFIDF_train = {}\n",
    "for ID, tweet in txtlist_train.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_train[ID] = tfidf\n",
    "\n",
    "TFIDF_test1 = {}\n",
    "for ID, tweet in txtlist_test1.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_test1[ID] = tfidf\n",
    "\n",
    "TFIDF_test2 = {}\n",
    "for ID, tweet in txtlist_test2.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_test2[ID] = tfidf\n",
    "\n",
    "TFIDF_test3 = {}\n",
    "for ID, tweet in txtlist_test3.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_test3[ID] = tfidf\n",
    "\n",
    "TFIDF_dev = {}\n",
    "for ID, tweet in txtlist_dev.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_dev[ID] = tfidf\n",
    "\n",
    "print(\"Starting sparse processing.\")\n",
    "\n",
    "# sparse TFIDF representation\n",
    "vector_list = [TFIDF_train[id] for id in TFIDF_train.keys()]\n",
    "TFIDFdense_train = np.vstack(vector_list)\n",
    "TFIDFsparse_train = csr_matrix(TFIDFdense_train)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [TFIDF_test1[id] for id in TFIDF_test1.keys()]\n",
    "TFIDFdense_test1 = np.vstack(vector_list)\n",
    "TFIDFsparse_test1 = csr_matrix(TFIDFdense_test1)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [TFIDF_test2[id] for id in TFIDF_test2.keys()]\n",
    "TFIDFdense_test2 = np.vstack(vector_list)\n",
    "TFIDFsparse_test2 = csr_matrix(TFIDFdense_test2)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [TFIDF_test3[id] for id in TFIDF_test3.keys()]\n",
    "TFIDFdense_test3 = np.vstack(vector_list)\n",
    "TFIDFsparse_test3 = csr_matrix(TFIDFdense_test3)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [TFIDF_dev[id] for id in TFIDF_dev.keys()]\n",
    "TFIDFdense_dev = np.vstack(vector_list)\n",
    "TFIDFsparse_dev = csr_matrix(TFIDFdense_dev)  # getting the sparse matrix\n",
    "\n",
    "vector_list1 = [TFIDF_train[id] for id in TFIDF_train.keys()]\n",
    "vector_list2 = [TFIDF_dev[id] for id in TFIDF_dev.keys()]\n",
    "temp1 = np.vstack(vector_list1)\n",
    "temp2 = np.vstack(vector_list2)\n",
    "TFIDFdense_train_dev = np.vstack((temp1, temp2))  # shape (45101, 59559)\n",
    "TFIDFsparse_train_dev = csr_matrix(TFIDFdense_train_dev)  # getting the sparse matrix\n",
    "\n",
    "# save the sparse representation\n",
    "sparse_dicts = [TFIDFsparse_train, TFIDFsparse_test1, TFIDFsparse_test2, TFIDFsparse_test3, TFIDFsparse_dev, TFIDFsparse_train_dev]\n",
    "with open(\"TFIDFsparse-plain.pkl\", 'wb') as out_file:\n",
    "    pickle.dump(sparse_dicts, out_file, protocol=-1)\n",
    "\n",
    "## loading preprocessed TFIDF sparse data\n",
    "# with open(\"TFIDFsparse.pkl\", 'rb') as inp_file:\n",
    "#     temp = pickle.load(inp_file)\n",
    "#     [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev] = temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "[BOW_train, BOW_test1,BOW_test2,BOW_test3,BOW_dev] = [TFIDF_train, TFIDF_test1,TFIDF_test2,TFIDF_test3,TFIDF_dev]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: GloVe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading the word embeddings vectors from\n",
    "print('Extracting the word vectors.')\n",
    "\n",
    "embeddings_dict = {}\n",
    "glove_path = join('..','glove', 'glove.6B.100d.txt')\n",
    "with open(glove_path, 'r', encoding='utf-8') as File:\n",
    "    for line in File:\n",
    "        vec = line.split()\n",
    "        word = vec[0]\n",
    "        coefs = np.asarray(vec[1:], dtype='float32')\n",
    "        embeddings_dict[word] = coefs\n",
    "\n",
    "\n",
    "print(f\"Extracted {len(embeddings_dict)} word embedding vectors.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build an embedding matrix\n",
    "word_list = list(embeddings_dict.keys())\n",
    "nums = range(len(word_list))\n",
    "word2ID = dict(zip(word_list, nums))   # the index of the embedding vector\n",
    "vector_list = [embeddings_dict[word] for word in word_list]\n",
    "embedding_matrix = np.vstack(vector_list)\n",
    "print(f\"Created matrix with shape {embedding_matrix.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# embedings_dict and embedding_matrix work the same way:\n",
    "print(embeddings_dict['yes'][0:7])\n",
    "print(embedding_matrix[word2ID['yes'], 0:7])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nearest Neighbour Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grid search for KNN classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "params = {'n_neighbors': list(range(1,15)),\n",
    "          'metric': ['l1', 'l2', 'cosine'],\n",
    "          'weights': ['uniform', 'distance']}\n",
    "\n",
    "grid_clf = GridSearchCV(\n",
    "    estimator = KNeighborsClassifier(),\n",
    "    scoring = 'f1_macro',   # accuracy, balanced_accuracy, f1, roc_auc, average_precision (=pr_auc)\n",
    "    cv = 3,\n",
    "    param_grid = params)\n",
    "\n",
    "grid_clf.fit(Xtrain, Ytrain_numeric)\n",
    "print('Best params: ', grid_clf.best_params_)\n",
    "print(grid_clf.best_estimator_)\n",
    "print(classification_report(y_test1_numeric, grid_clf.predict(sparse_test1)))\n",
    "scores = abs(grid_clf.cv_results_['mean_test_score'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (47101, 77248)\n",
      "../semeval-tweets/twitter-test1.txt (KNN): 0.423\n",
      "../semeval-tweets/twitter-test2.txt (KNN): 0.472\n",
      "../semeval-tweets/twitter-test3.txt (KNN): 0.418\n",
      "overall run time: 9.64136290550232\n",
      "average F1 score: 0.4374543519553146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "def sent2num(sent):\n",
    "    if sent == 'negative':\n",
    "        return -1\n",
    "    if sent == 'neutral':\n",
    "        return 0\n",
    "    if sent == 'positive':\n",
    "        return 1\n",
    "\n",
    "def num2sent(num):\n",
    "    if num == -1:\n",
    "        return 'negative'\n",
    "    if num == 0:\n",
    "        return 'neutral'\n",
    "    if num == 1:\n",
    "        return 'positive'\n",
    "\n",
    "# BOW_train, BOW_dev -> non-sparse implementation\n",
    "# IDtrain = list(BOW_train.keys())            # list of IDs in train set\n",
    "# IDdev = list(BOW_dev.keys())                # list of IDs in development (validation) set\n",
    "# Xtrain = np.array([BOW_train[id] for id in IDtrain])     # training set\n",
    "# Ytrain = np.array([sent_train[id] for id in IDtrain])    # training labels\n",
    "# Ytrain_numeric = np.array([sent2num(y) for y in Ytrain]) # numerical labels\n",
    "# Xdev = np.array([BOW_dev[id] for id in IDdev])  # development set\n",
    "# Ydev = np.array([sent_dev[id] for id in IDdev]) # training labels\n",
    "# Ydev_numeric = np.array([sent2num(y) for y in Ydev])\n",
    "\n",
    "# sparse implementation\n",
    "Xtrain = sparse_train_dev  # combining the two datasets\n",
    "print(\"shape\", Xtrain.shape)###\n",
    "ID_train = list(BOW_train.keys())            # list of IDs in train set\n",
    "ID_dev = list(BOW_dev.keys())\n",
    "ID_train_dev = ID_train + ID_dev\n",
    "sn_train = [sent_train[id] for id in ID_train]  # training labels train\n",
    "sn_dev = [sent_dev[id] for id in ID_dev]        # training labels dev\n",
    "Ytrain = np.array(sn_train)            # combining both labels\n",
    "Ytrain = np.array(sn_train + sn_dev)###            # combining both labels\n",
    "Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])   # numerical labels\n",
    "\n",
    "t0 = time.time() # timing the run\n",
    "clf = KNeighborsClassifier(n_neighbors=9, metric='cosine', weights='uniform')\n",
    "clf.fit(Xtrain, Ytrain_numeric)\n",
    "# y_scores = clf.predict_proba(Xdev)[:,1]\n",
    "\n",
    "\n",
    "# evaluation test1\n",
    "ID_test1 = list(BOW_test1.keys())\n",
    "y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test1)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test1, y_pred))\n",
    "s1 = evaluate(pred_dict, '../semeval-tweets/twitter-test1.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test2\n",
    "ID_test2 = list(BOW_test2.keys())\n",
    "y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test2)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test2, y_pred))\n",
    "s2 = evaluate(pred_dict, '../semeval-tweets/twitter-test2.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test3\n",
    "ID_test3 = list(BOW_test3.keys())\n",
    "y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test3)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test3, y_pred))\n",
    "s3 = evaluate(pred_dict, '../semeval-tweets/twitter-test3.txt', classifier=\"KNN\")  # best score 0.435\n",
    "t1 = time.time()  # timing the run\n",
    "print('overall run time:', t1-t0)\n",
    "print(\"average F1 score:\", (s1 + s2 + s3)/3)\n",
    "\n",
    "\n",
    "# other performance metrics\n",
    "#accuracy = np.mean(y_pred_numeric == y_test1_numeric)\n",
    "#confusion(pred_dict, '../semeval-tweets/twitter-test1.txt', classifier=\"KNN\")\n",
    "\n",
    "\n",
    "# conf = confusion_matrix(y_test1_numeric, y_pred_numeric)\n",
    "# print('  -1   0   1')\n",
    "# print(conf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayes Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Multinomial NaiveBayes: finding the best params using GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "params = {'alpha': np.logspace(2,-7, num=50)}\n",
    "\n",
    "grid_clf = GridSearchCV(\n",
    "    estimator = MultinomialNB(),\n",
    "    scoring = 'f1_macro',\n",
    "    cv = 3,\n",
    "    param_grid = params)\n",
    "\n",
    "grid_clf.fit(Xtrain, Ytrain_numeric)\n",
    "print('Best params: ', grid_clf.best_params_)\n",
    "print(grid_clf.best_estimator_)\n",
    "print(classification_report(y_test1_numeric, grid_clf.predict(sparse_test1)))\n",
    "scores = abs(grid_clf.cv_results_['mean_test_score'])\n",
    "\n",
    "# Best params:  {'alpha': 0.4094915062380423} -> macro avg f1  0.52"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (47101, 77248)\n",
      "../semeval-tweets/twitter-test1.txt (KNN): 0.480\n",
      "../semeval-tweets/twitter-test2.txt (KNN): 0.478\n",
      "../semeval-tweets/twitter-test3.txt (KNN): 0.465\n",
      "overall run time: 0.055906057357788086\n",
      "average F1 score: 0.4739579128181746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "Xtrain = sparse_train_dev  # combining the two datasets\n",
    "print(\"shape\", Xtrain.shape)###\n",
    "ID_train = list(BOW_train.keys())            # list of IDs in train set\n",
    "ID_dev = list(BOW_dev.keys())\n",
    "ID_train_dev = ID_train + ID_dev\n",
    "sn_train = [sent_train[id] for id in ID_train]  # training labels train\n",
    "sn_dev = [sent_dev[id] for id in ID_dev]        # training labels dev\n",
    "Ytrain = np.array(sn_train)            # combining both labels\n",
    "Ytrain = np.array(sn_train + sn_dev)###            # combining both labels\n",
    "Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])   # numerical labels\n",
    "\n",
    "t0 = time.time() # timing the run\n",
    "clf = MultinomialNB(alpha = 0.4)\n",
    "clf.fit(Xtrain,Ytrain_numeric)\n",
    "#y_scores = clf.predict_proba(Xv)[:,1]\n",
    "\n",
    "# evaluation test1\n",
    "ID_test1 = list(BOW_test1.keys())\n",
    "y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test1)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test1, y_pred))\n",
    "s1 = evaluate(pred_dict, '../semeval-tweets/twitter-test1.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test2\n",
    "ID_test2 = list(BOW_test2.keys())\n",
    "y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test2)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test2, y_pred))\n",
    "s2 = evaluate(pred_dict, '../semeval-tweets/twitter-test2.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test3\n",
    "ID_test3 = list(BOW_test3.keys())\n",
    "y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test3)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test3, y_pred))\n",
    "s3 = evaluate(pred_dict, '../semeval-tweets/twitter-test3.txt', classifier=\"KNN\")  # best score 0.435\n",
    "t1 = time.time()  # timing the run\n",
    "\n",
    "print('overall run time:', t1-t0)\n",
    "print(\"average F1 score:\", (s1 + s2 + s3)/3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear SVM classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'C': 0.020000000000000004, 'class_weight': 'balanced', 'max_iter': 2000, 'tol': 0.01}\n",
      "LinearSVC(C=0.020000000000000004, class_weight='balanced', max_iter=2000,\n",
      "          tol=0.01)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.64      0.40      0.49       557\n",
      "           0       0.61      0.78      0.68      1504\n",
      "           1       0.73      0.63      0.67      1470\n",
      "\n",
      "    accuracy                           0.66      3531\n",
      "   macro avg       0.66      0.60      0.62      3531\n",
      "weighted avg       0.67      0.66      0.65      3531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LinearSVM: finding the best params using GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "params = {'C':np.linspace(0.005,  0.05, 10),\n",
    "          'class_weight':['balanced'],\n",
    "          'tol': [0.01, 0.001],\n",
    "          'max_iter': [2000]\n",
    "          }\n",
    "\n",
    "grid_clf = GridSearchCV(\n",
    "    estimator = LinearSVC(),\n",
    "    scoring = 'f1_macro',\n",
    "    cv = 3,\n",
    "    param_grid = params)\n",
    "\n",
    "grid_clf.fit(Xtrain, Ytrain_numeric)\n",
    "print('Best params: ', grid_clf.best_params_)\n",
    "print(grid_clf.best_estimator_)\n",
    "print(classification_report(y_test1_numeric, grid_clf.predict(sparse_test1)))\n",
    "scores = abs(grid_clf.cv_results_['mean_test_score'])\n",
    "\n",
    "# best params -> LinearSVC(C=0.0200, class_weight='balanced', max_iter=2000, tol=0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (47101, 77248)\n",
      "../semeval-tweets/twitter-test1.txt (KNN): 0.581\n",
      "../semeval-tweets/twitter-test2.txt (KNN): 0.591\n",
      "../semeval-tweets/twitter-test3.txt (KNN): 0.540\n",
      "overall run time: 0.29659414291381836\n",
      "average F1 score: 0.5707718210480864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "Xtrain = sparse_train_dev  # combining the two datasets\n",
    "print(\"shape\", Xtrain.shape)###\n",
    "ID_train = list(BOW_train.keys())            # list of IDs in train set\n",
    "ID_dev = list(BOW_dev.keys())\n",
    "ID_train_dev = ID_train + ID_dev\n",
    "sn_train = [sent_train[id] for id in ID_train]  # training labels train\n",
    "sn_dev = [sent_dev[id] for id in ID_dev]        # training labels dev\n",
    "Ytrain = np.array(sn_train)            # combining both labels\n",
    "Ytrain = np.array(sn_train + sn_dev)###            # combining both labels\n",
    "Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])   # numerical labels\n",
    "\n",
    "t0 = time.time() # timing the run\n",
    "clf = LinearSVC(C=0.0200, class_weight='balanced', max_iter=2000, tol=0.01)\n",
    "clf.fit(Xtrain,Ytrain_numeric)\n",
    "#y_scores = clf.predict_proba(Xv)[:,1]\n",
    "\n",
    "# evaluation test1\n",
    "ID_test1 = list(BOW_test1.keys())\n",
    "y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test1)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test1, y_pred))\n",
    "s1 = evaluate(pred_dict, '../semeval-tweets/twitter-test1.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test2\n",
    "ID_test2 = list(BOW_test2.keys())\n",
    "y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test2)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test2, y_pred))\n",
    "s2 = evaluate(pred_dict, '../semeval-tweets/twitter-test2.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test3\n",
    "ID_test3 = list(BOW_test3.keys())\n",
    "y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test3)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test3, y_pred))\n",
    "s3 = evaluate(pred_dict, '../semeval-tweets/twitter-test3.txt', classifier=\"KNN\")  # best score 0.435\n",
    "t1 = time.time()  # timing the run\n",
    "\n",
    "print('overall run time:', t1-t0)\n",
    "print(\"average F1 score:\", (s1 + s2 + s3)/3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Comparison of all classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "for classifier in ['NearestNeighbour', 'NaiveBayes','SVM']:\n",
    "    for features in ['BOW', '<feature-2-name>']:\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if classifier == 'NearestNeighbour':\n",
    "            # write the svm classifier here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'NaiveBayes':\n",
    "            # write the classifier 2 here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'SVM':\n",
    "            # write the classifier 3 here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'LSTM':\n",
    "            # write the LSTM classifier here\n",
    "            if features == 'bow':\n",
    "                continue\n",
    "            print('Training ' + classifier)\n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "\n",
    "        # Predition performance of the classifiers\n",
    "        for testset in testsets:\n",
    "            id_preds = {}\n",
    "            # write the prediction and evaluation code here\n",
    "\n",
    "            testset_name = testset\n",
    "            testset_path = join('semeval-tweets', testset_name)\n",
    "            evaluate(id_preds, testset_path, features + '-' + classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
