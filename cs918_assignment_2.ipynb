{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a *Jupyter notebook* for the coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, donâ€™t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n",
    "You may import more packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages here\n",
    "import re\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../semeval-tweets/twitter-test1.txt', '../semeval-tweets/twitter-test2.txt', '../semeval-tweets/twitter-test3.txt']\n"
     ]
    }
   ],
   "source": [
    "# Define test sets\n",
    "dataDir = '../semeval-tweets'\n",
    "testsetStrings = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']\n",
    "testsets = [join(dataDir, t) for t in testsetStrings]\n",
    "print(testsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght: 139\n",
      "102313285628711403\n",
      "neutral\n",
      "\"Bing one-ups knowledge graph, hires Encyclopaedia Britannica to supply results:   It may have retired from the cut-throat world of pr...\"\n",
      "\n",
      "Lenght: 139\n",
      "653274888624828198\n",
      "neutral\n",
      "\"On Thursday, concealed-carry gun license holders will be given a new right in the state of Oklahoma: the ability... http://t.co/oSgGHKi1\"\n",
      "\n",
      "Lenght: 137\n",
      "420747042670198316\n",
      "negative\n",
      "Miyagi just got banned from yoga. He was caught him sniffing the sphincter of the girl in front of him. There may be police involvement!\n",
      "\n",
      "Lenght: 135\n",
      "822064800445716046\n",
      "neutral\n",
      "Join us tonight at Boston Pizza - Centre on Barton for THURSDAY NIGHT FOOTBALL! Tonight the Chiefs take on the... http://t.co/iegTxPQv\n",
      "\n",
      "Lenght: 139\n",
      "055480020953212084\n",
      "neutral\n",
      "\"#FX NEW YORK, Oct 18 (Reuters) - The Federal Reserve provided $4.701 billion of liquidity to the ... http://t.co/BJhIQTtO #EUR #AUD #CAD\"\n",
      "\n",
      "Lenght: 132\n",
      "429443270273347255\n",
      "neutral\n",
      "\"13 April 1996, History is made, as the MetroStars and the Los Angeles Galaxy meet at the Rose Bowl in Pasadena, California, (1/3)\"\n",
      "\n",
      "Lenght: 135\n",
      "220323262844863802\n",
      "neutral\n",
      "Make sure you tune into The Saga exclusively on Temple's only student run radio WHIP at iHeartRadio Monday through Thursday's at 9pm!!\n",
      "\n",
      "Lenght: 135\n",
      "266953303729385574\n",
      "positive\n",
      "\"Saturday Nov 17th, 2012 Q Bar and Grill is the Place to be!  UFC 90's Event with Special Guests:  DJ BOBBY D... http://t.co/z96soXQf\"\n",
      "\n",
      "Lenght: 134\n",
      "267372990522222442\n",
      "negative\n",
      "@Waka_BacaFlame I'm already on my 3rd year at Colton it wouldn't make sense to graduate from gt after spending all this time here >.<\n",
      "\n",
      "Lenght: 131\n",
      "578446202147364926\n",
      "neutral\n",
      "\"Ephesians 6:11 (NEB) - Ephesians 6:11 (NEB) Put on all the armour which God provides, so that you may be... http://t.co/cEt5WKVK\"\n",
      "\n",
      "Lenght: 141\n",
      "810763059461202071\n",
      "neutral\n",
      "Andando in bicicletta a Roma -- si puo' dividere (share) pure li! Shareable: Changing Bike Culture in the Eternal City: http://t.co/nT3c7ryI\n",
      "\n",
      "Lenght: 132\n",
      "885726184364930528\n",
      "positive\n",
      ".@Q1047 Would luv if you could make an announcement about Chris Rene concert @theroxy this Thursday!! Thanks!! http://t.co/aKzopIrd\n",
      "\n",
      "Lenght: 141\n",
      "960689731242555262\n",
      "negative\n",
      "So now it seems Daav(Conspiracy) of Yadav have back fired...and if true/proved may create troubles for Akhilesh Yadav and Samajwadi Party...\n",
      "\n",
      "Lenght: 139\n",
      "491718215382409200\n",
      "neutral\n",
      "\"Charlie Rose with Desmond Tutu; Bill Joy (October 5, 1999): The Archbishop of Cape Town, South Africa and Nobel ... http://t.co/BVoS0dnd\"\n",
      "\n",
      "Lenght: 143\n",
      "317970942816831516\n",
      "neutral\n",
      "\"Remember folks. Grabbed 3 more earnings plays today. CELG,CME and HSY all calls for tomorrow!!! 5/5 for the week   $CME http://t.co/ZcmjGsha\"\n",
      "\n",
      "Lenght: 137\n",
      "281466922061596703\n",
      "neutral\n",
      "\"Deadly Israeli strike, fire from Gaza mar truce: A deadly Israeli airstrike into the Gaza Strip and rocket and... http://t.co/E0h27Mcg\"\n",
      "\n",
      "Lenght: 137\n",
      "076653802834383376\n",
      "positive\n",
      "PST Keane delivers best MLS goal from Saturday: The Los Angeles Galaxy were on top of things early tonight again... http://t.co/aVYSaVOo\n",
      "\n",
      "Lenght: 142\n",
      "629377153435614908\n",
      "negative\n",
      "\"Heather is like the serpent in the garden trying 2 entice info from McBain! Michael Easton, dont ever go away that long! Watching Tues #GH.\"\n",
      "\n",
      "Lenght: 142\n",
      "255090901012423499\n",
      "neutral\n",
      "\"Southampton's Richard Bland is -9 after his 2nd round in the Madeira Islands Open on the European Tour. Currently tied 4th, 3 off the lead.\"\n",
      "\n",
      "Lenght: 135\n",
      "581663993931072193\n",
      "neutral\n",
      "\"Overexposed Tour, Maroon 5 With The Cab: Through The Eyes Of A Drummer - As you may have heard, The Cab just... http://t.co/H3c7kQsw\"\n",
      "\n",
      "Lenght: 136\n",
      "854756032273426749\n",
      "neutral\n",
      "Remember to join the Springboks at 3pm tomorrow at Orlando Stadium to watch their open training session as they... http://t.co/vaCN6EZ0\n",
      "\n",
      "Lenght: 137\n",
      "385766324957619690\n",
      "positive\n",
      "@MichelleLMyers #BreakingDawnPart2 The Saga may end but the love and friendships will last 4ever #LoveIsLouderthanRollingTheFinalCredits\n",
      "\n",
      "Lenght: 134\n",
      "844654517192428026\n",
      "positive\n",
      "\"Happy 102 years to L.A. artist Tyrus Wong! May the road rise to meet you, may the wind be always at your back: http://t.co/e8SB5S90\"\n",
      "\n",
      "Lenght: 137\n",
      "233928065818229615\n",
      "neutral\n",
      "Federal Reserve stands firm on bond-buying program: The Federal Reserve said on Wednesday that it is s... http://t.co/KnRMVgLu #business\n",
      "\n",
      "Lenght: 139\n",
      "226170643240610310\n",
      "positive\n",
      "\"Bersani, supports stability agreement to secure government: (AGI) Rome, October 24 - Tomorrow the agreement on t... http://t.co/ew0JWQgD\"\n",
      "\n",
      "Lenght: 141\n",
      "282416648112742867\n",
      "neutral\n",
      "\"About to watch Kony 2012 video. Despite 100m views, there are quite a number here watching for 1st time. Interested in the reactions.#yuga\"\n",
      "\n",
      "Lenght: 139\n",
      "790411654414429445\n",
      "neutral\n",
      "\"LG may update the Optimus 2X to ICS after all, releases source code: The saga of the elusive ICS update for the ... http://t.co/g0hT0NtX\"\n",
      "\n",
      "Lenght: 136\n",
      "712768996713302192\n",
      "positive\n",
      "Saturday night Adult Swim (Cowboy Bebop + Ghost in the Shell) reminds me how much I adore Yoko Kanno's music. Such an amazing composer.\n",
      "\n",
      "Lenght: 136\n",
      "170040141580222961\n",
      "neutral\n",
      "Just one of the songs to expect during THE FRAY's concert in Smart Araneta Coliseum on November 10. This is off... http://t.co/BUVMOlcM\n",
      "\n",
      "Lenght: 139\n",
      "836541798391946839\n",
      "neutral\n",
      "\"Seven Penny Stocks on the Move with Heavy Volume, April 24: CSOC shares have traded as high as $.18 over the pas... http://t.co/OX7oCx72\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the structure of the dataset\n",
    "with open(testsets[0], 'r', encoding='utf8') as f1:\n",
    "    i = 0\n",
    "    for line in f1:\n",
    "        fields = line.split('\\t')\n",
    "        if i < 30:\n",
    "            length = len(fields[2])\n",
    "            if length > 130:\n",
    "                print(f\"Lenght: {length}\")\n",
    "                print(fields[0])  # 1st column - tweet ID\n",
    "                print(fields[1])  # 2nd column - tweet sentiment\n",
    "                print(fields[2])  # 3rd column - tweet text\n",
    "                i += 1\n",
    "\n",
    "# preprocessing questions and notes: \n",
    "  # -> what about removing the @usernames, is it advisable?\n",
    "  # -> need to remove URLs!\n",
    "  # -> There is a lot of noise/mistakes in the data and absence of interpunction.\n",
    "  # -> what about adding of the starting token?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    reading the testset and return a dictionary with: ID -> sentiment\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}  # init the dictionary\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            positive  negative  neutral\n",
      "positive    1.000     0.000     0.000     \n",
      "negative    0.000     1.000     0.000     \n",
      "neutral     0.000     0.000     1.000     \n",
      "\n",
      "../semeval-tweets/twitter-test1.txt (PerfectClassifier): 1.000\n"
     ]
    }
   ],
   "source": [
    "# testing the evaluation functions\n",
    "tweetDict = read_test('../semeval-tweets/twitter-test1.txt')\n",
    "confusion(tweetDict, '../semeval-tweets/twitter-test1.txt', \"PerfectClassifier\")\n",
    "evaluate(tweetDict, '../semeval-tweets/twitter-test1.txt', \"PerfectClassifier\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, dev set and testing set\n",
    "\n",
    "dataDir = '../semeval-tweets'  # change to the proper directory\n",
    "datasetStrings = ['twitter-training-data.txt', 'twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt', 'twitter-dev-data.txt']\n",
    "datasets = [join(dataDir, t) for t in datasetStrings]\n",
    "\n",
    "tweet_IDs = {}          # init dictionary with tweet IDs\n",
    "tweet_sentiments = {}   # init dictionary with sentiments\n",
    "tweet_texts = {}        # init dictionary with tweet texts\n",
    "\n",
    "for DatasetString in datasets:\n",
    "    data_ID, data_sent, data_text  = {}, {}, {}    # temp dictionaries\n",
    "    with open(DatasetString, 'r', encoding='utf8') as f1:\n",
    "        for i, line in enumerate(f1):\n",
    "            fields = line.split('\\t')\n",
    "            data_ID[i] = fields[0]            # tweet IDs\n",
    "            data_sent[fields[0]] = fields[1]  # sentiments\n",
    "            data_text[fields[0]] = fields[2]  # tweet text\n",
    "    tweet_IDs[DatasetString] = data_ID\n",
    "    tweet_sentiments[DatasetString] = data_sent\n",
    "    tweet_texts[DatasetString] = data_text\n",
    "\n",
    "# sentiment dictionaries\n",
    "sent_train = tweet_sentiments[datasets[0]]\n",
    "sent_test1 = tweet_sentiments[datasets[1]]\n",
    "sent_test2 = tweet_sentiments[datasets[2]]\n",
    "sent_test3 = tweet_sentiments[datasets[3]]\n",
    "sent_dev = tweet_sentiments[datasets[4]]\n",
    "\n",
    "# tweet text dictionaries\n",
    "text_train = tweet_texts[datasets[0]]\n",
    "text_test1 = tweet_texts[datasets[1]]\n",
    "text_test2 = tweet_texts[datasets[2]]\n",
    "text_test3 = tweet_texts[datasets[3]]\n",
    "text_dev = tweet_texts[datasets[4]]\n",
    "\n",
    "# tweet IDs dictionaries\n",
    "IDs_train = tweet_IDs[datasets[0]]\n",
    "IDs_test1 = tweet_IDs[datasets[1]]\n",
    "IDs_test2 = tweet_IDs[datasets[2]]\n",
    "IDs_test3 = tweet_IDs[datasets[3]]\n",
    "IDs_dev = tweet_IDs[datasets[4]]\n",
    "\n",
    "\n",
    "## examples and tests\n",
    "# id = IDs_train[0]\n",
    "# id_dev = IDs_dev[0]\n",
    "# id1 = IDs_test1[0]\n",
    "# id2 = IDs_test2[0]\n",
    "# id3 = IDs_test3[0]\n",
    "# print(f\"-ID:{id} \\n-TEXT:{text_train[id]}-SENTIMENT: {sent_train[id]}\\n\")\n",
    "# print(f\"-ID:{id_dev} \\n-TEXT:{text_dev[id_dev]}-SENTIMENT: {sent_dev[id_dev]}\\n\")\n",
    "# print(f\"-ID:{id1} \\n-TEXT:{text_test1[id1]}-SENTIMENT: {sent_test1[id1]}\\n\")\n",
    "# print(f\"-ID:{id2} \\n-TEXT:{text_test2[id2]}-SENTIMENT: {sent_test2[id2]}\\n\")\n",
    "# print(f\"-ID:{id3} \\n-TEXT:{text_test3[id3]}-SENTIMENT: {sent_test3[id3]}\\n\")\n",
    "# print(len(IDs_train.keys()), len(text_train.keys()), len(sent_train.keys()))  # 45101\n",
    "# print(len(IDs_test1.keys()), len(text_test1.keys()), len(sent_test1.keys()))  # 3531\n",
    "# print(len(IDs_test2.keys()), len(text_test2.keys()), len(sent_test2.keys()))  # 1853\n",
    "# print(len(IDs_test3.keys()), len(text_test3.keys()), len(sent_test3.keys()))  # 2379\n",
    "# print(len(IDs_dev.keys()), len(text_dev.keys()), len(sent_dev.keys()))        # 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "#### Order of preprocessing\n",
    "* lowercase text\n",
    "* regex cleaning\n",
    "   * Remove URLs\n",
    "   * Remove non-alphanumeric characters (leave hashtags and usernames)\n",
    "   * Remove numbers that are fully made of digits\n",
    "   * (Remove words with only 1 character)\n",
    "\n",
    " #### Preprocessing questions and notes:\n",
    "   -> what about removing the @usernames, is it advisable?\n",
    "   -> need to remove URLs!\n",
    "   -> There is a lot of noise/mistakes in the data and absence of interpunction.\n",
    "   -> what about adding of the starting token?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Deleted link]\n",
      "[Deleted link]\n",
      "[Deleted link]\n",
      "[Deleted link]\n",
      "U.S.A.\n",
      "[Deleted link]\n",
      "[Deleted link]\n",
      "ltd.\n",
      "etc.\n",
      "[Deleted link]\n",
      "I said 'yes'.But I did not say Why.\n",
      "[Deleted link]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top100 = ['com', 'net', 'org', 'jp', 'de', 'uk', 'fr', 'br', 'it', 'ru', 'es', 'me', 'gov', 'pl', 'ca', 'au', 'cn', 'co', 'in', 'nl', 'edu', 'info', 'eu', 'ch', 'id', 'at', 'kr', 'cz', 'mx', 'be', 'tv', 'se', 'tr', 'tw', 'al', 'ua', 'ir', 'vn', 'cl', 'sk', 'ly', 'cc', 'to', 'no', 'fi', 'us', 'pt', 'dk', 'ar', 'hu', 'tk', 'gr', 'il', 'news', 'ro', 'my', 'biz', 'ie', 'za', 'nz', 'sg', 'ee', 'th', 'io', 'xyz', 'pe', 'bg', 'hk', 'rs', 'lt', 'link', 'ph', 'club', 'si', 'site', 'mobi', 'by', 'cat', 'wiki', 'la', 'ga', 'xxx', 'cf', 'hr', 'ng', 'jobs', 'online', 'kz', 'ug', 'gq', 'ae', 'is', 'lv', 'pro', 'fm', 'tips', 'ms', 'sa', 'app', 'lat']\n",
    "\n",
    "\n",
    "text = '''\n",
    "www.abc.com\n",
    "www.bcd.net\n",
    "www.sss.cc\n",
    "www.dcamp.uk\n",
    "U.S.A.\n",
    "google.fr\n",
    "google.it\n",
    "ltd.\n",
    "etc.\n",
    "sell.uk\n",
    "I said 'yes'.But I did not say Why.\n",
    "https://www.scoutdns.com/100-most-popular-tlds-by-google-index/\n",
    "'''\n",
    "\n",
    "\n",
    "for ext in top100:\n",
    "    re_string = \"[^\\s]*\\.\" + ext + \"[^\\s]*\"\n",
    "    new_text = re.sub(re_string, '[Deleted link]', text)\n",
    "    text = new_text\n",
    "print(new_text)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "if os.path.isfile(\"preprocessing.pkl\"):  # loading preprocessed datasets\n",
    "    with open('preprocessing.pkl', 'rb') as inp_file:\n",
    "        temp_dicts = pickle.load(inp_file)\n",
    "        txt_dicts = temp_dicts[0:5]\n",
    "        txtlist_dicts = temp_dicts[5:]\n",
    "\n",
    "else:\n",
    "    ID_dicts = [IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev]\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev]\n",
    "    txtlist_dicts = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  # init the lemmatizer\n",
    "    POSconvert = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "\n",
    "    for i, IDdict in enumerate(ID_dicts):\n",
    "        output = txt_dicts[i]\n",
    "        output_txt = {}\n",
    "        for id in IDdict.values():\n",
    "            text = output[id].lower()\n",
    "\n",
    "            # replace/delete all URLs starting with 'http' and 'www'\n",
    "            new_text = re.sub(\"http[^\\s]*\", '', text)\n",
    "            new_text = re.sub(\"www[^\\s]*\", '', new_text)\n",
    "\n",
    "            # delete all URLs which have one of 100 most common extensions ('.com', '.net', ...)\n",
    "            top100 = ['com', 'net', 'org', 'jp', 'de', 'uk', 'fr', 'br', 'it', 'ru', 'es', 'me', 'gov', 'pl', 'ca', 'au', 'cn', 'co', 'in', 'nl', 'edu', 'info', 'eu', 'ch', 'id', 'at', 'kr', 'cz', 'mx', 'be', 'tv', 'se', 'tr', 'tw', 'al', 'ua', 'ir', 'vn', 'cl', 'sk', 'ly', 'cc', 'to', 'no', 'fi', 'us', 'pt', 'dk', 'ar', 'hu', 'tk', 'gr', 'il', 'news', 'ro', 'my', 'biz', 'ie', 'za', 'nz', 'sg', 'ee', 'th', 'io', 'xyz', 'pe', 'bg', 'hk', 'rs', 'lt', 'link', 'ph', 'club', 'si', 'site', 'mobi', 'by', 'cat', 'wiki', 'la', 'ga', 'xxx', 'cf', 'hr', 'ng', 'jobs', 'online', 'kz', 'ug', 'gq', 'ae', 'is', 'lv', 'pro', 'fm', 'tips', 'ms', 'sa', 'app', 'lat']\n",
    "            for ext in top100:\n",
    "                re_string = \"[^\\s]*\\.\" + ext + \"[^\\s]*\"\n",
    "                new_text = re.sub(re_string, '', new_text)\n",
    "\n",
    "            # removing '&amp'\n",
    "            new_text = re.sub('&amp','', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars except for '# and @'\n",
    "            new_text = re.sub('[^\\w\\s@#]','', new_text)\n",
    "\n",
    "            # remove strings with '#' not on the beginning (to keep only hashtags)\n",
    "            new_text = re.sub('\\s[\\w]+#[\\w]*','', new_text)\n",
    "\n",
    "            # numbers fully made of digits\n",
    "            new_text = re.sub('[\\d]+\\s','', new_text)\n",
    "\n",
    "            # remove words with only 1 character\n",
    "            new_text = re.sub('\\\\b\\\\w{1}\\\\b','', new_text)\n",
    "\n",
    "            # remove newline chars\n",
    "            new_text = new_text.replace('\\n', ' ')\n",
    "\n",
    "            # replace a multiple spaces with a single space\n",
    "            new_text = re.sub('\\s+',' ', new_text)\n",
    "\n",
    "            # using the lemmatizer\n",
    "            txt_list = nltk.word_tokenize(new_text)\n",
    "            for k, word in enumerate(txt_list):  # fixing the separation of hashtags by the tokenizer\n",
    "                if word == '#' or word == '@':\n",
    "                    if k < len(txt_list) - 1:\n",
    "                        txt_list[k] = txt_list[k] + txt_list[k+1]\n",
    "                        txt_list.pop(k+1)\n",
    "            POS = nltk.pos_tag(txt_list)                  # POS tags from nltk\n",
    "            WordNetPOS = [POSconvert(P[1]) for P in POS]  # POS tags for lemmatizer\n",
    "            for j in range(len(txt_list)):\n",
    "                word = txt_list[j]\n",
    "                lemmatized = lemmatizer.lemmatize(word, WordNetPOS[j])  # process each token/word one by one\n",
    "                txt_list[j] = lemmatized  # update the word in the txt_list\n",
    "\n",
    "            ## UPDATE the dictionary\n",
    "            output_txt[id] = ' '.join(txt_list)\n",
    "            output[id] = txt_list\n",
    "\n",
    "        txt_dicts[i] = output_txt\n",
    "        txtlist_dicts.append(output)\n",
    "\n",
    "text_train = txt_dicts[0]\n",
    "text_test1 = txt_dicts[1]\n",
    "text_test2 = txt_dicts[2]\n",
    "text_test3 = txt_dicts[3]\n",
    "text_dev = txt_dicts[4]\n",
    "txtlist_train = txtlist_dicts[0]\n",
    "txtlist_test1 = txtlist_dicts[1]\n",
    "txtlist_test2 = txtlist_dicts[2]\n",
    "txtlist_test3 = txtlist_dicts[3]\n",
    "txtlist_dev = txtlist_dicts[4]\n",
    "\n",
    "# saving preprocessing.pkl\n",
    "if not os.path.isfile(\"preprocessing.pkl\"):\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev, txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev]\n",
    "    with open('preprocessing.pkl', 'wb') as out_file:\n",
    "        pickle.dump(txt_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: Bag of words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# Bag of Words - my implementation:\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords as Stopwords\n",
    "\n",
    "# 1) removing stop words\n",
    "stopwords = Stopwords.words('english')\n",
    "stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "# 2) extracting the dictionary/vocabulary\n",
    "freq = FreqDist()   # frequency distribution\n",
    "for Dict in txtlist_dicts:\n",
    "    for tweet in Dict.values():\n",
    "        for word in tweet:\n",
    "            if not word in stopwords:\n",
    "                freq[word] += 1\n",
    "\n",
    "nums = range(len(freq.keys()))\n",
    "vocabulary = list(freq.keys())              # creating the dictionary\n",
    "vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out of vocabulary' word\n",
    "vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words -> len: 69742\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its BoW representation as np.array\n",
    "def text2BOW(text_list, vocabulary, stopwords):\n",
    "    BOW_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in text_list:\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                BOW_vec[vocab2num[word]] += 1\n",
    "            else:\n",
    "                BOW_vec[vocab2num['<OOV>']] += 1\n",
    "    return BOW_vec\n",
    "\n",
    "# if os.path.isfile(\"BOWs.pkl\"):  # loading preprocessed datasets\n",
    "#     with open('BOWs.pkl', 'rb') as inp_file:\n",
    "#         ll = pickle.load(inp_file)\n",
    "#         BOW_train, BOW_test1, BOW_test2, BOW_test3, BOW_dev = ll[0], ll[1], ll[2], ll[3], ll[4]\n",
    "#\n",
    "# else:\n",
    "# Bag of Words (BOW) for each tweet\n",
    "BOW_train = {}\n",
    "for ID, tweet in txtlist_train.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_train[ID] = BOW\n",
    "\n",
    "BOW_test1 = {}\n",
    "for ID, tweet in txtlist_test1.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_test1[ID] = BOW\n",
    "\n",
    "BOW_test2 = {}\n",
    "for ID, tweet in txtlist_test2.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_test2[ID] = BOW\n",
    "\n",
    "BOW_test3 = {}\n",
    "for ID, tweet in txtlist_test3.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_test3[ID] = BOW\n",
    "\n",
    "BOW_dev = {}\n",
    "for ID, tweet in txtlist_dev.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_dev[ID] = BOW\n",
    "\n",
    "\n",
    "# saving BOWs.pkl: very large file - maybe not the best idea to save it?\n",
    "    # if not os.path.isfile(\"BOWs.pkl\"):\n",
    "    #     BOW_dicts = [BOW_train, BOW_test1, BOW_test2, BOW_test3, BOW_dev]\n",
    "    #     with open(\"BOWs.pkl\", 'wb') as out_file:\n",
    "    #         pickle.dump(BOW_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "# sparse representation\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "sparse_train = {}\n",
    "for ID, vec in BOW_train.items():\n",
    "    sparse_train[ID] = csr_matrix(vec)\n",
    "\n",
    "sparse_test1 = {}\n",
    "for ID, vec in BOW_test1.items():\n",
    "    sparse_test1[ID] = csr_matrix(vec)\n",
    "\n",
    "sparse_test2 = {}\n",
    "for ID, vec in BOW_test2.items():\n",
    "    sparse_test2[ID] = csr_matrix(vec)\n",
    "\n",
    "sparse_test3 = {}\n",
    "for ID, vec in BOW_test3.items():\n",
    "    sparse_test3[ID] = csr_matrix(vec)\n",
    "\n",
    "sparse_dev = {}\n",
    "for ID, vec in BOW_dev.items():\n",
    "    sparse_dev[ID] = csr_matrix(vec)\n",
    "\n",
    "\n",
    "# save the sparse representation\n",
    "sparse_dicts = [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev]\n",
    "with open(\"BOWsparse.pkl\", 'wb') as out_file:\n",
    "    pickle.dump(sparse_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: TF-IDF weighted Bag of words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "# extracting the dictionary\n",
    "# freq = FreqDist()   # frequency distribution\n",
    "# for Dict in txtlist_dicts:\n",
    "#     for tweet in Dict.values():\n",
    "#         for word in tweet:\n",
    "#             if not word in stopwords:\n",
    "#                 freq[word] += 1\n",
    "#\n",
    "# nums = range(len(freq.keys()))\n",
    "# vocabulary = list(freq.keys())              # creating the dictionary\n",
    "# vocabulary_array = np.array(vocabulary)     # np.array of the dictionary\n",
    "# vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out of vocabulary' word\n",
    "# vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "# vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words -> len: 69742\n",
    "\n",
    "# extracting the dictionary\n",
    "DFfreq = FreqDist()   # document frequency distribution\n",
    "Ntexts = len(IDs_train) + len(IDs_test1) + len(IDs_test2) + len(IDs_test3) + len(IDs_dev)\n",
    "for Dict in txtlist_dicts:\n",
    "    for tweet in Dict.values():\n",
    "        for word in np.unique(tweet):\n",
    "            if not word in stopwords:\n",
    "                DFfreq[word] += 1\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its TFIDF representation as np.array\n",
    "def text2TFIDF(text_list, vocabulary, stopwords, Ntexts):\n",
    "    TFIDF_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in np.unique(text_list):\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / DFfreq[word])\n",
    "                TFIDF_vec[vocab2num[word]] = tf * idf\n",
    "            else:\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / 0.000001 )\n",
    "                TFIDF_vec[vocab2num['<OOV>']] = tf * idf\n",
    "    return TFIDF_vec\n",
    "\n",
    "\n",
    "# TFIDF-weighted Bag of Words for each tweet\n",
    "TFIDF_train = {}\n",
    "for ID, tweet in txtlist_train.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_train[ID] = tfidf\n",
    "\n",
    "# TFIDF_test1 = {}\n",
    "# for ID, tweet in txtlist_test1.items():\n",
    "#     tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "#     TFIDF_test1[ID] = tfidf\n",
    "#\n",
    "# TFIDF_test2 = {}\n",
    "# for ID, tweet in txtlist_test2.items():\n",
    "#     tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "#     TFIDF_test2[ID] = tfidf\n",
    "#\n",
    "# TFIDF_test3 = {}\n",
    "# for ID, tweet in txtlist_test3.items():\n",
    "#     tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "#     TFIDF_test3[ID] = tfidf\n",
    "#\n",
    "# TFIDF_dev = {}\n",
    "# for ID, tweet in txtlist_dev.items():\n",
    "#     tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "#     TFIDF_dev[ID] = tfidf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: GloVe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "'../glove/glove.6B.100d.txt'"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = join('..','glove', 'glove.6B.100d.txt')\n",
    "doc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the word vectors.\n",
      "Extracted 400000 word embedding vectors.\n"
     ]
    }
   ],
   "source": [
    "# Loading the word embeddings vectors from\n",
    "print('Extracting the word vectors.')\n",
    "\n",
    "embeddings_dict = {}\n",
    "glove_path = join('..','glove', 'glove.6B.100d.txt')\n",
    "with open(glove_path, 'r', encoding='utf-8') as File:\n",
    "    for line in File:\n",
    "        vec = line.split()\n",
    "        word = vec[0]\n",
    "        coefs = np.asarray(vec[1:], dtype='float32')\n",
    "        embeddings_dict[word] = coefs\n",
    "\n",
    "\n",
    "print(f\"Extracted {len(embeddings_dict)} word embedding vectors.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created matrix with shape (400000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Build an embedding matrix\n",
    "word_list = list(embeddings_dict.keys())\n",
    "nums = range(len(word_list))\n",
    "word2ID = dict(zip(word_list, nums))   # the index of the embedding vector\n",
    "vector_list = [embeddings_dict[word] for word in word_list]\n",
    "embedding_matrix = np.vstack(vector_list)\n",
    "print(f\"Created matrix with shape {embedding_matrix.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.050131  0.66356   0.84315  -0.75295  -0.42848   0.161    -0.03673 ]\n",
      "[-0.050131  0.66356   0.84315  -0.75295  -0.42848   0.161    -0.03673 ]\n"
     ]
    }
   ],
   "source": [
    "# embedings_dict and embedding_matrix work the same way:\n",
    "print(embeddings_dict['yes'][0:7])\n",
    "print(embedding_matrix[word2ID['yes'], 0:7])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nearest Neighbour Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-225-1947b11a4555>\", line 27, in <module>\n",
      "    y_pred = clf.predict(Xdev)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/neighbors/_classification.py\", line 197, in predict\n",
      "    neigh_dist, neigh_ind = self.kneighbors(X)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/neighbors/_base.py\", line 705, in kneighbors\n",
      "    chunked_results = list(pairwise_distances_chunked(\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\", line 1623, in pairwise_distances_chunked\n",
      "    D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\", line 1790, in pairwise_distances\n",
      "    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\", line 1359, in _parallel_pairwise\n",
      "    return func(X, Y, **kwds)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\", line 272, in euclidean_distances\n",
      "    X, Y = check_pairwise_arrays(X, Y)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\", line 149, in check_pairwise_arrays\n",
      "    Y = check_array(Y, accept_sparse=accept_sparse, dtype=dtype,\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 663, in check_array\n",
      "    _assert_all_finite(array,\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 96, in _assert_all_finite\n",
      "    if is_float and (np.isfinite(_safe_accumulator_op(np.sum, X))):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py\", line 689, in _safe_accumulator_op\n",
      "    result = op(x, *args, **kwargs)\n",
      "  File \"<__array_function__ internals>\", line 5, in sum\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 2241, in sum\n",
      "    return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 87, in _wrapreduction\n",
      "    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/anaconda3/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/anaconda3/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/anaconda3/lib/python3.8/inspect.py\", line 745, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m<ipython-input-225-1947b11a4555>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0my_scores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_proba\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mXdev\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m \u001B[0my_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mXdev\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m \u001B[0mpred_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mIDdev\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/neighbors/_classification.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 197\u001B[0;31m         \u001B[0mneigh_dist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mneigh_ind\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkneighbors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    198\u001B[0m         \u001B[0mclasses_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclasses_\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/neighbors/_base.py\u001B[0m in \u001B[0;36mkneighbors\u001B[0;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 705\u001B[0;31m             chunked_results = list(pairwise_distances_chunked(\n\u001B[0m\u001B[1;32m    706\u001B[0m                 \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_X\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduce_func\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreduce_func\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001B[0m in \u001B[0;36mpairwise_distances_chunked\u001B[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001B[0m\n\u001B[1;32m   1622\u001B[0m             \u001B[0mX_chunk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0msl\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1623\u001B[0;31m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001B[0m\u001B[1;32m   1624\u001B[0m                                      n_jobs=n_jobs, **kwds)\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001B[0m in \u001B[0;36mpairwise_distances\u001B[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001B[0m\n\u001B[1;32m   1789\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1790\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_parallel_pairwise\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mY\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1791\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001B[0m in \u001B[0;36m_parallel_pairwise\u001B[0;34m(X, Y, func, n_jobs, **kwds)\u001B[0m\n\u001B[1;32m   1358\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0meffective_n_jobs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_jobs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1359\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mY\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1360\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001B[0m in \u001B[0;36meuclidean_distances\u001B[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001B[0m\n\u001B[1;32m    271\u001B[0m     \"\"\"\n\u001B[0;32m--> 272\u001B[0;31m     \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mY\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_pairwise_arrays\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mY\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    273\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001B[0m in \u001B[0;36mcheck_pairwise_arrays\u001B[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001B[0m\n\u001B[1;32m    148\u001B[0m                         estimator=estimator)\n\u001B[0;32m--> 149\u001B[0;31m         Y = check_array(Y, accept_sparse=accept_sparse, dtype=dtype,\n\u001B[0m\u001B[1;32m    150\u001B[0m                         \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforce_all_finite\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mforce_all_finite\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[1;32m    662\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mforce_all_finite\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 663\u001B[0;31m             _assert_all_finite(array,\n\u001B[0m\u001B[1;32m    664\u001B[0m                                allow_nan=force_all_finite == 'allow-nan')\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36m_assert_all_finite\u001B[0;34m(X, allow_nan, msg_dtype)\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[0mis_float\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkind\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m'fc'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 96\u001B[0;31m     \u001B[0;32mif\u001B[0m \u001B[0mis_float\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misfinite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_safe_accumulator_op\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     97\u001B[0m         \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001B[0m in \u001B[0;36m_safe_accumulator_op\u001B[0;34m(op, x, *args, **kwargs)\u001B[0m\n\u001B[1;32m    688\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 689\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    690\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36msum\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001B[0m in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2240\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2241\u001B[0;31m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001B[0m\u001B[1;32m   2242\u001B[0m                           initial=initial, where=where)\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001B[0m in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mufunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mpasskwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2060\u001B[0m                         \u001B[0;31m# in the engines. This should return a list of strings.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2061\u001B[0;31m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2062\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2061\u001B[0m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2062\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2063\u001B[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001B[0m\u001B[1;32m   2064\u001B[0m                                             value, tb, tb_offset=tb_offset)\n\u001B[1;32m   2065\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1365\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1366\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1367\u001B[0;31m         return FormattedTB.structured_traceback(\n\u001B[0m\u001B[1;32m   1368\u001B[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001B[1;32m   1369\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1265\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mverbose_modes\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1266\u001B[0m             \u001B[0;31m# Verbose modes need a full traceback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1267\u001B[0;31m             return VerboseTB.structured_traceback(\n\u001B[0m\u001B[1;32m   1268\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumber_of_lines_of_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1269\u001B[0m             )\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1122\u001B[0m         \u001B[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1123\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1124\u001B[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001B[0m\u001B[1;32m   1125\u001B[0m                                                                tb_offset)\n\u001B[1;32m   1126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mformat_exception_as_a_whole\u001B[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1081\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1082\u001B[0;31m         \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfind_recursion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0morig_etype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1084\u001B[0m         \u001B[0mframes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat_records\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mfind_recursion\u001B[0;34m(etype, value, records)\u001B[0m\n\u001B[1;32m    380\u001B[0m     \u001B[0;31m# first frame (from in to out) that looks different.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    381\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_recursion_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 382\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    383\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    384\u001B[0m     \u001B[0;31m# Select filename, lineno, func_name to track frames with\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def sent2num(sent):\n",
    "    if sent == 'negative':\n",
    "        return -1\n",
    "    if sent == 'neutral':\n",
    "        return 0\n",
    "    if sent == 'positive':\n",
    "        return 1\n",
    "\n",
    "# BOW_train, BOW_dev -> non-sparse implementation\n",
    "# IDtrain = list(BOW_train.keys())            # list of IDs in train set\n",
    "# IDdev = list(BOW_dev.keys())                # list of IDs in development (validation) set\n",
    "# Xtrain = np.array([BOW_train[id] for id in IDtrain])     # training set\n",
    "# Ytrain = np.array([sent_train[id] for id in IDtrain])    # training labels\n",
    "# Ytrain_numeric = np.array([sent2num(y) for y in Ytrain]) # numerical labels\n",
    "# Xdev = np.array([BOW_dev[id] for id in IDdev])  # development set\n",
    "# Ydev = np.array([sent_dev[id] for id in IDdev]) # training labels\n",
    "# Ydev_numeric = np.array([sent2num(y) for y in Ydev])\n",
    "\n",
    "# sparse implementation\n",
    "IDtrain = list(BOW_train.keys())            # list of IDs in train set\n",
    "IDdev = list(BOW_dev.keys())                # list of IDs in development (validation) set\n",
    "Xtrain = np.array([BOW_train[id] for id in IDtrain])     # training set\n",
    "Ytrain = np.array([sent_train[id] for id in IDtrain])    # training labels\n",
    "Ytrain_numeric = np.array([sent2num(y) for y in Ytrain]) # numerical labels\n",
    "Xdev = np.array([BOW_dev[id] for id in IDdev])  # development set\n",
    "Ydev = np.array([sent_dev[id] for id in IDdev]) # training labels\n",
    "Ydev_numeric = np.array([sent2num(y) for y in Ydev])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(Xtrain, Ytrain_numeric)\n",
    "y_scores = clf.predict_proba(Xdev)[:,1]\n",
    "y_pred = clf.predict(Xdev)\n",
    "pred_dict = dict(zip((IDdev, y_pred)))\n",
    "\n",
    "# performance metrics\n",
    "accuracy = np.mean(y_pred==Ydev_numeric)\n",
    "confusion(pred_dict, '../semeval-tweets/twitter-dev-data.txt', classifier=\"KNN\")\n",
    "evaluate(pred_dict, '../semeval-tweets/twitter-dev-data.txt', classifier=\"KNN\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "[[0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "a = np.array([0,0,0,1,0,0,0])\n",
    "S = csr_matrix(a)\n",
    "print(S)\n",
    "\n",
    "B = np.array(S.todense())\n",
    "print(B)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayes Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear SVM classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Comparison of all classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "for classifier in ['NearestNeighbour', 'NaiveBayes','SVM']:\n",
    "    for features in ['BOW', '<feature-2-name>']:\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if classifier == 'NearestNeighbour':\n",
    "            # write the svm classifier here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'NaiveBayes':\n",
    "            # write the classifier 2 here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'SVM':\n",
    "            # write the classifier 3 here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'LSTM':\n",
    "            # write the LSTM classifier here\n",
    "            if features == 'bow':\n",
    "                continue\n",
    "            print('Training ' + classifier)\n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "\n",
    "        # Predition performance of the classifiers\n",
    "        for testset in testsets:\n",
    "            id_preds = {}\n",
    "            # write the prediction and evaluation code here\n",
    "\n",
    "            testset_name = testset\n",
    "            testset_path = join('semeval-tweets', testset_name)\n",
    "            evaluate(id_preds, testset_path, features + '-' + classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
