{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a *Jupyter notebook* for the coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, donâ€™t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n",
    "You may import more packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages here\n",
    "import re\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../semeval-tweets/twitter-test1.txt', '../semeval-tweets/twitter-test2.txt', '../semeval-tweets/twitter-test3.txt']\n"
     ]
    }
   ],
   "source": [
    "# Define test sets\n",
    "dataDir = '../semeval-tweets'\n",
    "testsetStrings = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']\n",
    "testsets = [join(dataDir, t) for t in testsetStrings]\n",
    "print(testsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght: 139\n",
      "102313285628711403\n",
      "neutral\n",
      "\"Bing one-ups knowledge graph, hires Encyclopaedia Britannica to supply results:   It may have retired from the cut-throat world of pr...\"\n",
      "\n",
      "Lenght: 139\n",
      "653274888624828198\n",
      "neutral\n",
      "\"On Thursday, concealed-carry gun license holders will be given a new right in the state of Oklahoma: the ability... http://t.co/oSgGHKi1\"\n",
      "\n",
      "Lenght: 137\n",
      "420747042670198316\n",
      "negative\n",
      "Miyagi just got banned from yoga. He was caught him sniffing the sphincter of the girl in front of him. There may be police involvement!\n",
      "\n",
      "Lenght: 135\n",
      "822064800445716046\n",
      "neutral\n",
      "Join us tonight at Boston Pizza - Centre on Barton for THURSDAY NIGHT FOOTBALL! Tonight the Chiefs take on the... http://t.co/iegTxPQv\n",
      "\n",
      "Lenght: 139\n",
      "055480020953212084\n",
      "neutral\n",
      "\"#FX NEW YORK, Oct 18 (Reuters) - The Federal Reserve provided $4.701 billion of liquidity to the ... http://t.co/BJhIQTtO #EUR #AUD #CAD\"\n",
      "\n",
      "Lenght: 132\n",
      "429443270273347255\n",
      "neutral\n",
      "\"13 April 1996, History is made, as the MetroStars and the Los Angeles Galaxy meet at the Rose Bowl in Pasadena, California, (1/3)\"\n",
      "\n",
      "Lenght: 135\n",
      "220323262844863802\n",
      "neutral\n",
      "Make sure you tune into The Saga exclusively on Temple's only student run radio WHIP at iHeartRadio Monday through Thursday's at 9pm!!\n",
      "\n",
      "Lenght: 135\n",
      "266953303729385574\n",
      "positive\n",
      "\"Saturday Nov 17th, 2012 Q Bar and Grill is the Place to be!  UFC 90's Event with Special Guests:  DJ BOBBY D... http://t.co/z96soXQf\"\n",
      "\n",
      "Lenght: 134\n",
      "267372990522222442\n",
      "negative\n",
      "@Waka_BacaFlame I'm already on my 3rd year at Colton it wouldn't make sense to graduate from gt after spending all this time here >.<\n",
      "\n",
      "Lenght: 131\n",
      "578446202147364926\n",
      "neutral\n",
      "\"Ephesians 6:11 (NEB) - Ephesians 6:11 (NEB) Put on all the armour which God provides, so that you may be... http://t.co/cEt5WKVK\"\n",
      "\n",
      "Lenght: 141\n",
      "810763059461202071\n",
      "neutral\n",
      "Andando in bicicletta a Roma -- si puo' dividere (share) pure li! Shareable: Changing Bike Culture in the Eternal City: http://t.co/nT3c7ryI\n",
      "\n",
      "Lenght: 132\n",
      "885726184364930528\n",
      "positive\n",
      ".@Q1047 Would luv if you could make an announcement about Chris Rene concert @theroxy this Thursday!! Thanks!! http://t.co/aKzopIrd\n",
      "\n",
      "Lenght: 141\n",
      "960689731242555262\n",
      "negative\n",
      "So now it seems Daav(Conspiracy) of Yadav have back fired...and if true/proved may create troubles for Akhilesh Yadav and Samajwadi Party...\n",
      "\n",
      "Lenght: 139\n",
      "491718215382409200\n",
      "neutral\n",
      "\"Charlie Rose with Desmond Tutu; Bill Joy (October 5, 1999): The Archbishop of Cape Town, South Africa and Nobel ... http://t.co/BVoS0dnd\"\n",
      "\n",
      "Lenght: 143\n",
      "317970942816831516\n",
      "neutral\n",
      "\"Remember folks. Grabbed 3 more earnings plays today. CELG,CME and HSY all calls for tomorrow!!! 5/5 for the week   $CME http://t.co/ZcmjGsha\"\n",
      "\n",
      "Lenght: 137\n",
      "281466922061596703\n",
      "neutral\n",
      "\"Deadly Israeli strike, fire from Gaza mar truce: A deadly Israeli airstrike into the Gaza Strip and rocket and... http://t.co/E0h27Mcg\"\n",
      "\n",
      "Lenght: 137\n",
      "076653802834383376\n",
      "positive\n",
      "PST Keane delivers best MLS goal from Saturday: The Los Angeles Galaxy were on top of things early tonight again... http://t.co/aVYSaVOo\n",
      "\n",
      "Lenght: 142\n",
      "629377153435614908\n",
      "negative\n",
      "\"Heather is like the serpent in the garden trying 2 entice info from McBain! Michael Easton, dont ever go away that long! Watching Tues #GH.\"\n",
      "\n",
      "Lenght: 142\n",
      "255090901012423499\n",
      "neutral\n",
      "\"Southampton's Richard Bland is -9 after his 2nd round in the Madeira Islands Open on the European Tour. Currently tied 4th, 3 off the lead.\"\n",
      "\n",
      "Lenght: 135\n",
      "581663993931072193\n",
      "neutral\n",
      "\"Overexposed Tour, Maroon 5 With The Cab: Through The Eyes Of A Drummer - As you may have heard, The Cab just... http://t.co/H3c7kQsw\"\n",
      "\n",
      "Lenght: 136\n",
      "854756032273426749\n",
      "neutral\n",
      "Remember to join the Springboks at 3pm tomorrow at Orlando Stadium to watch their open training session as they... http://t.co/vaCN6EZ0\n",
      "\n",
      "Lenght: 137\n",
      "385766324957619690\n",
      "positive\n",
      "@MichelleLMyers #BreakingDawnPart2 The Saga may end but the love and friendships will last 4ever #LoveIsLouderthanRollingTheFinalCredits\n",
      "\n",
      "Lenght: 134\n",
      "844654517192428026\n",
      "positive\n",
      "\"Happy 102 years to L.A. artist Tyrus Wong! May the road rise to meet you, may the wind be always at your back: http://t.co/e8SB5S90\"\n",
      "\n",
      "Lenght: 137\n",
      "233928065818229615\n",
      "neutral\n",
      "Federal Reserve stands firm on bond-buying program: The Federal Reserve said on Wednesday that it is s... http://t.co/KnRMVgLu #business\n",
      "\n",
      "Lenght: 139\n",
      "226170643240610310\n",
      "positive\n",
      "\"Bersani, supports stability agreement to secure government: (AGI) Rome, October 24 - Tomorrow the agreement on t... http://t.co/ew0JWQgD\"\n",
      "\n",
      "Lenght: 141\n",
      "282416648112742867\n",
      "neutral\n",
      "\"About to watch Kony 2012 video. Despite 100m views, there are quite a number here watching for 1st time. Interested in the reactions.#yuga\"\n",
      "\n",
      "Lenght: 139\n",
      "790411654414429445\n",
      "neutral\n",
      "\"LG may update the Optimus 2X to ICS after all, releases source code: The saga of the elusive ICS update for the ... http://t.co/g0hT0NtX\"\n",
      "\n",
      "Lenght: 136\n",
      "712768996713302192\n",
      "positive\n",
      "Saturday night Adult Swim (Cowboy Bebop + Ghost in the Shell) reminds me how much I adore Yoko Kanno's music. Such an amazing composer.\n",
      "\n",
      "Lenght: 136\n",
      "170040141580222961\n",
      "neutral\n",
      "Just one of the songs to expect during THE FRAY's concert in Smart Araneta Coliseum on November 10. This is off... http://t.co/BUVMOlcM\n",
      "\n",
      "Lenght: 139\n",
      "836541798391946839\n",
      "neutral\n",
      "\"Seven Penny Stocks on the Move with Heavy Volume, April 24: CSOC shares have traded as high as $.18 over the pas... http://t.co/OX7oCx72\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the structure of the dataset\n",
    "with open(testsets[0], 'r', encoding='utf8') as f1:\n",
    "    i = 0\n",
    "    for line in f1:\n",
    "        fields = line.split('\\t')\n",
    "        if i < 30:\n",
    "            length = len(fields[2])\n",
    "            if length > 130:\n",
    "                print(f\"Lenght: {length}\")\n",
    "                print(fields[0])  # 1st column - tweet ID\n",
    "                print(fields[1])  # 2nd column - tweet sentiment\n",
    "                print(fields[2])  # 3rd column - tweet text\n",
    "                i += 1\n",
    "\n",
    "# preprocessing questions and notes: \n",
    "  # -> what about removing the @usernames, is it advisable?\n",
    "  # -> need to remove URLs!\n",
    "  # -> There is a lot of noise/mistakes in the data and absence of interpunction.\n",
    "  # -> what about adding of the starting token?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    reading the testset and return a dictionary with: ID -> sentiment\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}  # init the dictionary\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)\n",
    "    return semevalmacrof1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            positive  negative  neutral\n",
      "positive    1.000     0.000     0.000     \n",
      "negative    0.000     1.000     0.000     \n",
      "neutral     0.000     0.000     1.000     \n",
      "\n",
      "../semeval-tweets/twitter-test1.txt (PerfectClassifier): 1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the evaluation functions\n",
    "tweetDict = read_test('../semeval-tweets/twitter-test1.txt')\n",
    "confusion(tweetDict, '../semeval-tweets/twitter-test1.txt', \"PerfectClassifier\")\n",
    "evaluate(tweetDict, '../semeval-tweets/twitter-test1.txt', \"PerfectClassifier\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, dev set and testing set\n",
    "\n",
    "dataDir = '../semeval-tweets'  # change to the proper directory\n",
    "datasetStrings = ['twitter-training-data.txt', 'twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt', 'twitter-dev-data.txt']\n",
    "datasets = [join(dataDir, t) for t in datasetStrings]\n",
    "\n",
    "tweet_IDs = {}          # init dictionary with tweet IDs\n",
    "tweet_sentiments = {}   # init dictionary with sentiments\n",
    "tweet_texts = {}        # init dictionary with tweet texts\n",
    "\n",
    "for DatasetString in datasets:\n",
    "    data_ID, data_sent, data_text  = {}, {}, {}    # temp dictionaries\n",
    "    with open(DatasetString, 'r', encoding='utf8') as f1:\n",
    "        for i, line in enumerate(f1):\n",
    "            fields = line.split('\\t')\n",
    "            data_ID[i] = fields[0]            # tweet IDs\n",
    "            data_sent[fields[0]] = fields[1]  # sentiments\n",
    "            data_text[fields[0]] = fields[2]  # tweet text\n",
    "    tweet_IDs[DatasetString] = data_ID\n",
    "    tweet_sentiments[DatasetString] = data_sent\n",
    "    tweet_texts[DatasetString] = data_text\n",
    "\n",
    "# sentiment dictionaries\n",
    "sent_train = tweet_sentiments[datasets[0]]\n",
    "sent_test1 = tweet_sentiments[datasets[1]]\n",
    "sent_test2 = tweet_sentiments[datasets[2]]\n",
    "sent_test3 = tweet_sentiments[datasets[3]]\n",
    "sent_dev = tweet_sentiments[datasets[4]]\n",
    "\n",
    "# tweet text dictionaries\n",
    "text_train = tweet_texts[datasets[0]]\n",
    "text_test1 = tweet_texts[datasets[1]]\n",
    "text_test2 = tweet_texts[datasets[2]]\n",
    "text_test3 = tweet_texts[datasets[3]]\n",
    "text_dev = tweet_texts[datasets[4]]\n",
    "\n",
    "# tweet IDs dictionaries\n",
    "IDs_train = tweet_IDs[datasets[0]]\n",
    "IDs_test1 = tweet_IDs[datasets[1]]\n",
    "IDs_test2 = tweet_IDs[datasets[2]]\n",
    "IDs_test3 = tweet_IDs[datasets[3]]\n",
    "IDs_dev = tweet_IDs[datasets[4]]\n",
    "\n",
    "\n",
    "## examples and tests\n",
    "# id = IDs_train[0]\n",
    "# id_dev = IDs_dev[0]\n",
    "# id1 = IDs_test1[0]\n",
    "# id2 = IDs_test2[0]\n",
    "# id3 = IDs_test3[0]\n",
    "# print(f\"-ID:{id} \\n-TEXT:{text_train[id]}-SENTIMENT: {sent_train[id]}\\n\")\n",
    "# print(f\"-ID:{id_dev} \\n-TEXT:{text_dev[id_dev]}-SENTIMENT: {sent_dev[id_dev]}\\n\")\n",
    "# print(f\"-ID:{id1} \\n-TEXT:{text_test1[id1]}-SENTIMENT: {sent_test1[id1]}\\n\")\n",
    "# print(f\"-ID:{id2} \\n-TEXT:{text_test2[id2]}-SENTIMENT: {sent_test2[id2]}\\n\")\n",
    "# print(f\"-ID:{id3} \\n-TEXT:{text_test3[id3]}-SENTIMENT: {sent_test3[id3]}\\n\")\n",
    "# print(len(IDs_train.keys()), len(text_train.keys()), len(sent_train.keys()))  # 45101\n",
    "# print(len(IDs_test1.keys()), len(text_test1.keys()), len(sent_test1.keys()))  # 3531\n",
    "# print(len(IDs_test2.keys()), len(text_test2.keys()), len(sent_test2.keys()))  # 1853\n",
    "# print(len(IDs_test3.keys()), len(text_test3.keys()), len(sent_test3.keys()))  # 2379\n",
    "# print(len(IDs_dev.keys()), len(text_dev.keys()), len(sent_dev.keys()))        # 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "#### Order of preprocessing\n",
    "* lowercase text\n",
    "* regex cleaning\n",
    "   * Remove URLs\n",
    "   * Remove non-alphanumeric characters (leave hashtags and usernames)\n",
    "   * Remove numbers that are fully made of digits\n",
    "   * (Remove words with only 1 character)\n",
    "\n",
    " #### Preprocessing questions and notes:\n",
    "   -> what about removing the @usernames, is it advisable?\n",
    "   -> need to remove URLs!\n",
    "   -> There is a lot of noise/mistakes in the data and absence of interpunction.\n",
    "   -> what about adding of the starting token?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# loading preprocessed datasets\n",
    "#file_to_load = \"preprocessing-plain.pkl\"  # \"preprocessing-glove.pkl\"\n",
    "file_to_load = \"preprocessing-glove.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(file_to_load, 'rb') as inp_file:\n",
    "        temp_dicts = pickle.load(inp_file)\n",
    "        txt_dicts = temp_dicts[0:5]\n",
    "        txtlist_dicts = temp_dicts[5:]\n",
    "\n",
    "else:\n",
    "    top100 = ['com', 'net', 'org', 'jp', 'de', 'uk', 'fr', 'br', 'it', 'ru', 'es', 'me', 'gov', 'pl', 'ca', 'au', 'cn', 'co', 'in', 'nl', 'edu', 'info', 'eu', 'ch', 'id', 'at', 'kr', 'cz', 'mx', 'be', 'tv', 'se', 'tr', 'tw', 'al', 'ua', 'ir', 'vn', 'cl', 'sk', 'ly', 'cc', 'to', 'no', 'fi', 'us', 'pt', 'dk', 'ar', 'hu', 'tk', 'gr', 'il', 'news', 'ro', 'my', 'biz', 'ie', 'za', 'nz', 'sg', 'ee', 'th', 'io', 'xyz', 'pe', 'bg', 'hk', 'rs', 'lt', 'link', 'ph', 'club', 'si', 'site', 'mobi', 'by', 'cat', 'wiki', 'la', 'ga', 'xxx', 'cf', 'hr', 'ng', 'jobs', 'online', 'kz', 'ug', 'gq', 'ae', 'is', 'lv', 'pro', 'fm', 'tips', 'ms', 'sa', 'app', 'lat']\n",
    "\n",
    "    glove_emoticons = [';)', '=)', ':]', ':3', ':(', ':-)', '0:3', ':@', ':)', ':|', '=p']\n",
    "    glove_emoticon_strings = ['emoticon' + str(num) for num in range(len(emoticons))]\n",
    "    emoticon2string = dict(zip(glove_emoticons, glove_emoticon_strings))\n",
    "    string2emoticon = dict(zip(glove_emoticon_strings, glove_emoticons))\n",
    "\n",
    "    ID_dicts = [IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev]\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev]\n",
    "    txtlist_dicts = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  # init the lemmatizer\n",
    "    POSconvert = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "\n",
    "    for i, IDdict in enumerate(ID_dicts):\n",
    "        output = txt_dicts[i]\n",
    "        output_txt = {}\n",
    "        for id in IDdict.values():\n",
    "            text = output[id].lower()\n",
    "\n",
    "            # replace/delete all URLs starting with 'http' and 'www'\n",
    "            new_text = re.sub(\"http[^\\s]*\", '', text)\n",
    "            new_text = re.sub(\"www[^\\s]*\", '', new_text)\n",
    "\n",
    "            # delete all URLs which have one of 100 most common extensions ('.com', '.net', ...)\n",
    "            for ext in top100:\n",
    "                re_string = \"[^\\s]*\\.\" + ext + \"[^\\s]*\"\n",
    "                new_text = re.sub(re_string, '', new_text)\n",
    "\n",
    "            #replace all emoticons with an emoticon string:  #emoticon42\n",
    "            for em in glove_emoticons:\n",
    "                re_string = '\\s' + re.escape(em) + '\\s'\n",
    "                replace_string = ' ' + emoticon2string[em] + ' '\n",
    "                new_text = re.sub(re_string, replace_string, new_text)\n",
    "\n",
    "            # removing '&amp'\n",
    "            new_text = re.sub('&amp','', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars except for '# and @'\n",
    "            new_text = re.sub('[^\\w\\s@#]','', new_text)\n",
    "\n",
    "            # replace all @usernames with 'username'\n",
    "            new_text = re.sub('\\s@[^\\s]+',' username', new_text)  # middle\n",
    "            new_text = re.sub('^@[^\\s]+','username', new_text)    # start\n",
    "\n",
    "            # remove strings with '#' not on the beginning (to keep only hashtags)\n",
    "            new_text = re.sub('\\s[\\w]+#[\\w]*','', new_text)\n",
    "\n",
    "            # replace #hashtags with 'hashtag' and '#hashtag1 #hashtag2' with 'hashtags'\n",
    "            new_text = re.sub('#[^\\s]*\\s',' hashtag ', new_text)\n",
    "            new_text = re.sub('\\s#[^\\s]*$',' hashtag ', new_text)\n",
    "            new_text = re.sub('(\\s+hashtag){2,}', ' hashtags', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars\n",
    "            new_text = re.sub('[^\\w\\s]','', new_text)\n",
    "\n",
    "            # numbers fully made of digits\n",
    "            new_text = re.sub('\\s[\\d]+\\s','', new_text)\n",
    "\n",
    "            # remove words with only 1 character\n",
    "            new_text = re.sub('\\\\b\\\\w{1}\\\\b','', new_text)\n",
    "\n",
    "            # remove newline chars\n",
    "            new_text = new_text.replace('\\n', ' ')\n",
    "\n",
    "            # replace a multiple spaces with a single space\n",
    "            new_text = re.sub('\\s+',' ', new_text)\n",
    "\n",
    "            # using the lemmatizer\n",
    "            txt_list = nltk.word_tokenize(new_text)\n",
    "            for k, word in enumerate(txt_list):  # fixing the separation of hashtags by the tokenizer\n",
    "                if word == '#' or word == '@':\n",
    "                    if k < len(txt_list) - 1:\n",
    "                        txt_list[k] = txt_list[k] + txt_list[k+1]\n",
    "                        txt_list.pop(k+1)\n",
    "            POS = nltk.pos_tag(txt_list)                  # POS tags from nltk\n",
    "            WordNetPOS = [POSconvert(P[1]) for P in POS]  # POS tags for lemmatizer\n",
    "            for j in range(len(txt_list)):\n",
    "                word = txt_list[j]\n",
    "                lemmatized = lemmatizer.lemmatize(word, WordNetPOS[j])  # process each token/word one by one\n",
    "                if lemmatized in glove_emoticon_strings:\n",
    "                    lemmatized = string2emoticon[lemmatized]\n",
    "                txt_list[j] = lemmatized  # update the word in the txt_list\n",
    "\n",
    "            ## UPDATE the dictionary\n",
    "            output_txt[id] = ' '.join(txt_list)\n",
    "            output[id] = txt_list\n",
    "\n",
    "        txt_dicts[i] = output_txt\n",
    "        txtlist_dicts.append(output)\n",
    "\n",
    "text_train = txt_dicts[0]\n",
    "text_test1 = txt_dicts[1]\n",
    "text_test2 = txt_dicts[2]\n",
    "text_test3 = txt_dicts[3]\n",
    "text_dev = txt_dicts[4]\n",
    "txtlist_train = txtlist_dicts[0]\n",
    "txtlist_test1 = txtlist_dicts[1]\n",
    "txtlist_test2 = txtlist_dicts[2]\n",
    "txtlist_test3 = txtlist_dicts[3]\n",
    "txtlist_dev = txtlist_dicts[4]\n",
    "\n",
    "# saving preprocessing.pkl\n",
    "file_to_save = \"preprocessing\" + file_to_load[13:]\n",
    "if not os.path.isfile(file_to_save):\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev, txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev]\n",
    "    with open(file_to_save, 'wb') as out_file:\n",
    "        pickle.dump(txt_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rip frank gifford monday night football have never be the same without you #turnoutthelightsthepartysover\n",
      "today the last two wisconsin recall for now keep up the extraordinary work wisconsin\n",
      "@goldberg do you agree with me that spear christian be your 2nd best spear after to that guy you do spear to in royal rumble\n",
      "trump aim his sharpest tweet at jeb bush but with ben carson surge in 2nd place be trump miss the target @saramurray have more\n",
      "#putin have have it with #obama and have decide to throw #america out of #syria negotiation\n",
      "remember stand on suncorpmins b4 1st lion test @seanny202 say this be rugby match it mate #marlinswallabies\n",
      "the government need to start listening and stop do damage to uk higher education with negative massage on immi\n",
      "@nidge1uk yer off to take son for his first day at big school then it google analytics train for me how be your night\n",
      "my bad let me fixate yall may be muslim but youre not practice muslim\n",
      "use of htc vive virtual realitymy experience with google earth vr\n",
      "nigeria coach sunday oliseh have reveal that he crave the success he have be able to achieve a player with the super eagle\n",
      "when you make opponent publicly apologizewell #youreafacist #mangomussolini #democrats #calexit #humanrights\n",
      "@peteabe hold out hope for valentine he may win manager of the year\n",
      "why be david wright batting in the cleanup spot in his 1st game back that be why upper deck home run on the 3rd pitch #mets #davidwright\n",
      "red sox highlight jackie bradley jr hit solo hr david ortiz with rbi double in 4run 7th inwin over phillies espn\n",
      "@thevampsph @byerope may please have the picture of the seatplan for the vamp concert please\n",
      "my love for maya rudolph may be borderline obsessive just looooove her\n",
      "want in on free trial of #fit4twos mom baby yoga class at the centre on septembercheck it out\n",
      "@utahjazz thats so awesome guy would have love to be volunteer for this much love and happy thanksgiving\n",
      "that minister may influence how you see christian but please dont let his blog be the place you learn about christianity\n",
      "drank so much beer yesterday that somehow bypass drunk end up completely sober blame david blaine\n",
      "tomorrow be national dog day not to be confuse with national hot dog day though if you live in the south lot of heat it be the same\n",
      "@hughhewitt @realdonaldtrump d have absolutely lose their @# hack @elizabethforma @repcummings call for trump to be investigate\n",
      "film independent spirit award attendees react to time recent study of the membership of the motion picture academy\n",
      "cant overstate the importance of kris bryants walkoff hr after be sweep over the weekend and blow 3r lead in the 9th tonight\n",
      "@thehill will someone think of michael moore right now prayers to @mmflint he must be inconsolable\n",
      "you do have valentine last year tho but thats cool too\n",
      "rip former nfl star and amaze monday night football commentator frank gifford thanks for the memory\n",
      "of all thing espns resident style guru pick it my vest im wear tony romo sock\n",
      "@rbohlender will hear u2 when pas through the pearly gate too bad im not sunday school teacher id teach that\n",
      "5th round send me picture of your favorite artist first topoints get dm with niall #otramontreal\n",
      "yep and that might my move in dallas tonight rt @antwannettebond true tht would fun @breedlove_08\n",
      "@calila1988 serena look like she want to cry majority of the 3rd set\n",
      "the spanish national manager vicente del bosque believe sunday el clasico set the perfect example\n",
      "dragon ball tomorrow but dont know if want to wake up early to see it at when can see f4ntastic or antman at later time\n",
      "de gea will still be real madrid player tomorrow it real madrid they bend the rule the spanish fa will bend over for them\n",
      "donald trump cabinet be on track to be the least experienced in modern history via @huffpostpol #stillwithher\n",
      "gigi hadid be feel the burn of backlash\n",
      "really good day a danica patrick finish 10th at kansa nationwide race nascar news fox sport on msn\n",
      "#gotopless tomorrow afternoon in venice beach come one come all and let make history\n",
      "anyone wan na go to jason aldean with me saturday\n",
      "@rossoneriblog think he probably need to be on plane early tomorrow morning for it to happen milan always leak everything\n",
      "@bluerule24 @brasspetal kinda me on tuesday lol be numb and do with hannibal it be scary hop back on the feel train on wed tho\n",
      "indirects free bc teen wolf come back in january nd want some new mutuals song\n",
      "dont forget tomorrow be the annual general body meeting come out to learn more about isat and the organization we be affiliate with\n",
      "my mom under the hilarious impression that im spending my saturday load unloading furniture from trailer in natchitoches\n",
      "kris bryant breaksup no hitter in the 5th by hit baseballfeet off the left field video board #nbd #cubs\n",
      "@brandy_la @swanofqueens go in february there be whole large group planning to go around valentine day\n",
      "#what be all the different sex position ashley graham sex fuck sexy mom\n",
      "friday morning special guest live in studio drake white just get the opening slot on the zac brown band tour\n",
      "saturday night be at the barack shrine in monroe #teamlcf go be in full affect #gsu #jsu #ull #ulm #latech #letzgetit @lacollegefest\n",
      "and if it be sunday werent enough to make you happy hip hip hooray for national ice cream day follow u all\n",
      "need 3rd game to record any suggestion guy at the moment have until dawn and minecraft\n",
      "haruna lukmon may av just play himself out of super eagle under coach sunday oliseh\n",
      "please believe on sept 9th will be in the theater watch kevin harts new movie\n",
      "obama be the greatest potus so sad trump will always be picture beside him in history trump isnt worthy\n",
      "gon na record me and ciarans mash up of fleetwood mac song tomorrow ha so bad but it fun\n",
      "like that bellucci be make murray work for it in this 3rd set #usopen @usopen @espntennis\n",
      "it may only be the round ofbut holy cow dont tell that anderson and murray incredible match 3hrs and 40min so far #usopen2015\n",
      "kickin the friday off with gem of jam disclosure ft sam smith omen this be gon be classic\n",
      "@busyphilipps25 wow busy just saw the promo on tb for cougar town in jan you be absolutely vision an angel in white beautiful\n",
      "if you desperately want to get your hand on galaxy noteand be in europe this may be the petition for you\n",
      "we be in search for trade sale manager in muscat oman if interested you may apply @\n",
      "@dougbenson leonard martin category saturday not live movie with chris farley phil hartman or john belushi\n",
      "time may be run out for auburns benton knew wasnt run it full speed effort isnt predetermine\n",
      "it official im go to the justin bieber concert tomorrow\n",
      "magic mike xxl may be my favorite movie of the summer be smile and laugh the entire time\n",
      "@armandosalguero next that moron will be call george soros dear leader leave this awful country colin you freakin idiot\n",
      "come join u for friday night nirvana @red square party center im there play fromtocome hang with u\n",
      "16 useful feature on facebook google twitter and linkedin you may have miss\n",
      "emergency state declare in #haifa #hamas praise for more fire heavy retaliation and scalation to full openwar with #gaza expect\n",
      "happy thursday the last day to register to win luke bryan ticketsand zac brown band win em\n",
      "@jimmyfallon 4kirkmiss pattysookiemr gilmore #gilmoregirlstop4\n",
      "beyond frustrate with my #xbox360 right now and that a of june @microsoft doesnt support it get ta find someone else to fix the drive\n",
      "@kensingtonroyal happy 2nd birthday prince george hope you have fun day\n",
      "@timwattsmp @grahamperrettmp thats rich come from labour after the medicare lie you shower of con men lie to the elderly for shame\n",
      "this mess be come towards u tomorrow spc have slight risk out for tomorrow and well know more about what it\n",
      "it really shameful that @sitaramyechury part of indian liberal be glorify such brutal regime\n",
      "ok so hulk hogan may have take it bite too far this time\n",
      "siri how do david blaine stab an ice pick through his hand without any lasting nerve damage\n",
      "go to watch connor run the 4x100m relay in state athletics championship at olympic park tomorrow wish him luck #runforrestrun\n",
      "tgif everyone be so happy it friday\n",
      "still wait for someone anyone to show me where fluke ask the gov to pay for her anything tap foot check watch cmon\n",
      "@asthefairiesare yeah want to go see it and tell them go for it pass paper town be play but id rather read the book 1st\n",
      "friday night prombley just gon na watch jim henson memorial on youtube or something\n",
      "grayson allen suck so bad his name isnt even worth capitalize\n",
      "do you know that josh hamilton be trend topic on fridayforhours in dallasft worth #trndnl\n",
      "wednesday one minute warm up chart of thepunts last wknd pat ower robert kraft get his own nike shoe\n",
      "replay wwe july if be jj security would never try to mess with brock lesnar he cool omg\n",
      "cant watch monday night raw anymore miss the stone cold steve austin ric flair shawn michael eddie guerrero the hardy brother\n",
      "stream come tomorrow stream minecraft and madden come check it out\n",
      "oklahoma have get decent sized earthquake every month since october yall need to stop fracking lol\n",
      "martin must be dread it it make or break for him tomorrow it the nature of the beast\n",
      "shit tasha may think ghost kill shawn because he tell her that when shawn try kill him he tell him get out of town\n",
      "@tgawd____ trash ol theyve look excellent so far trash rbsjstew run foryards in the finalgames last season 2nd only to murray\n",
      "pretty little liar just blow my mind #atrain be everything id hop for be it january yet\n",
      "check out this song on sutros via @sutros\n",
      "tripple be never ric flair bitchbut this sunday ima make you mine john cena to seth rollins\n",
      "josh hamilton the best 1st pitch player in baseball get slow hang curveball down the middle just look at it lol\n",
      "@stephaniepratt make in reality order from amazon prime ill have it tomorrow cant wait excite timesxxxx\n"
     ]
    }
   ],
   "source": [
    "# checking the preprocessed output\n",
    "for id in list(IDs_train.values())[200:300]:\n",
    "    print(text_train[id])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: Bag of words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sparse processing.\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words - my implementation:\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords as Stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# loading preprocessed datasets\n",
    "#file_to_load = \"BOWsparse-plain.pkl\"  # \"BOWsparse-glove.pkl\"\n",
    "file_to_load =  \"BOWsparse-glove.pkl\"\n",
    "if os.path.isfile(file_to_load):\n",
    "    with open(file_to_load, 'rb') as inp_file:\n",
    "        temp = pickle.load(inp_file)\n",
    "        [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev] = temp\n",
    "else:\n",
    "    # 1) removing stop words\n",
    "    stopwords = Stopwords.words('english')\n",
    "    stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "    # 2) extracting the dictionary/vocabulary\n",
    "    freq = FreqDist()   # frequency distribution\n",
    "    for Dict in txtlist_dicts:\n",
    "        for tweet in Dict.values():\n",
    "            for word in tweet:\n",
    "                if not word in stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "    nums = range(1, len(freq.keys())+1)\n",
    "    vocabulary = list(freq.keys())              # creating the dictionary\n",
    "    vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out of vocabulary' word\n",
    "    vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "    vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words -> len: 69742\n",
    "\n",
    "    # auxiliary ftion which takes list of words and returns its BoW representation as np.array\n",
    "    def text2BOW(text_list, vocabulary, stopwords):\n",
    "        BOW_vec = np.zeros(len(vocabulary) + 1)\n",
    "        for word in text_list:\n",
    "            if not word in stopwords:\n",
    "                if word in vocabulary:\n",
    "                    BOW_vec[vocab2num[word]] += 1\n",
    "                else:\n",
    "                    BOW_vec[vocab2num['<OOV>']] += 1\n",
    "        return BOW_vec\n",
    "\n",
    "    # if os.path.isfile(\"BOWs.pkl\"):  # loading preprocessed datasets\n",
    "    #     with open('BOWs.pkl', 'rb') as inp_file:\n",
    "    #         ll = pickle.load(inp_file)\n",
    "    #         BOW_train, BOW_test1, BOW_test2, BOW_test3, BOW_dev = ll[0], ll[1], ll[2], ll[3], ll[4]\n",
    "    #\n",
    "    # else:\n",
    "    # Bag of Words (BOW) for each tweet\n",
    "    BOW_train = {}\n",
    "    for ID, tweet in txtlist_train.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "        BOW_train[ID] = BOW\n",
    "\n",
    "    BOW_test1 = {}\n",
    "    for ID, tweet in txtlist_test1.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "        BOW_test1[ID] = BOW\n",
    "\n",
    "    BOW_test2 = {}\n",
    "    for ID, tweet in txtlist_test2.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "        BOW_test2[ID] = BOW\n",
    "\n",
    "    BOW_test3 = {}\n",
    "    for ID, tweet in txtlist_test3.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "        BOW_test3[ID] = BOW\n",
    "\n",
    "    BOW_dev = {}\n",
    "    for ID, tweet in txtlist_dev.items():\n",
    "        BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "        BOW_dev[ID] = BOW\n",
    "\n",
    "\n",
    "    # saving BOWs.pkl: very large file - maybe not the best idea to save it?\n",
    "        # if not os.path.isfile(\"BOWs.pkl\"):\n",
    "        #     BOW_dicts = [BOW_train, BOW_test1, BOW_test2, BOW_test3, BOW_dev]\n",
    "        #     with open(\"BOWs.pkl\", 'wb') as out_file:\n",
    "        #         pickle.dump(BOW_dicts, out_file, protocol=-1)\n",
    "\n",
    "    print(\"Starting sparse processing.\")\n",
    "\n",
    "    # sparse representation\n",
    "    vector_list = [BOW_train[id] for id in BOW_train.keys()]\n",
    "    dense_train = np.vstack(vector_list)    # shape (45101, 59559)\n",
    "    sparse_train = csr_matrix(dense_train)  # getting the sparse matrix\n",
    "\n",
    "    vector_list = [BOW_test1[id] for id in BOW_test1.keys()]\n",
    "    dense_test1 = np.vstack(vector_list)    # shape (3531, 59559)\n",
    "    sparse_test1 = csr_matrix(dense_test1)  # getting the sparse matrix\n",
    "\n",
    "    vector_list = [BOW_test2[id] for id in BOW_test2.keys()]\n",
    "    dense_test2 = np.vstack(vector_list)    # shape (1853, 59559)\n",
    "    sparse_test2 = csr_matrix(dense_test2)  # getting the sparse matrix\n",
    "\n",
    "    vector_list = [BOW_test3[id] for id in BOW_test3.keys()]\n",
    "    dense_test3 = np.vstack(vector_list)    # shape (2379, 59559)\n",
    "    sparse_test3 = csr_matrix(dense_test3)  # getting the sparse matrix\n",
    "\n",
    "    vector_list = [BOW_dev[id] for id in BOW_dev.keys()]\n",
    "    dense_dev = np.vstack(vector_list)      # shape (2000, 59559)\n",
    "    sparse_dev = csr_matrix(dense_dev)      # getting the sparse matrix\n",
    "\n",
    "    # train + dev together (combined)\n",
    "    vector_list1 = [BOW_train[id] for id in BOW_train.keys()]\n",
    "    vector_list2 = [BOW_dev[id] for id in BOW_dev.keys()]\n",
    "    temp1 = np.vstack(vector_list1)\n",
    "    temp2 = np.vstack(vector_list2)\n",
    "    dense_train_dev = np.vstack((temp1, temp2))  # shape (45101, 59559)\n",
    "    sparse_train_dev = csr_matrix(dense_train_dev)  # getting the sparse matrix\n",
    "\n",
    "\n",
    "# save the sparse representation\n",
    "file_to_save = \"BOWsparse\" + file_to_load[9:]\n",
    "if not os.path.isfile(file_to_save):\n",
    "    sparse_dicts = [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev]\n",
    "    with open(file_to_save, 'wb') as out_file:\n",
    "        pickle.dump(sparse_dicts, out_file, protocol=-1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"BOWsparse-plain.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev] = temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: TF-IDF weighted Bag of words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords as Stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# stop words\n",
    "stopwords = Stopwords.words('english')\n",
    "stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "# extracting the dictionary\n",
    "freq = FreqDist()   # frequency distribution\n",
    "for Dict in txtlist_dicts:\n",
    "    for tweet in Dict.values():\n",
    "        for word in tweet:\n",
    "            if not word in stopwords:\n",
    "                freq[word] += 1\n",
    "\n",
    "nums = range(1,len(freq.keys())+1)\n",
    "vocabulary = list(freq.keys())              # creating the dictionary\n",
    "vocabulary_array = np.array(vocabulary)     # np.array of the dictionary\n",
    "vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out of vocabulary' word\n",
    "vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words -> len: 69742\n",
    "\n",
    "# extracting the dictionary\n",
    "DFfreq = FreqDist()   # document frequency distribution\n",
    "Ntexts = len(IDs_train) + len(IDs_test1) + len(IDs_test2) + len(IDs_test3) + len(IDs_dev)\n",
    "for Dict in txtlist_dicts:\n",
    "    for tweet in Dict.values():\n",
    "        for word in np.unique(tweet):\n",
    "            if not word in stopwords:\n",
    "                DFfreq[word] += 1\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its TFIDF representation as np.array\n",
    "def text2TFIDF(text_list, vocabulary, stopwords, Ntexts):\n",
    "    TFIDF_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in np.unique(text_list):\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / DFfreq[word])\n",
    "                TFIDF_vec[vocab2num[word]] = tf * idf\n",
    "            else:\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / 0.000001 )\n",
    "                TFIDF_vec[vocab2num['<OOV>']] = tf * idf\n",
    "    return TFIDF_vec\n",
    "\n",
    "\n",
    "# TFIDF-weighted Bag of Words for each tweet\n",
    "TFIDF_train = {}\n",
    "for ID, tweet in txtlist_train.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_train[ID] = tfidf\n",
    "\n",
    "TFIDF_test1 = {}\n",
    "for ID, tweet in txtlist_test1.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_test1[ID] = tfidf\n",
    "\n",
    "TFIDF_test2 = {}\n",
    "for ID, tweet in txtlist_test2.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_test2[ID] = tfidf\n",
    "\n",
    "TFIDF_test3 = {}\n",
    "for ID, tweet in txtlist_test3.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_test3[ID] = tfidf\n",
    "\n",
    "TFIDF_dev = {}\n",
    "for ID, tweet in txtlist_dev.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_dev[ID] = tfidf\n",
    "\n",
    "print(\"Starting sparse processing.\")\n",
    "\n",
    "# sparse TFIDF representation\n",
    "vector_list = [TFIDF_train[id] for id in TFIDF_train.keys()]\n",
    "TFIDFdense_train = np.vstack(vector_list)\n",
    "TFIDFsparse_train = csr_matrix(TFIDFdense_train)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [TFIDF_test1[id] for id in TFIDF_test1.keys()]\n",
    "TFIDFdense_test1 = np.vstack(vector_list)\n",
    "TFIDFsparse_test1 = csr_matrix(TFIDFdense_test1)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [TFIDF_test2[id] for id in TFIDF_test2.keys()]\n",
    "TFIDFdense_test2 = np.vstack(vector_list)\n",
    "TFIDFsparse_test2 = csr_matrix(TFIDFdense_test2)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [TFIDF_test3[id] for id in TFIDF_test3.keys()]\n",
    "TFIDFdense_test3 = np.vstack(vector_list)\n",
    "TFIDFsparse_test3 = csr_matrix(TFIDFdense_test3)  # getting the sparse matrix\n",
    "\n",
    "vector_list = [TFIDF_dev[id] for id in TFIDF_dev.keys()]\n",
    "TFIDFdense_dev = np.vstack(vector_list)\n",
    "TFIDFsparse_dev = csr_matrix(TFIDFdense_dev)  # getting the sparse matrix\n",
    "\n",
    "vector_list1 = [TFIDF_train[id] for id in TFIDF_train.keys()]\n",
    "vector_list2 = [TFIDF_dev[id] for id in TFIDF_dev.keys()]\n",
    "temp1 = np.vstack(vector_list1)\n",
    "temp2 = np.vstack(vector_list2)\n",
    "TFIDFdense_train_dev = np.vstack((temp1, temp2))  # shape (45101, 59559)\n",
    "TFIDFsparse_train_dev = csr_matrix(TFIDFdense_train_dev)  # getting the sparse matrix\n",
    "\n",
    "# save the sparse representation\n",
    "sparse_dicts = [TFIDFsparse_train, TFIDFsparse_test1, TFIDFsparse_test2, TFIDFsparse_test3, TFIDFsparse_dev, TFIDFsparse_train_dev]\n",
    "with open(\"TFIDFsparse-plain.pkl\", 'wb') as out_file:\n",
    "    pickle.dump(sparse_dicts, out_file, protocol=-1)\n",
    "\n",
    "## loading preprocessed TFIDF sparse data\n",
    "# with open(\"TFIDFsparse.pkl\", 'rb') as inp_file:\n",
    "#     temp = pickle.load(inp_file)\n",
    "#     [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev] = temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# loading preprocessed TFIDF sparse data\n",
    "with open(\"TFIDFsparse.pkl\", 'rb') as inp_file:\n",
    "    temp = pickle.load(inp_file)\n",
    "    [sparse_train, sparse_test1, sparse_test2, sparse_test3, sparse_dev, sparse_train_dev] = temp\n",
    "#[BOW_train, BOW_test1,BOW_test2,BOW_test3,BOW_dev] = [TFIDF_train, TFIDF_test1,TFIDF_test2,TFIDF_test3,TFIDF_dev]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: GloVe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tups = {'word1': 100,'word2': 5,'word3': 13 }\n",
    "#print(sorted(tups, key=lambda data: data[1], reverse=True))\n",
    "\n",
    "Sorted = sorted([it for it in tups.items()], key=lambda data: data[1], reverse=True)\n",
    "print([ tup[0] for tup in Sorted ])\n",
    "print([ tup[1] for tup in Sorted ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords as Stopwords\n",
    "# Loading the word embeddings vectors from GloVE\n",
    "\n",
    "print('Extracting the word vectors.')\n",
    "\n",
    "txtlist_dicts = [txtlist_train, txtlist_dev]\n",
    "\n",
    "# 1) removing stop words\n",
    "stopwords = Stopwords.words('english')\n",
    "stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "# 2) extracting the dictionary/vocabulary\n",
    "freq = FreqDist()   # frequency distribution\n",
    "for Dict in txtlist_dicts:\n",
    "    for tweet in Dict.values():\n",
    "        for word in tweet:\n",
    "            if not word in stopwords:\n",
    "                freq[word] += 1\n",
    "\n",
    "full_embedding_dict = {}\n",
    "glove_path = join('..','glove', 'glove.6B.100d.txt')\n",
    "with open(glove_path, 'r', encoding='utf-8') as File:\n",
    "    for line in File:\n",
    "        vec = line.split()\n",
    "        word = vec[0]\n",
    "        coefs = np.asarray(vec[1:], dtype='float32')\n",
    "        full_embedding_dict[word] = coefs\n",
    "\n",
    "print(f\"Extracted {len(embeddings_dict)} word embedding vectors.\")\n",
    "\n",
    "sorted_vocabulary = sorted([it for it in freq.items()], key=lambda data: data[1], reverse=True)\n",
    "full_vocabulary = [ tup[0] for tup in sorted_vocabulary ]\n",
    "\n",
    "embedding_dict = {}  # word embeddings of 6000 words from vocabulary\n",
    "temp = 0\n",
    "for word in full_vocabulary:\n",
    "    if word in full_embedding_dict.keys():\n",
    "        embedding_dict[word] = full_embedding_dict[word]\n",
    "        temp += 1\n",
    "    if temp == 5998:\n",
    "        break\n",
    "vocabulary = list(embedding_dict.keys())  # obtain the dictionary of 6000 most common words\n",
    "\n",
    "print(f\"Created dictionary of {len(embedding_dict)} most common words.\")\n",
    "\n",
    "# extract the <OOV> vector by setting it to be the weighted avg of unused words\n",
    "Total = np.zeros(100)\n",
    "Sum = 0\n",
    "for word in full_vocabulary:\n",
    "    if word not in vocabulary:                              # if word is not among 6000 words\n",
    "        if word in full_embedding_dict.keys():              # and it is in glove\n",
    "            Total += freq[word] * full_embedding_dict[word] # take the weighted avg\n",
    "            Sum += freq[word]\n",
    "OOV_vector = Total / Sum\n",
    "embedding_dict['<OOV>'] = OOV_vector\n",
    "\n",
    "print(f\"The embedding dictionary has {len(embedding_dict)} words, the last one is: {list(embedding_dict.keys())[-1]}\")\n",
    "\n",
    "# Build an embedding matrix\n",
    "word_list = list(embedding_dict.keys())\n",
    "nums = range(1,len(word_list)+1)\n",
    "word2ID = dict(zip(word_list, nums))   # the index of the embedding vector\n",
    "vector_list = [embedding_dict[word] for word in word_list]\n",
    "\n",
    "embedding_matrix = np.vstack(vector_list)\n",
    "embedding_matrix = np.vstack((np.zeros(100), embedding_matrix))\n",
    "\n",
    "print(f\"Created matrix with shape {embedding_matrix.shape}\")  # the first row is a dummy row\n",
    "\n",
    "# save the embeddings\n",
    "with open(\"embeddings.pkl\", 'wb') as out_file:\n",
    "    temp = [embedding_matrix, word2ID, embedding_dict]\n",
    "    pickle.dump(temp, out_file, protocol=-1)\n",
    "\n",
    "## loading preprocessed embeddings (embedding matrix, word to index map, embedding dictionary)\n",
    "# with open(\"embeddings.pkl\", 'rb') as inp_file:\n",
    "#     temp = pickle.load(inp_file)\n",
    "#     [embedding_matrix, word2ID, embedding_dict] = temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nearest Neighbour Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grid search for KNN classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "params = {'n_neighbors': list(range(1,15)),\n",
    "          'metric': ['l1', 'l2', 'cosine'],\n",
    "          'weights': ['uniform', 'distance']}\n",
    "\n",
    "grid_clf = GridSearchCV(\n",
    "    estimator = KNeighborsClassifier(),\n",
    "    scoring = 'f1_macro',   # accuracy, balanced_accuracy, f1, roc_auc, average_precision (=pr_auc)\n",
    "    cv = 3,\n",
    "    param_grid = params)\n",
    "\n",
    "grid_clf.fit(Xtrain, Ytrain_numeric)\n",
    "print('Best params: ', grid_clf.best_params_)\n",
    "print(grid_clf.best_estimator_)\n",
    "print(classification_report(y_test1_numeric, grid_clf.predict(sparse_test1)))\n",
    "scores = abs(grid_clf.cv_results_['mean_test_score'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "def sent2num(sent):\n",
    "    if sent == 'negative':\n",
    "        return -1\n",
    "    if sent == 'neutral':\n",
    "        return 0\n",
    "    if sent == 'positive':\n",
    "        return 1\n",
    "\n",
    "def num2sent(num):\n",
    "    if num == -1:\n",
    "        return 'negative'\n",
    "    if num == 0:\n",
    "        return 'neutral'\n",
    "    if num == 1:\n",
    "        return 'positive'\n",
    "\n",
    "# BOW_train, BOW_dev -> non-sparse implementation\n",
    "# IDtrain = list(BOW_train.keys())            # list of IDs in train set\n",
    "# IDdev = list(BOW_dev.keys())                # list of IDs in development (validation) set\n",
    "# Xtrain = np.array([BOW_train[id] for id in IDtrain])     # training set\n",
    "# Ytrain = np.array([sent_train[id] for id in IDtrain])    # training labels\n",
    "# Ytrain_numeric = np.array([sent2num(y) for y in Ytrain]) # numerical labels\n",
    "# Xdev = np.array([BOW_dev[id] for id in IDdev])  # development set\n",
    "# Ydev = np.array([sent_dev[id] for id in IDdev]) # training labels\n",
    "# Ydev_numeric = np.array([sent2num(y) for y in Ydev])\n",
    "\n",
    "# sparse implementation\n",
    "Xtrain = sparse_train_dev  # combining the two datasets\n",
    "print(\"shape\", Xtrain.shape)###\n",
    "ID_train = list(BOW_train.keys())            # list of IDs in train set\n",
    "ID_dev = list(BOW_dev.keys())\n",
    "ID_train_dev = ID_train + ID_dev\n",
    "sn_train = [sent_train[id] for id in ID_train]  # training labels train\n",
    "sn_dev = [sent_dev[id] for id in ID_dev]        # training labels dev\n",
    "Ytrain = np.array(sn_train)            # combining both labels\n",
    "Ytrain = np.array(sn_train + sn_dev)###            # combining both labels\n",
    "Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])   # numerical labels\n",
    "\n",
    "t0 = time.time() # timing the run\n",
    "clf = KNeighborsClassifier(n_neighbors=9, metric='cosine', weights='uniform')\n",
    "clf.fit(Xtrain, Ytrain_numeric)\n",
    "# y_scores = clf.predict_proba(Xdev)[:,1]\n",
    "\n",
    "\n",
    "# evaluation test1\n",
    "ID_test1 = list(BOW_test1.keys())\n",
    "y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test1)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test1, y_pred))\n",
    "s1 = evaluate(pred_dict, '../semeval-tweets/twitter-test1.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test2\n",
    "ID_test2 = list(BOW_test2.keys())\n",
    "y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test2)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test2, y_pred))\n",
    "s2 = evaluate(pred_dict, '../semeval-tweets/twitter-test2.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test3\n",
    "ID_test3 = list(BOW_test3.keys())\n",
    "y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test3)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test3, y_pred))\n",
    "s3 = evaluate(pred_dict, '../semeval-tweets/twitter-test3.txt', classifier=\"KNN\")  # best score 0.435\n",
    "t1 = time.time()  # timing the run\n",
    "print('overall run time:', t1-t0)\n",
    "print(\"average F1 score:\", (s1 + s2 + s3)/3)\n",
    "\n",
    "\n",
    "# other performance metrics\n",
    "#accuracy = np.mean(y_pred_numeric == y_test1_numeric)\n",
    "#confusion(pred_dict, '../semeval-tweets/twitter-test1.txt', classifier=\"KNN\")\n",
    "\n",
    "\n",
    "# conf = confusion_matrix(y_test1_numeric, y_pred_numeric)\n",
    "# print('  -1   0   1')\n",
    "# print(conf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayes Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'alpha': 0.4094915062380423}\n",
      "MultinomialNB(alpha=0.4094915062380423)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.51      0.37      0.43       557\n",
      "           0       0.58      0.52      0.55      1504\n",
      "           1       0.58      0.70      0.64      1470\n",
      "\n",
      "    accuracy                           0.57      3531\n",
      "   macro avg       0.56      0.53      0.54      3531\n",
      "weighted avg       0.57      0.57      0.57      3531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NaiveBayes: finding the best params using GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "params = {'alpha': np.logspace(2,-7, num=50)}\n",
    "\n",
    "grid_clf = GridSearchCV(\n",
    "    estimator = MultinomialNB(),\n",
    "    scoring = 'f1_macro',\n",
    "    cv = 3,\n",
    "    param_grid = params)\n",
    "\n",
    "grid_clf.fit(Xtrain, Ytrain_numeric)\n",
    "print('Best params: ', grid_clf.best_params_)\n",
    "print(grid_clf.best_estimator_)\n",
    "print(classification_report(y_test1_numeric, grid_clf.predict(sparse_test1)))\n",
    "scores = abs(grid_clf.cv_results_['mean_test_score'])\n",
    "\n",
    "# Best params:  {'alpha': 0.4094915062380423} -> macro avg f1  0.52"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (47101, 49175)\n",
      "../semeval-tweets/twitter-test1.txt (KNN): 0.530\n",
      "../semeval-tweets/twitter-test2.txt (KNN): 0.522\n",
      "../semeval-tweets/twitter-test3.txt (KNN): 0.499\n",
      "overall run time: 0.03917503356933594\n",
      "average F1 score: 0.5169335106311347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "Xtrain = sparse_train_dev  # combining the two datasets\n",
    "print(\"shape\", Xtrain.shape)###\n",
    "ID_train = list(BOW_train.keys())            # list of IDs in train set\n",
    "ID_dev = list(BOW_dev.keys())\n",
    "ID_train_dev = ID_train + ID_dev\n",
    "sn_train = [sent_train[id] for id in ID_train]  # training labels train\n",
    "sn_dev = [sent_dev[id] for id in ID_dev]        # training labels dev\n",
    "Ytrain = np.array(sn_train)            # combining both labels\n",
    "Ytrain = np.array(sn_train + sn_dev)###            # combining both labels\n",
    "Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])   # numerical labels\n",
    "\n",
    "t0 = time.time() # timing the run\n",
    "clf = MultinomialNB(alpha = 0.4)\n",
    "clf.fit(Xtrain,Ytrain_numeric)\n",
    "#y_scores = clf.predict_proba(Xv)[:,1]\n",
    "\n",
    "# evaluation test1\n",
    "ID_test1 = list(BOW_test1.keys())\n",
    "y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test1)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test1, y_pred))\n",
    "s1 = evaluate(pred_dict, '../semeval-tweets/twitter-test1.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test2\n",
    "ID_test2 = list(BOW_test2.keys())\n",
    "y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test2)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test2, y_pred))\n",
    "s2 = evaluate(pred_dict, '../semeval-tweets/twitter-test2.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test3\n",
    "ID_test3 = list(BOW_test3.keys())\n",
    "y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test3)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test3, y_pred))\n",
    "s3 = evaluate(pred_dict, '../semeval-tweets/twitter-test3.txt', classifier=\"KNN\")  # best score 0.435\n",
    "t1 = time.time()  # timing the run\n",
    "\n",
    "print('overall run time:', t1-t0)\n",
    "print(\"average F1 score:\", (s1 + s2 + s3)/3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear SVM classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LinearSVM: finding the best params using GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "params = {'C':np.linspace(0.005,  0.05, 10),\n",
    "          'class_weight':['balanced'],\n",
    "          'tol': [0.01, 0.001],\n",
    "          'max_iter': [2000]\n",
    "          }\n",
    "\n",
    "grid_clf = GridSearchCV(\n",
    "    estimator = LinearSVC(),\n",
    "    scoring = 'f1_macro',\n",
    "    cv = 3,\n",
    "    param_grid = params)\n",
    "\n",
    "grid_clf.fit(Xtrain, Ytrain_numeric)\n",
    "print('Best params: ', grid_clf.best_params_)\n",
    "print(grid_clf.best_estimator_)\n",
    "print(classification_report(y_test1_numeric, grid_clf.predict(sparse_test1)))\n",
    "scores = abs(grid_clf.cv_results_['mean_test_score'])\n",
    "\n",
    "# best params -> LinearSVC(C=0.0200, class_weight='balanced', max_iter=2000, tol=0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "Xtrain = sparse_train_dev  # combining the two datasets\n",
    "print(\"shape\", Xtrain.shape)###\n",
    "ID_train = list(BOW_train.keys())            # list of IDs in train set\n",
    "ID_dev = list(BOW_dev.keys())\n",
    "ID_train_dev = ID_train + ID_dev\n",
    "sn_train = [sent_train[id] for id in ID_train]  # training labels train\n",
    "sn_dev = [sent_dev[id] for id in ID_dev]        # training labels dev\n",
    "Ytrain = np.array(sn_train)            # combining both labels\n",
    "Ytrain = np.array(sn_train + sn_dev)###            # combining both labels\n",
    "Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])   # numerical labels\n",
    "\n",
    "t0 = time.time() # timing the run\n",
    "clf = LinearSVC(C=0.0200, class_weight='balanced', max_iter=2000, tol=0.01)\n",
    "clf.fit(Xtrain,Ytrain_numeric)\n",
    "#y_scores = clf.predict_proba(Xv)[:,1]\n",
    "\n",
    "# evaluation test1\n",
    "ID_test1 = list(BOW_test1.keys())\n",
    "y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test1)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test1, y_pred))\n",
    "s1 = evaluate(pred_dict, '../semeval-tweets/twitter-test1.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test2\n",
    "ID_test2 = list(BOW_test2.keys())\n",
    "y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test2)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test2, y_pred))\n",
    "s2 = evaluate(pred_dict, '../semeval-tweets/twitter-test2.txt', classifier=\"KNN\")  # best score 0.435\n",
    "\n",
    "# evaluation test3\n",
    "ID_test3 = list(BOW_test3.keys())\n",
    "y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test3)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test3, y_pred))\n",
    "s3 = evaluate(pred_dict, '../semeval-tweets/twitter-test3.txt', classifier=\"KNN\")  # best score 0.435\n",
    "t1 = time.time()  # timing the run\n",
    "\n",
    "print('overall run time:', t1-t0)\n",
    "print(\"average F1 score:\", (s1 + s2 + s3)/3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression (MaxEnt) Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "params = {'C':[  0.07, 0.1, 0.15, 0.2],\n",
    "          'class_weight':['balanced'],\n",
    "          'tol': [0.001],\n",
    "          'max_iter': [500]\n",
    "          }\n",
    "\n",
    "grid_clf = GridSearchCV(\n",
    "    estimator = LogisticRegression(),\n",
    "    scoring = 'f1_macro',\n",
    "    cv = 3,\n",
    "    param_grid = params)\n",
    "\n",
    "grid_clf.fit(Xtrain, Ytrain_numeric)\n",
    "print('Best params: ', grid_clf.best_params_)\n",
    "print(grid_clf.best_estimator_)\n",
    "print(classification_report(y_test1_numeric, grid_clf.predict(sparse_test1)))\n",
    "scores = abs(grid_clf.cv_results_['mean_test_score'])\n",
    "\n",
    "# best params -> {'C': 0.15, 'class_weight': 'balanced', 'max_iter': 500, 'tol': 0.001}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (47101, 59467)\n",
      "../semeval-tweets/twitter-test1.txt (KNN): 0.619\n",
      "../semeval-tweets/twitter-test2.txt (KNN): 0.624\n",
      "../semeval-tweets/twitter-test3.txt (KNN): 0.563\n",
      "overall run time: 5.53150200843811\n",
      "average F1 score: 0.6020903614585245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "Xtrain = sparse_train_dev  # combining the two datasets\n",
    "print(\"shape\", Xtrain.shape)###\n",
    "ID_train = list(text_train.keys())            # list of IDs in train set\n",
    "ID_dev = list(text_dev.keys())\n",
    "ID_train_dev = ID_train + ID_dev\n",
    "sn_train = [sent_train[id] for id in ID_train]  # training labels train\n",
    "sn_dev = [sent_dev[id] for id in ID_dev]        # training labels dev\n",
    "Ytrain = np.array(sn_train)            # combining both labels\n",
    "Ytrain = np.array(sn_train + sn_dev)###            # combining both labels\n",
    "Ytrain_numeric = np.array([sent2num(y) for y in Ytrain])   # numerical labels\n",
    "\n",
    "\n",
    "t0 = time.time() # timing the run\n",
    "#clf = LogisticRegression(C=0.15, tol=0.001, penalty='l2', class_weight='balanced', max_iter=1000, multi_class='multinomial')\n",
    "clf = LogisticRegression(C=0.15, tol=0.001, class_weight='balanced', max_iter=1000, multi_class='multinomial')\n",
    "clf.fit(Xtrain,Ytrain_numeric)\n",
    "\n",
    "# evaluation test1\n",
    "ID_test1 = list(text_test1.keys())\n",
    "y_test1_numeric = np.array([sent2num(sent) for sent in sent_test1.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test1)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test1, y_pred))\n",
    "s1 = evaluate(pred_dict, '../semeval-tweets/twitter-test1.txt', classifier=\"KNN\")  # best score 0.562\n",
    "\n",
    "# evaluation test2\n",
    "ID_test2 = list(text_test2.keys())\n",
    "y_test2_numeric = np.array([sent2num(sent) for sent in sent_test2.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test2)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test2, y_pred))\n",
    "s2 = evaluate(pred_dict, '../semeval-tweets/twitter-test2.txt', classifier=\"KNN\")  # best score 0.597\n",
    "\n",
    "# evaluation test3\n",
    "ID_test3 = list(text_test3.keys())\n",
    "y_test3_numeric = np.array([sent2num(sent) for sent in sent_test3.values()])\n",
    "y_pred_numeric = clf.predict(sparse_test3)\n",
    "y_pred = [num2sent(num) for num in y_pred_numeric]\n",
    "pred_dict = dict(zip(ID_test3, y_pred))\n",
    "s3 = evaluate(pred_dict, '../semeval-tweets/twitter-test3.txt', classifier=\"KNN\")  # best score 0.556\n",
    "t1 = time.time()  # timing the run\n",
    "\n",
    "print('overall run time:', t1-t0)\n",
    "print(\"average F1 score:\", (s1 + s2 + s3)/3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Comparison of all classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "for classifier in ['NearestNeighbour', 'NaiveBayes','SVM']:\n",
    "    for features in ['BOW', '<feature-2-name>']:\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if classifier == 'NearestNeighbour':\n",
    "            # write the svm classifier here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'NaiveBayes':\n",
    "            # write the classifier 2 here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'SVM':\n",
    "            # write the classifier 3 here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'LSTM':\n",
    "            # write the LSTM classifier here\n",
    "            if features == 'bow':\n",
    "                continue\n",
    "            print('Training ' + classifier)\n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "\n",
    "        # Predition performance of the classifiers\n",
    "        for testset in testsets:\n",
    "            id_preds = {}\n",
    "            # write the prediction and evaluation code here\n",
    "\n",
    "            testset_name = testset\n",
    "            testset_path = join('semeval-tweets', testset_name)\n",
    "            evaluate(id_preds, testset_path, features + '-' + classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
