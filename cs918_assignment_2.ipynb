{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a *Jupyter notebook* for the coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, donâ€™t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n",
    "You may import more packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages here\n",
    "import re\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../semeval-tweets/twitter-test1.txt', '../semeval-tweets/twitter-test2.txt', '../semeval-tweets/twitter-test3.txt']\n"
     ]
    }
   ],
   "source": [
    "# Define test sets\n",
    "dataDir = '../semeval-tweets'\n",
    "testsetStrings = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']\n",
    "testsets = [join(dataDir, t) for t in testsetStrings]\n",
    "print(testsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght: 139\n",
      "102313285628711403\n",
      "neutral\n",
      "\"Bing one-ups knowledge graph, hires Encyclopaedia Britannica to supply results:   It may have retired from the cut-throat world of pr...\"\n",
      "\n",
      "Lenght: 139\n",
      "653274888624828198\n",
      "neutral\n",
      "\"On Thursday, concealed-carry gun license holders will be given a new right in the state of Oklahoma: the ability... http://t.co/oSgGHKi1\"\n",
      "\n",
      "Lenght: 137\n",
      "420747042670198316\n",
      "negative\n",
      "Miyagi just got banned from yoga. He was caught him sniffing the sphincter of the girl in front of him. There may be police involvement!\n",
      "\n",
      "Lenght: 135\n",
      "822064800445716046\n",
      "neutral\n",
      "Join us tonight at Boston Pizza - Centre on Barton for THURSDAY NIGHT FOOTBALL! Tonight the Chiefs take on the... http://t.co/iegTxPQv\n",
      "\n",
      "Lenght: 139\n",
      "055480020953212084\n",
      "neutral\n",
      "\"#FX NEW YORK, Oct 18 (Reuters) - The Federal Reserve provided $4.701 billion of liquidity to the ... http://t.co/BJhIQTtO #EUR #AUD #CAD\"\n",
      "\n",
      "Lenght: 132\n",
      "429443270273347255\n",
      "neutral\n",
      "\"13 April 1996, History is made, as the MetroStars and the Los Angeles Galaxy meet at the Rose Bowl in Pasadena, California, (1/3)\"\n",
      "\n",
      "Lenght: 135\n",
      "220323262844863802\n",
      "neutral\n",
      "Make sure you tune into The Saga exclusively on Temple's only student run radio WHIP at iHeartRadio Monday through Thursday's at 9pm!!\n",
      "\n",
      "Lenght: 135\n",
      "266953303729385574\n",
      "positive\n",
      "\"Saturday Nov 17th, 2012 Q Bar and Grill is the Place to be!  UFC 90's Event with Special Guests:  DJ BOBBY D... http://t.co/z96soXQf\"\n",
      "\n",
      "Lenght: 134\n",
      "267372990522222442\n",
      "negative\n",
      "@Waka_BacaFlame I'm already on my 3rd year at Colton it wouldn't make sense to graduate from gt after spending all this time here >.<\n",
      "\n",
      "Lenght: 131\n",
      "578446202147364926\n",
      "neutral\n",
      "\"Ephesians 6:11 (NEB) - Ephesians 6:11 (NEB) Put on all the armour which God provides, so that you may be... http://t.co/cEt5WKVK\"\n",
      "\n",
      "Lenght: 141\n",
      "810763059461202071\n",
      "neutral\n",
      "Andando in bicicletta a Roma -- si puo' dividere (share) pure li! Shareable: Changing Bike Culture in the Eternal City: http://t.co/nT3c7ryI\n",
      "\n",
      "Lenght: 132\n",
      "885726184364930528\n",
      "positive\n",
      ".@Q1047 Would luv if you could make an announcement about Chris Rene concert @theroxy this Thursday!! Thanks!! http://t.co/aKzopIrd\n",
      "\n",
      "Lenght: 141\n",
      "960689731242555262\n",
      "negative\n",
      "So now it seems Daav(Conspiracy) of Yadav have back fired...and if true/proved may create troubles for Akhilesh Yadav and Samajwadi Party...\n",
      "\n",
      "Lenght: 139\n",
      "491718215382409200\n",
      "neutral\n",
      "\"Charlie Rose with Desmond Tutu; Bill Joy (October 5, 1999): The Archbishop of Cape Town, South Africa and Nobel ... http://t.co/BVoS0dnd\"\n",
      "\n",
      "Lenght: 143\n",
      "317970942816831516\n",
      "neutral\n",
      "\"Remember folks. Grabbed 3 more earnings plays today. CELG,CME and HSY all calls for tomorrow!!! 5/5 for the week   $CME http://t.co/ZcmjGsha\"\n",
      "\n",
      "Lenght: 137\n",
      "281466922061596703\n",
      "neutral\n",
      "\"Deadly Israeli strike, fire from Gaza mar truce: A deadly Israeli airstrike into the Gaza Strip and rocket and... http://t.co/E0h27Mcg\"\n",
      "\n",
      "Lenght: 137\n",
      "076653802834383376\n",
      "positive\n",
      "PST Keane delivers best MLS goal from Saturday: The Los Angeles Galaxy were on top of things early tonight again... http://t.co/aVYSaVOo\n",
      "\n",
      "Lenght: 142\n",
      "629377153435614908\n",
      "negative\n",
      "\"Heather is like the serpent in the garden trying 2 entice info from McBain! Michael Easton, dont ever go away that long! Watching Tues #GH.\"\n",
      "\n",
      "Lenght: 142\n",
      "255090901012423499\n",
      "neutral\n",
      "\"Southampton's Richard Bland is -9 after his 2nd round in the Madeira Islands Open on the European Tour. Currently tied 4th, 3 off the lead.\"\n",
      "\n",
      "Lenght: 135\n",
      "581663993931072193\n",
      "neutral\n",
      "\"Overexposed Tour, Maroon 5 With The Cab: Through The Eyes Of A Drummer - As you may have heard, The Cab just... http://t.co/H3c7kQsw\"\n",
      "\n",
      "Lenght: 136\n",
      "854756032273426749\n",
      "neutral\n",
      "Remember to join the Springboks at 3pm tomorrow at Orlando Stadium to watch their open training session as they... http://t.co/vaCN6EZ0\n",
      "\n",
      "Lenght: 137\n",
      "385766324957619690\n",
      "positive\n",
      "@MichelleLMyers #BreakingDawnPart2 The Saga may end but the love and friendships will last 4ever #LoveIsLouderthanRollingTheFinalCredits\n",
      "\n",
      "Lenght: 134\n",
      "844654517192428026\n",
      "positive\n",
      "\"Happy 102 years to L.A. artist Tyrus Wong! May the road rise to meet you, may the wind be always at your back: http://t.co/e8SB5S90\"\n",
      "\n",
      "Lenght: 137\n",
      "233928065818229615\n",
      "neutral\n",
      "Federal Reserve stands firm on bond-buying program: The Federal Reserve said on Wednesday that it is s... http://t.co/KnRMVgLu #business\n",
      "\n",
      "Lenght: 139\n",
      "226170643240610310\n",
      "positive\n",
      "\"Bersani, supports stability agreement to secure government: (AGI) Rome, October 24 - Tomorrow the agreement on t... http://t.co/ew0JWQgD\"\n",
      "\n",
      "Lenght: 141\n",
      "282416648112742867\n",
      "neutral\n",
      "\"About to watch Kony 2012 video. Despite 100m views, there are quite a number here watching for 1st time. Interested in the reactions.#yuga\"\n",
      "\n",
      "Lenght: 139\n",
      "790411654414429445\n",
      "neutral\n",
      "\"LG may update the Optimus 2X to ICS after all, releases source code: The saga of the elusive ICS update for the ... http://t.co/g0hT0NtX\"\n",
      "\n",
      "Lenght: 136\n",
      "712768996713302192\n",
      "positive\n",
      "Saturday night Adult Swim (Cowboy Bebop + Ghost in the Shell) reminds me how much I adore Yoko Kanno's music. Such an amazing composer.\n",
      "\n",
      "Lenght: 136\n",
      "170040141580222961\n",
      "neutral\n",
      "Just one of the songs to expect during THE FRAY's concert in Smart Araneta Coliseum on November 10. This is off... http://t.co/BUVMOlcM\n",
      "\n",
      "Lenght: 139\n",
      "836541798391946839\n",
      "neutral\n",
      "\"Seven Penny Stocks on the Move with Heavy Volume, April 24: CSOC shares have traded as high as $.18 over the pas... http://t.co/OX7oCx72\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the structure of the dataset\n",
    "with open(testsets[0], 'r', encoding='utf8') as f1:\n",
    "    i = 0\n",
    "    for line in f1:\n",
    "        fields = line.split('\\t')\n",
    "        if i < 30:\n",
    "            length = len(fields[2])\n",
    "            if length > 130:\n",
    "                print(f\"Lenght: {length}\")\n",
    "                print(fields[0])  # 1st column - tweet ID\n",
    "                print(fields[1])  # 2nd column - tweet sentiment\n",
    "                print(fields[2])  # 3rd column - tweet text\n",
    "                i += 1\n",
    "\n",
    "# preprocessing questions and notes: \n",
    "  # -> what about removing the @usernames, is it advisable?\n",
    "  # -> need to remove URLs!\n",
    "  # -> There is a lot of noise/mistakes in the data and absence of interpunction.\n",
    "  # -> what about adding of the starting token?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    reading the testset and return a dictionary with: ID -> sentiment\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}  # init the dictionary\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            positive  negative  neutral\n",
      "positive    1.000     0.000     0.000     \n",
      "negative    0.000     1.000     0.000     \n",
      "neutral     0.000     0.000     1.000     \n",
      "\n",
      "../semeval-tweets/twitter-test1.txt (PerfectClassifier): 1.000\n"
     ]
    }
   ],
   "source": [
    "# testing the evaluation functions\n",
    "tweetDict = read_test('../semeval-tweets/twitter-test1.txt')\n",
    "confusion(tweetDict, '../semeval-tweets/twitter-test1.txt', \"PerfectClassifier\")\n",
    "evaluate(tweetDict, '../semeval-tweets/twitter-test1.txt', \"PerfectClassifier\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, dev set and testing set\n",
    "\n",
    "dataDir = '../semeval-tweets'  # change to the proper directory\n",
    "datasetStrings = ['twitter-training-data.txt', 'twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt', 'twitter-dev-data.txt']\n",
    "datasets = [join(dataDir, t) for t in datasetStrings]\n",
    "\n",
    "tweet_IDs = {}          # init dictionary with tweet IDs\n",
    "tweet_sentiments = {}   # init dictionary with sentiments\n",
    "tweet_texts = {}        # init dictionary with tweet texts\n",
    "\n",
    "for DatasetString in datasets:\n",
    "    data_ID, data_sent, data_text  = {}, {}, {}    # temp dictionaries\n",
    "    with open(DatasetString, 'r', encoding='utf8') as f1:\n",
    "        for i, line in enumerate(f1):\n",
    "            fields = line.split('\\t')\n",
    "            data_ID[i] = fields[0]            # tweet IDs\n",
    "            data_sent[fields[0]] = fields[1]  # sentiments\n",
    "            data_text[fields[0]] = fields[2]  # tweet text\n",
    "    tweet_IDs[DatasetString] = data_ID\n",
    "    tweet_sentiments[DatasetString] = data_sent\n",
    "    tweet_texts[DatasetString] = data_text\n",
    "\n",
    "# sentiment dictionaries\n",
    "sent_train = tweet_sentiments[datasets[0]]\n",
    "sent_test1 = tweet_sentiments[datasets[1]]\n",
    "sent_test2 = tweet_sentiments[datasets[2]]\n",
    "sent_test3 = tweet_sentiments[datasets[3]]\n",
    "sent_dev = tweet_sentiments[datasets[4]]\n",
    "\n",
    "# tweet text dictionaries\n",
    "text_train = tweet_texts[datasets[0]]\n",
    "text_test1 = tweet_texts[datasets[1]]\n",
    "text_test2 = tweet_texts[datasets[2]]\n",
    "text_test3 = tweet_texts[datasets[3]]\n",
    "text_dev = tweet_texts[datasets[4]]\n",
    "\n",
    "# tweet IDs dictionaries\n",
    "IDs_train = tweet_IDs[datasets[0]]\n",
    "IDs_test1 = tweet_IDs[datasets[1]]\n",
    "IDs_test2 = tweet_IDs[datasets[2]]\n",
    "IDs_test3 = tweet_IDs[datasets[3]]\n",
    "IDs_dev = tweet_IDs[datasets[4]]\n",
    "\n",
    "\n",
    "## examples and tests\n",
    "# id = IDs_train[0]\n",
    "# id_dev = IDs_dev[0]\n",
    "# id1 = IDs_test1[0]\n",
    "# id2 = IDs_test2[0]\n",
    "# id3 = IDs_test3[0]\n",
    "# print(f\"-ID:{id} \\n-TEXT:{text_train[id]}-SENTIMENT: {sent_train[id]}\\n\")\n",
    "# print(f\"-ID:{id_dev} \\n-TEXT:{text_dev[id_dev]}-SENTIMENT: {sent_dev[id_dev]}\\n\")\n",
    "# print(f\"-ID:{id1} \\n-TEXT:{text_test1[id1]}-SENTIMENT: {sent_test1[id1]}\\n\")\n",
    "# print(f\"-ID:{id2} \\n-TEXT:{text_test2[id2]}-SENTIMENT: {sent_test2[id2]}\\n\")\n",
    "# print(f\"-ID:{id3} \\n-TEXT:{text_test3[id3]}-SENTIMENT: {sent_test3[id3]}\\n\")\n",
    "# print(len(IDs_train.keys()), len(text_train.keys()), len(sent_train.keys()))  # 45101\n",
    "# print(len(IDs_test1.keys()), len(text_test1.keys()), len(sent_test1.keys()))  # 3531\n",
    "# print(len(IDs_test2.keys()), len(text_test2.keys()), len(sent_test2.keys()))  # 1853\n",
    "# print(len(IDs_test3.keys()), len(text_test3.keys()), len(sent_test3.keys()))  # 2379\n",
    "# print(len(IDs_dev.keys()), len(text_dev.keys()), len(sent_dev.keys()))        # 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "#### Order of preprocessing\n",
    "* lowercase text\n",
    "* regex cleaning\n",
    "   * Remove URLs\n",
    "   * Remove non-alphanumeric characters (leave hashtags and usernames)\n",
    "   * Remove numbers that are fully made of digits\n",
    "   * (Remove words with only 1 character)\n",
    "\n",
    " #### Preprocessing questions and notes:\n",
    "   -> what about removing the @usernames, is it advisable?\n",
    "   -> need to remove URLs!\n",
    "   -> There is a lot of noise/mistakes in the data and absence of interpunction.\n",
    "   -> what about adding of the starting token?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Deleted link]\n",
      "[Deleted link]\n",
      "[Deleted link]\n",
      "[Deleted link]\n",
      "U.S.A.\n",
      "[Deleted link]\n",
      "[Deleted link]\n",
      "ltd.\n",
      "etc.\n",
      "[Deleted link]\n",
      "I said 'yes'.But I did not say Why.\n",
      "[Deleted link]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top100 = ['com', 'net', 'org', 'jp', 'de', 'uk', 'fr', 'br', 'it', 'ru', 'es', 'me', 'gov', 'pl', 'ca', 'au', 'cn', 'co', 'in', 'nl', 'edu', 'info', 'eu', 'ch', 'id', 'at', 'kr', 'cz', 'mx', 'be', 'tv', 'se', 'tr', 'tw', 'al', 'ua', 'ir', 'vn', 'cl', 'sk', 'ly', 'cc', 'to', 'no', 'fi', 'us', 'pt', 'dk', 'ar', 'hu', 'tk', 'gr', 'il', 'news', 'ro', 'my', 'biz', 'ie', 'za', 'nz', 'sg', 'ee', 'th', 'io', 'xyz', 'pe', 'bg', 'hk', 'rs', 'lt', 'link', 'ph', 'club', 'si', 'site', 'mobi', 'by', 'cat', 'wiki', 'la', 'ga', 'xxx', 'cf', 'hr', 'ng', 'jobs', 'online', 'kz', 'ug', 'gq', 'ae', 'is', 'lv', 'pro', 'fm', 'tips', 'ms', 'sa', 'app', 'lat']\n",
    "\n",
    "\n",
    "text = '''\n",
    "www.abc.com\n",
    "www.bcd.net\n",
    "www.sss.cc\n",
    "www.dcamp.uk\n",
    "U.S.A.\n",
    "google.fr\n",
    "google.it\n",
    "ltd.\n",
    "etc.\n",
    "sell.uk\n",
    "I said 'yes'.But I did not say Why.\n",
    "https://www.scoutdns.com/100-most-popular-tlds-by-google-index/\n",
    "'''\n",
    "\n",
    "\n",
    "for ext in top100:\n",
    "    re_string = \"[^\\s]*\\.\" + ext + \"[^\\s]*\"\n",
    "    new_text = re.sub(re_string, '[Deleted link]', text)\n",
    "    text = new_text\n",
    "print(new_text)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "if os.path.isfile(\"preprocessing.pkl\"):  # loading preprocessed datasets\n",
    "    with open('preprocessing.pkl', 'rb') as inp_file:\n",
    "        temp_dicts = pickle.load(inp_file)\n",
    "        txt_dicts = temp_dicts[0:5]\n",
    "        txtlist_dicts = temp_dicts[5:]\n",
    "\n",
    "else:\n",
    "    ID_dicts = [IDs_train, IDs_test1, IDs_test2, IDs_test3, IDs_dev]\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev]\n",
    "    txtlist_dicts = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  # init the lemmatizer\n",
    "    POSconvert = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "\n",
    "    for i, IDdict in enumerate(ID_dicts):\n",
    "        output = txt_dicts[i]\n",
    "        output_txt = {}\n",
    "        for id in IDdict.values():\n",
    "            text = output[id].lower()\n",
    "\n",
    "            # replace/delete all URLs starting with 'http' and 'www'\n",
    "            new_text = re.sub(\"http[^\\s]*\", '', text)\n",
    "            new_text = re.sub(\"www[^\\s]*\", '', new_text)\n",
    "\n",
    "            # delete all URLs which have one of 100 most common extensions ('.com', '.net', ...)\n",
    "            top100 = ['com', 'net', 'org', 'jp', 'de', 'uk', 'fr', 'br', 'it', 'ru', 'es', 'me', 'gov', 'pl', 'ca', 'au', 'cn', 'co', 'in', 'nl', 'edu', 'info', 'eu', 'ch', 'id', 'at', 'kr', 'cz', 'mx', 'be', 'tv', 'se', 'tr', 'tw', 'al', 'ua', 'ir', 'vn', 'cl', 'sk', 'ly', 'cc', 'to', 'no', 'fi', 'us', 'pt', 'dk', 'ar', 'hu', 'tk', 'gr', 'il', 'news', 'ro', 'my', 'biz', 'ie', 'za', 'nz', 'sg', 'ee', 'th', 'io', 'xyz', 'pe', 'bg', 'hk', 'rs', 'lt', 'link', 'ph', 'club', 'si', 'site', 'mobi', 'by', 'cat', 'wiki', 'la', 'ga', 'xxx', 'cf', 'hr', 'ng', 'jobs', 'online', 'kz', 'ug', 'gq', 'ae', 'is', 'lv', 'pro', 'fm', 'tips', 'ms', 'sa', 'app', 'lat']\n",
    "            for ext in top100:\n",
    "                re_string = \"[^\\s]*\\.\" + ext + \"[^\\s]*\"\n",
    "                new_text = re.sub(re_string, '', new_text)\n",
    "\n",
    "            # removing '&amp'\n",
    "            new_text = re.sub('&amp','', new_text)\n",
    "\n",
    "            # remove all non-alphanumeric chars except for '# and @'\n",
    "            new_text = re.sub('[^\\w\\s@#]','', new_text)\n",
    "\n",
    "            # remove strings with '#' not on the beginning (to keep only hashtags)\n",
    "            new_text = re.sub('\\s[\\w]+#[\\w]*','', new_text)\n",
    "\n",
    "            # numbers fully made of digits\n",
    "            new_text = re.sub('[\\d]+\\s','', new_text)\n",
    "\n",
    "            # remove words with only 1 character\n",
    "            new_text = re.sub('\\\\b\\\\w{1}\\\\b','', new_text)\n",
    "\n",
    "            # remove newline chars\n",
    "            new_text = new_text.replace('\\n', ' ')\n",
    "\n",
    "            # replace a multiple spaces with a single space\n",
    "            new_text = re.sub('\\s+',' ', new_text)\n",
    "\n",
    "            # using the lemmatizer\n",
    "            txt_list = nltk.word_tokenize(new_text)\n",
    "            for k, word in enumerate(txt_list):  # fixing the separation of hashtags by the tokenizer\n",
    "                if word == '#' or word == '@':\n",
    "                    if k < len(txt_list) - 1:\n",
    "                        txt_list[k] = txt_list[k] + txt_list[k+1]\n",
    "                        txt_list.pop(k+1)\n",
    "            POS = nltk.pos_tag(txt_list)                  # POS tags from nltk\n",
    "            WordNetPOS = [POSconvert(P[1]) for P in POS]  # POS tags for lemmatizer\n",
    "            for j in range(len(txt_list)):\n",
    "                word = txt_list[j]\n",
    "                lemmatized = lemmatizer.lemmatize(word, WordNetPOS[j])  # process each token/word one by one\n",
    "                txt_list[j] = lemmatized  # update the word in the txt_list\n",
    "\n",
    "            ## UPDATE the dictionary\n",
    "            output_txt[id] = ' '.join(txt_list)\n",
    "            output[id] = txt_list\n",
    "\n",
    "        txt_dicts[i] = output_txt\n",
    "        txtlist_dicts.append(output)\n",
    "\n",
    "text_train = txt_dicts[0]\n",
    "text_test1 = txt_dicts[1]\n",
    "text_test2 = txt_dicts[2]\n",
    "text_test3 = txt_dicts[3]\n",
    "text_dev = txt_dicts[4]\n",
    "txtlist_train = txtlist_dicts[0]\n",
    "txtlist_test1 = txtlist_dicts[1]\n",
    "txtlist_test2 = txtlist_dicts[2]\n",
    "txtlist_test3 = txtlist_dicts[3]\n",
    "txtlist_dev = txtlist_dicts[4]\n",
    "\n",
    "# saving preprocessing.pkl\n",
    "if not os.path.isfile(\"preprocessing.pkl\"):\n",
    "    txt_dicts = [text_train, text_test1, text_test2, text_test3, text_dev, txtlist_train, txtlist_test1, txtlist_test2, txtlist_test3, txtlist_dev]\n",
    "    with open('preprocessing.pkl', 'wb') as out_file:\n",
    "        pickle.dump(txt_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: Bag of words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# Bag of Words - my implementation:\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords as Stopwords\n",
    "\n",
    "# 1) removing stop words\n",
    "stopwords = Stopwords.words('english')\n",
    "stopwords = [word.replace('\\'', '') for word in stopwords]\n",
    "\n",
    "# 2) extracting the dictionary/vocabulary\n",
    "freq = FreqDist()   # frequency distribution\n",
    "for Dict in txtlist_dicts:\n",
    "    for tweet in Dict.values():\n",
    "        for word in tweet:\n",
    "            if not word in stopwords:\n",
    "                freq[word] += 1\n",
    "\n",
    "nums = range(len(freq.keys()))\n",
    "vocabulary = list(freq.keys())              # creating the dictionary\n",
    "vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out of vocabulary' word\n",
    "vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words -> len: 69742\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its BoW representation as np.array\n",
    "def text2BOW(text_list, vocabulary, stopwords):\n",
    "    BOW_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in text_list:\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                BOW_vec[vocab2num[word]] += 1\n",
    "            else:\n",
    "                BOW_vec[vocab2num['<OOV>']] += 1\n",
    "    return BOW_vec\n",
    "\n",
    "# if os.path.isfile(\"BOWs.pkl\"):  # loading preprocessed datasets\n",
    "#     with open('BOWs.pkl', 'rb') as inp_file:\n",
    "#         ll = pickle.load(inp_file)\n",
    "#         BOW_train, BOW_test1, BOW_test2, BOW_test3, BOW_dev = ll[0], ll[1], ll[2], ll[3], ll[4]\n",
    "#\n",
    "# else:\n",
    "# Bag of Words (BOW) for each tweet\n",
    "BOW_train = {}\n",
    "for ID, tweet in txtlist_train.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_train[ID] = BOW\n",
    "\n",
    "BOW_test1 = {}\n",
    "for ID, tweet in txtlist_test1.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_test1[ID] = BOW\n",
    "\n",
    "BOW_test2 = {}\n",
    "for ID, tweet in txtlist_test2.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_test2[ID] = BOW\n",
    "\n",
    "BOW_test3 = {}\n",
    "for ID, tweet in txtlist_test3.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_test3[ID] = BOW\n",
    "\n",
    "BOW_dev = {}\n",
    "for ID, tweet in txtlist_dev.items():\n",
    "    BOW = text2BOW(tweet, vocabulary=vocabulary, stopwords=stopwords)\n",
    "    BOW_dev[ID] = BOW\n",
    "\n",
    "\n",
    "# saving BOWs.pkl: very large file - maybe not the best idea to save it?\n",
    "    # if not os.path.isfile(\"BOWs.pkl\"):\n",
    "    #     BOW_dicts = [BOW_train, BOW_test1, BOW_test2, BOW_test3, BOW_dev]\n",
    "    #     with open(\"BOWs.pkl\", 'wb') as out_file:\n",
    "    #         pickle.dump(BOW_dicts, out_file, protocol=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: TF-IDF weighted Bag of words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(['a', 'b', 'b'])\n",
    "np.count_nonzero(a == 'b')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "# extracting the dictionary\n",
    "# freq = FreqDist()   # frequency distribution\n",
    "# for Dict in txtlist_dicts:\n",
    "#     for tweet in Dict.values():\n",
    "#         for word in tweet:\n",
    "#             if not word in stopwords:\n",
    "#                 freq[word] += 1\n",
    "#\n",
    "# nums = range(len(freq.keys()))\n",
    "# vocabulary = list(freq.keys())              # creating the dictionary\n",
    "# vocabulary_array = np.array(vocabulary)     # np.array of the dictionary\n",
    "# vocabularyOOV = vocabulary + ['<OOV>']      # dictionary with 'out of vocabulary' word\n",
    "# vocab2num = dict(zip(vocabulary, nums))     # word to index mapping\n",
    "# vocab2num['<OOV>'] = max(vocab2num.values()) + 1  # out of vocabulary words -> len: 69742\n",
    "\n",
    "# extracting the dictionary\n",
    "DFfreq = FreqDist()   # document frequency distribution\n",
    "Ntexts = len(IDs_train) + len(IDs_test1) + len(IDs_test2) + len(IDs_test3) + len(IDs_dev)\n",
    "for Dict in txtlist_dicts:\n",
    "    for tweet in Dict.values():\n",
    "        for word in np.unique(tweet):\n",
    "            if not word in stopwords:\n",
    "                DFfreq[word] += 1\n",
    "\n",
    "# auxiliary ftion which takes list of words and returns its TFIDF representation as np.array\n",
    "def text2TFIDF(text_list, vocabulary, stopwords, Ntexts):\n",
    "    TFIDF_vec = np.zeros(len(vocabulary) + 1)\n",
    "    for word in np.unique(text_list):\n",
    "        if not word in stopwords:\n",
    "            if word in vocabulary:\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / DFfreq[word])\n",
    "                TFIDF_vec[vocab2num[word]] = tf * idf\n",
    "            else:\n",
    "                tf = np.count_nonzero(np.array(text_list) == word) / len(text_list)\n",
    "                idf = np.log2(Ntexts / 0.000001 )\n",
    "                TFIDF_vec[vocab2num['<OOV>']] = tf * idf\n",
    "    return TFIDF_vec\n",
    "\n",
    "\n",
    "# TFIDF-weighted Bag of Words for each tweet\n",
    "TFIDF_train = {}\n",
    "for ID, tweet in txtlist_train.items():\n",
    "    tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "    TFIDF_train[ID] = tfidf\n",
    "\n",
    "# TFIDF_test1 = {}\n",
    "# for ID, tweet in txtlist_test1.items():\n",
    "#     tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "#     TFIDF_test1[ID] = tfidf\n",
    "#\n",
    "# TFIDF_test2 = {}\n",
    "# for ID, tweet in txtlist_test2.items():\n",
    "#     tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "#     TFIDF_test2[ID] = tfidf\n",
    "#\n",
    "# TFIDF_test3 = {}\n",
    "# for ID, tweet in txtlist_test3.items():\n",
    "#     tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "#     TFIDF_test3[ID] = tfidf\n",
    "#\n",
    "# TFIDF_dev = {}\n",
    "# for ID, tweet in txtlist_dev.items():\n",
    "#     tfidf = text2TFIDF(tweet, vocabulary=vocabulary, stopwords=stopwords, Ntexts=Ntexts)\n",
    "#     TFIDF_dev[ID] = tfidf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45101 45101\n"
     ]
    },
    {
     "data": {
      "text/plain": "14"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(TFIDF_train), len(text_train))\n",
    "np.count_nonzero(TFIDF_train[IDs_train[500]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('but', 1), ('hate', 1), ('i', 3), ('language', 1), ('love', 1), ('natural', 1), ('processing', 3), ('python', 1), ('image', 2), ('like', 2), ('and', 1), ('signal', 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFfreq = FreqDist()   # document frequency distribution\n",
    "temp = [['i', 'love', 'natural', 'language', 'processing', 'but', 'i', 'hate', 'python'],\n",
    "['i', 'like' , 'image', 'processing'],['i', 'like', 'signal', 'processing', 'and' , 'image', 'processing' ]]\n",
    "\n",
    "for tweet in temp:\n",
    "    for word in np.unique(tweet):\n",
    "        if not word in []:\n",
    "            DFfreq[word] += 1\n",
    "\n",
    "\n",
    "print(DFfreq.items())\n",
    "DFfreq['but']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Feature Extraction: GloVe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "for classifier in ['NeirestNeighbour', 'NaiveBayes','SVM']:\n",
    "    for features in ['BOW', '<feature-2-name>']:\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if classifier == 'NeirestNeighbour':\n",
    "            # write the svm classifier here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'NaiveBayes':\n",
    "            # write the classifier 2 here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'SVM':\n",
    "            # write the classifier 3 here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'LSTM':\n",
    "            # write the LSTM classifier here\n",
    "            if features == 'bow':\n",
    "                continue\n",
    "            print('Training ' + classifier)\n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "\n",
    "        # Predition performance of the classifiers\n",
    "        for testset in testsets:\n",
    "            id_preds = {}\n",
    "            # write the prediction and evaluation code here\n",
    "\n",
    "            testset_name = testset\n",
    "            testset_path = join('semeval-tweets', testset_name)\n",
    "            evaluate(id_preds, testset_path, features + '-' + classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
